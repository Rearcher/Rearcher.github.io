<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Life&#39;s monolog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rearcher.github.io/"/>
  <updated>2018-09-24T12:48:46.349Z</updated>
  <id>https://rearcher.github.io/</id>
  
  <author>
    <name>Rahul</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>有趣的背包问题</title>
    <link href="https://rearcher.github.io/interesting-knapsack-problem.html"/>
    <id>https://rearcher.github.io/interesting-knapsack-problem.html</id>
    <published>2018-09-24T10:29:48.000Z</published>
    <updated>2018-09-24T12:48:46.349Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在写动态规划相关的算法题，遇到了很多背包问题的变种问题，解法也很精彩，趁着中秋有空，记录于此。</p><h2 id="基本的背包问题"><a href="#基本的背包问题" class="headerlink" title="基本的背包问题"></a>基本的背包问题</h2><p>背包问题的定义：</p><blockquote><p>有N种物品，对于第i种物品，其体积是$V_i$，价值是$W_i$，数量是$M_i$。现有容量为C的背包，要求从这N种物品中选择一定数量的物品放入背包，使得背包所装的物品价值总和最大。</p></blockquote><p>根据物品数量的不同，又可以把背包问题分为以下三种类型：</p><blockquote><ol><li>每种物品的数量都是1，这时称为<strong>01背包问题</strong>；</li><li>每种物品的数量都是无限，这时称为<strong>完全背包问题</strong>；</li><li>每种物品的数量都是1个或多个，这时称为<strong>多重背包问题</strong>。</li></ol></blockquote><h3 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h3><p>最简单的是01背包问题，这里简单定义$f(i,j)$为考虑使用前i种物品和容量为j的背包时，所能获取的最大价值。对于第i种物品有两种策略，一种是取，一种是不取，那么01背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max\{f(i - 1, j),  f(i - 1, j - V_i) + W_i\}</script><p>其中，$f(i - 1, j)$表示不取第i种物品，$f(i - 1, j - V_i) + W_i$表示取第i种物品。此时的代码是一个二重循环，因为递推式中只用到了上一轮的状态，所以可以优化空间复杂度，一维数组就可以求解。代码类似于下面这样：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 01背包</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = C; j &gt;= V[i]; --j)</span><br><span class="line">        dp[j] = max(dp[j], dp[j - V[i]] + W[i]);</span><br></pre></td></tr></table></figure></p><p>注意到第二层循环是从后往前的，因为$f(i, j)$需要用到$f(i, j - V_i)$，只有从后往前，才能保证用到的是上一轮的状态，即“没有选取过第i种物品”的状态。</p><h3 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h3><p>完全背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max\{f(i - 1, j),  f(i, j - V_i) + W_i\}</script><p>其中，$f(i-1,j)$代表第i种物品一件都不取的子状态，$f(i, j - V_i) + W_i$代表已经取了第i种物品（多少件不知道）的子状态。代码如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 完全背包</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = V[i]; j &lt;= C; ++j)</span><br><span class="line">        dp[j] = max(dp[j], dp[j - V[i]] + W[i]);</span><br></pre></td></tr></table></figure></p><p>可以看到，与01背包的区别只是第二层的循环顺序不同而已。</p><h3 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h3><p>基本思想是每件物品可以取0,1,2,…,$M_i$件，此时多重背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max \{ f(i - 1, j - k * V_i)\}, 0 \le k \le M_i</script><p>时间复杂度是$O(V\Sigma M_i)$。这个时间复杂度在物品件数较高的情况下是较高的，常见的优化为二进制优化，将多重背包问题转化为01背包问题来求解，此时时间复杂度可以下降到$O(Vlog\Sigma M_i)$。</p><p>二进制优化：</p><blockquote><p>将第i种物品分成若干件01背包中的物品，其中每件物品有一个系数，该物品的价值和体积都是原来的价值和体积乘上这个系数。这些系数为$1,2,4,8,…,2^{k-1},M_i - 2^k + 1$，其中k是满足$M_i - 2 ^ k + 1 &gt; 0$的最大整数。</p></blockquote><h2 id="一些变种背包问题"><a href="#一些变种背包问题" class="headerlink" title="一些变种背包问题"></a>一些变种背包问题</h2><h3 id="要求背包装满"><a href="#要求背包装满" class="headerlink" title="要求背包装满"></a>要求背包装满</h3><p>题目链接在<a href="http://poj.org/problem?id=1742" target="_blank" rel="noopener">这里</a>。题目大意是有一堆硬币，每个硬币有不同的面值以及数量，给定一个值m，要求1至m中有多少个值可以被这些硬币凑出来。看成背包问题来做，好像是一个多重背包问题，背包的最大容量是m。但这题与普通的背包问题又有点区别，普通的背包问题要求的是“背包里物品的价值最大”，而这题转换成背包问题是“哪些容量的背包可以被装满”。这区别可以用初始化来体现，如果不要求背包装满，那么所有的初始状态都是0；如果要求装满，那么只将$f(0,0)$设为0，其他设置为一个表示不合法的特殊值即可。</p><p>上面说到多重背包问题可以用二进制优化的思想，转化为01背包问题求解。但对于这道题，这样的时间复杂度还是太高。当背包问题不要求背包里物品价值最大，而仅仅要求装满背包时，可以有$O(NV)$的解法：用$num_j$表示装满容量为j的背包当前物品最多需要多少个，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对于每一轮，用num[j]表示装满容量为j的背包当前物品最多需要多少个，</span></span><br><span class="line"><span class="comment"> * dp[j] = 1表示能将容量为j的背包装满，dp[j] = 0表示不能，dp[0] = 1，</span></span><br><span class="line"><span class="comment"> * 最后dp[1..V]中为1的都是对应的能被装满的容量。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">    <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="keyword">sizeof</span>(num));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = V[i]; j &lt;= m; ++j) &#123;</span><br><span class="line">        <span class="comment">// 如果当前容量j的背包还不能被装满，且j-V[i]的背包能被装满，</span></span><br><span class="line">        <span class="comment">// 且装满j-V[i]容量的背包所用的当前物品的数量小于M[i]。</span></span><br><span class="line">        <span class="keyword">if</span> (!dp[j] &amp;&amp; dp[j - V[i]] &amp;&amp; num[j - V[i]] &lt; M[i]) &#123;</span><br><span class="line">            dp[j] = <span class="number">1</span>;</span><br><span class="line">            num[j] = num[j - V[i]] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="物品的重量为负值"><a href="#物品的重量为负值" class="headerlink" title="物品的重量为负值"></a>物品的重量为负值</h3><p>题目链接在<a href="http://poj.org/problem?id=2184" target="_blank" rel="noopener">这里</a>。题目大意是每头牛都有一个智力值$S_i$和幽默值$F_i$，要求选出一些牛，这些牛的智力值总和和幽默值总和最大，并且智力值总和必须大于0，幽默值总和也必须大于0。</p><p>这题也可以看成背包问题来做，是01背包问题。把智力值当成物品的重量，幽默值当成物品的价值，并且也属于将背包装满的问题。问题在于，物品的重量存在负值。因为数组的索引最小是0，不能存在负值，所以可以人为加一个偏移值offset（比如100000），初始化时令<code>dp[offset]=0</code>，其余位置都设置为一个不合法的值即可。还有一个问题是，01背包问题递推时，要求从后往前，因为$f(i,j)$需要用到$f(i, j-V_i)$的子状态，但是如果$V_i &lt; 0$，此时$j - V_i &gt; j$，应该<strong>从前往后更新</strong>！具体AC代码节选如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>, offset = <span class="number">100000</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">200000</span>; ++i) dp[i] = -inf;</span><br><span class="line">dp[offset] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (s[i] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">200000</span>; j &gt;= s[i]; --j)</span><br><span class="line">            <span class="keyword">if</span> (dp[j - s[i]] &gt; -inf)</span><br><span class="line">                dp[j] =  max(dp[j], dp[j - s[i]] + f[i]);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;  j - s[i] &lt;= <span class="number">200000</span>; ++j)</span><br><span class="line">            <span class="keyword">if</span> (dp[j - s[i]] &gt; -inf)</span><br><span class="line">                dp[j] = max(dp[j], dp[j - s[i]] + f[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以从前往后更新也不一定就是完全背包哦^_^。</p><h3 id="对于不同物品具有不同的背包上限"><a href="#对于不同物品具有不同的背包上限" class="headerlink" title="对于不同物品具有不同的背包上限"></a>对于不同物品具有不同的背包上限</h3><p>题目链接在<a href="http://poj.org/problem?id=2392`" target="_blank" rel="noopener">这里</a>。题目大意是有各种不同高度和数量的砖块，每种砖块的高度是$h_i$，数量是$c_i$，第i种砖块最高只能用在$a_i$以下高度的地方，要求用这些砖块能达到最大的高度是多少。</p><p>可以看到这也是一个多重背包问题，物品的重量和价值相同，都是砖块的高度$h_i$，同时也属于把背包装满的问题。考虑到能用尽可能多的砖块，所以$a_i$小的砖块应该先用，所以首先需要对砖块进行排序。同时对于砖块i，其背包容量的上限是$a_i$。总的背包容量上限是最大的$a_i$。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>题目总是千变万化，关键是如何将未知的题目转化为已知的题型来求解，这要求对经典的解法有深入的理解。本文所谈及的背包问题变种还只是冰山一角，还有更多未涉及的话题。“我亦无他，唯手熟尔”，很多解题思想并不是只能用在一个地方，而是很多地方都能运用，只有多多练习，以后遇到新的题型才能有更多想法吧，keep moving!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在写动态规划相关的算法题，遇到了很多背包问题的变种问题，解法也很精彩，趁着中秋有空，记录于此。&lt;/p&gt;
&lt;h2 id=&quot;基本的背包问题&quot;&gt;&lt;a href=&quot;#基本的背包问题&quot; class=&quot;headerlink&quot; title=&quot;基本的背包问题&quot;&gt;&lt;/a&gt;基本的背包问
      
    
    </summary>
    
      <category term="算法" scheme="https://rearcher.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="背包问题" scheme="https://rearcher.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    
      <category term="动态规划" scheme="https://rearcher.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>树状数组区间更新的O(logn)实现</title>
    <link href="https://rearcher.github.io/bit-logn-range-update.html"/>
    <id>https://rearcher.github.io/bit-logn-range-update.html</id>
    <published>2018-07-25T10:47:29.000Z</published>
    <updated>2018-07-27T09:28:27.259Z</updated>
    
    <content type="html"><![CDATA[<p>树状数组常用于高效求解前缀和、区间和，复杂度都在$O(logn)$，同时支持单点更新。但是区间更新效率低下，朴素的实现需要对区间内的每一个元素实行单点更新，时间复杂度在$O(n)$。可以通过维护两个数组，来高效地实现区间更新。</p><p>假设目前有一列数据$a_1, a_2, a_3, …, a_n$，用两个树状数组来高效进行这些数据的区间更新与求和。令<code>sum(bit, i)</code>是树状数组bit的前i项和，构建两个树状数组bit0和bit1，并且假设数列${a_i}$的前i项和为</p><script type="math/tex; mode=display">\Sigma_{j = 1} ^ {i} a_j = sum(bit1, i) * i + sum(bit0, i)</script><p>其中树状数组bit0代表的是原数组，bit1代表的原数组上的区间更新量。那么在$[l, r]$区间上同时加上$x$就可以看成是：</p><blockquote><ul><li>在bit0的$l$位置上加上$-x(l - 1)$</li><li>在bit1的$l$位置上加上$x$</li><li>在bit0的$r+1$位置上加上$xr$</li><li>在bit1的$r+1$位置上加上$-x$</li></ul></blockquote><p>下面考虑三种情况，来验证上面的更新策略，假设要求数组a的前i项和：</p><ul><li>$i &lt; l$：不影响</li><li>$l \le i \le r$：相当于加上了$x * i - x(l - 1) = x * (i - l + 1)$，正确</li><li>$l &gt; r$：相当于加上了$0 * i + xr - x(l - 1) = x * (r - l + 1)$，正确</li></ul><p>基于c++的代码实现：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAX_N = <span class="number">100001</span>;</span><br><span class="line"><span class="keyword">int</span> bit0[MAX_N], bit1[MAX_N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sum</span><span class="params">(<span class="keyword">int</span>* bit, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        res += bit[i];</span><br><span class="line">        i -= i &amp; (-i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span>* bit, <span class="keyword">int</span> i, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= MAX_N) &#123;</span><br><span class="line">        bit[i] += x;</span><br><span class="line">        i += i &amp; (-i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求和[1, i]</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sum</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> sum(bit1, i) * i + sum(bit0, i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 区间更新，[from, to]的元素都加上x，时间复杂度logn</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rangeUpdate</span><span class="params">(<span class="keyword">int</span> from, <span class="keyword">int</span> to, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    add(bit0, from, -x * (from - <span class="number">1</span>));</span><br><span class="line">    add(bit1, from, x);</span><br><span class="line">    add(bit0, to + <span class="number">1</span>, x * to);</span><br><span class="line">    add(bit1, to + <span class="number">1</span>, -x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/24749842/" target="_blank" rel="noopener">《挑战程序设计竞赛》</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;树状数组常用于高效求解前缀和、区间和，复杂度都在$O(logn)$，同时支持单点更新。但是区间更新效率低下，朴素的实现需要对区间内的每一个元素实行单点更新，时间复杂度在$O(n)$。可以通过维护两个数组，来高效地实现区间更新。&lt;/p&gt;
&lt;p&gt;假设目前有一列数据$a_1, a
      
    
    </summary>
    
      <category term="算法" scheme="https://rearcher.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://rearcher.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>用python实现朴素贝叶斯分类算法</title>
    <link href="https://rearcher.github.io/naive_bayes.html"/>
    <id>https://rearcher.github.io/naive_bayes.html</id>
    <published>2018-04-24T07:08:19.000Z</published>
    <updated>2018-04-24T09:11:04.544Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>朴素贝叶斯算法主要用于分类问题，原理十分简单，主要采用后验概率最大化的方法来判定测试样本的类别。对于某个给定的测试样本$ x = \{x_1,…,x_n\} $，其类别为$y$的概率可以通过贝叶斯公式计算：</p><script type="math/tex; mode=display">P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}</script><p>其中$P(y)$是类别$y$的先验概率，$P(x_1,…,x_n|y)$是条件概率。后验概率最大化就是找出能够使得$P(y|x_1,…,x_n)$最大的类别$y$。</p><p>由于朴素贝叶斯算法假设各个特征之间相互独立，所以可以得到下面的公式：</p><script type="math/tex; mode=display">P(x_1,...,x_n|y) = \prod_{i=1}^{n}P(x_i|y)</script><p>因为$P(x_1,…,x_n)$是个不变量，所以只要考虑分子，最后朴素贝叶斯算法分类的公式可以表示为：</p><script type="math/tex; mode=display">\hat{y} = \arg\max_y P(y)\prod_{i=1}^{n}P(x_i|y)</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>《统计学习方法》中举了一个关于朴素贝叶斯算法分类的例子，但这个例子中特征是离散特征，所以需要分别对于每一类特征的每一个值计算条件概率$P(x_i | y)$。对于连续型的数值特征，比较常见的做法是假设特征符合某项概率分布，例如高斯分布、伯努利分布等。本文的实现针对连续型的数值特征，假设其符合高斯分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""朴素贝叶斯算法</span></span><br><span class="line"><span class="string">        假设条件概率p(x|y)符合高斯分布</span></span><br><span class="line"><span class="string">        为每一类的样本中特征的每一个维度拟合一个高斯分布</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练特征</span></span><br><span class="line"><span class="string">            y: 训练标签</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.X, self.y = X, y</span><br><span class="line">        self.classes = np.unique(y)</span><br><span class="line">        self.parameters = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.classes):</span><br><span class="line">            tmp_X = X[np.where(y == c)]</span><br><span class="line">            self.parameters.append([])</span><br><span class="line">            <span class="keyword">for</span> col <span class="keyword">in</span> tmp_X.T:</span><br><span class="line">                parameters = &#123;<span class="string">"mean"</span>: col.mean(), <span class="string">"var"</span>: col.var()&#125;</span><br><span class="line">                self.parameters[i].append(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_priori</span><span class="params">(self, c)</span>:</span></span><br><span class="line">        <span class="string">"""根据训练数据计算每一类的先验概率"""</span></span><br><span class="line">        tmp_X = self.X[np.where(self.y == c)]</span><br><span class="line">        <span class="keyword">return</span> len(tmp_X) / len(self.X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_likelihood</span><span class="params">(self, mean, var, x)</span>:</span></span><br><span class="line">        <span class="string">"""计算条件概率"""</span></span><br><span class="line">        eps = <span class="number">1e-4</span></span><br><span class="line">        coef = <span class="number">1.0</span> / math.sqrt(<span class="number">2.0</span> * math.pi * var + eps)</span><br><span class="line">        exponent = math.exp(-math.pow(x - mean, <span class="number">2</span>) / (<span class="number">2</span> * var + eps))</span><br><span class="line">        <span class="keyword">return</span> coef * exponent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_classify</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="string">"""采用最大后验概率来分类"""</span></span><br><span class="line">        posteriors = []</span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.classes):</span><br><span class="line">            posterior = self._calculate_priori(c)</span><br><span class="line">            <span class="keyword">for</span> feature_value, params <span class="keyword">in</span> zip(sample, self.parameters[i]):</span><br><span class="line">                likelihood = self._calculate_likelihood(params[<span class="string">'mean'</span>], params[<span class="string">'var'</span>], feature_value)</span><br><span class="line">                posterior *= likelihood</span><br><span class="line">            posteriors.append(posterior)</span><br><span class="line">        <span class="keyword">return</span> self.classes[np.argmax(posteriors)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        pred = [self._classify(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><p>下面采用sklearn自带的iris数据集测试实现的模型，可以看到分类的accuracy在0.96。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = NaiveBayes()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">model.fit(iris.data, iris.target)</span><br><span class="line">y_pred = model.predict(iris.data)</span><br><span class="line">print(<span class="string">f'accuracy = <span class="subst">&#123;accuracy_score(iris.target, y_pred)&#125;</span>'</span>) <span class="comment"># 输出 accuracy = 0.96</span></span><br></pre></td></tr></table></figure></p><p>完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/naive_bayes.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li><li><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/naive_bayes.py" target="_blank" rel="noopener">ML-From-Scratch</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;朴素贝叶斯算法主要用于分类问题，原理十分简单，主要采用后验概率最大化的方法来判定测试样本的类别。对于某个给定的测试样本$ x = \{x_1
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现k近邻、kd树搜索</title>
    <link href="https://rearcher.github.io/k_nearest_neighbors.html"/>
    <id>https://rearcher.github.io/k_nearest_neighbors.html</id>
    <published>2018-04-22T04:51:40.000Z</published>
    <updated>2018-04-22T06:27:16.298Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>k近邻算法是一种非常简单、直观的算法：给定一个数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该新的输入实例分为这个类。</p><p>这里主要涉及到两个问题：一是实例间的距离度量，二是如何找到所有训练数据集中最靠近新的输入实例的k个实例。常用的距离度量有欧氏距离（Euclidean Distance）和Minkowski距离等，本文实现的k近邻算法就采用了欧氏距离。至于如何找到靠近输入实例最近的k个实例，一种比较朴素的实现方式是对整个训练数据集进行线性扫描，维护一个大小为k的优先队列，保存距离输入实例最近的k个实例点，也俗称暴力搜索，但是当训练集非常大时，计算非常耗时，这种方法就有点捉襟见肘了。为了提高搜索的效率，可以使用特殊的数据结构来存储训练数据，以减少搜索时间，常见的有ball-tree、kd-tree等，本文通过实现kd-tree来加快k近邻的搜索。</p><h2 id="kd-tree的构建与搜索"><a href="#kd-tree的构建与搜索" class="headerlink" title="kd-tree的构建与搜索"></a>kd-tree的构建与搜索</h2><p>假设训练数据中的样本属于k维空间，kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每一个节点对应于一个k维超矩形区域。</p><p>构造kd树的算法如下：</p><blockquote><p>假设输入的训练集为T，训练集中每个实例的维度是n；</p><p>构造树节点：假设当前树节点的深度为depth, 以样本的第(depth % n)维为坐标轴，以T中所有实例的第(depth % n)维的中位数为切分点，将训练集分为t1和t2两部分。其中t1是切分点左边的数据集，t2是切分点右边的数据集。将切分点保存在当前树节点，用数据集t1递归构造当前节点的左节点，用数据集t2递归构造当前节点的右节点。当输入的数据集为空时递归终止。</p></blockquote><p>kd树的k近邻搜索算法如下：</p><blockquote><p>假设要搜索实例点x的k近邻，结果保存在L中；</p><ol><li>在kd树中找到距离x最近的叶节点：从根节点出发，递归向下访问kd树，如果x当前维的坐标小于切分点的坐标，则向左子树搜索，反之向右子树搜索，直到到达叶节点为止；</li><li><p>以当前叶节点为起点，开始递归向上搜索，直到到达根节点。对每个节点进行以下操作：</p><p>(a) 如果L中不足k个实例点，或者当前树节点保存的实例与x的距离小于L中的最大距离，用当前树节点保存的实例点替换L中距离最大的点；</p><p>(b) 计算x与当前切分轴的距离，如果此距离小于L中的最大距离，则当前树节点的另一个子节点区域可能存在更近的点，对另一个子节点递归调用k近邻搜索算法。因为是向上回退搜索，如果上一步是从左节点退到父节点，就应该对右节点进行递归搜索，另一种情况同理。</p></li></ol></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">3</span>)</span>:</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.neighbors = []  <span class="comment"># 用于保存kd树搜索过程中k个最近邻居的标签</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_vote</span><span class="params">(self, neighbors)</span>:</span></span><br><span class="line">        <span class="string">"""投票算法，选取k个邻居中出现次数最多的类别"""</span></span><br><span class="line">        counts = np.bincount(neighbors.astype(<span class="string">'int'</span>))</span><br><span class="line">        <span class="keyword">return</span> counts.argmax()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_kd_tree</span><span class="params">(self, data, depth)</span>:</span></span><br><span class="line">        <span class="string">"""建立kd树</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            data: 需要训练的数据</span></span><br><span class="line"><span class="string">            depth: 当前建立的数的深度</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            kd树的节点</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        data = np.array(data)</span><br><span class="line">        n_samples = data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> n_samples == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n_features = data.shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">            current_node = dict()</span><br><span class="line">            current_node[<span class="string">'split_axis'</span>] = depth % n_features + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            data = sorted(data, key=<span class="keyword">lambda</span> x: x[current_node[<span class="string">'split_axis'</span>]])</span><br><span class="line">            split_idx = n_samples // <span class="number">2</span></span><br><span class="line">            current_node[<span class="string">'data'</span>] = data[split_idx]</span><br><span class="line">            current_node[<span class="string">'left'</span>] = self._build_kd_tree(data[:split_idx], depth + <span class="number">1</span>)</span><br><span class="line">            current_node[<span class="string">'right'</span>] = self._build_kd_tree(data[split_idx+<span class="number">1</span>:], depth + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> current_node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_search_tree</span><span class="params">(self, root, x)</span>:</span></span><br><span class="line">        <span class="string">"""搜索kd树</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root: 开始搜索的树节点</span></span><br><span class="line"><span class="string">            x: 需要判定类别的样本</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        split_axis = root[<span class="string">'split_axis'</span>]</span><br><span class="line">        <span class="keyword">if</span> x[split_axis] &lt; root[<span class="string">'data'</span>][split_axis]:</span><br><span class="line">            self._search_tree(root[<span class="string">'left'</span>], x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._search_tree(root[<span class="string">'right'</span>], x)</span><br><span class="line"></span><br><span class="line">        heapq.heappush(self.neighbors, (<span class="number">-1</span> * euclidean_distance(x, root[<span class="string">'data'</span>][<span class="number">1</span>:]), next(counter), root[<span class="string">'data'</span>]))</span><br><span class="line">        <span class="keyword">if</span> len(self.neighbors) &gt; self.k:</span><br><span class="line">            heapq.heappop(self.neighbors)</span><br><span class="line"></span><br><span class="line">        split_dist = abs(x[split_axis - <span class="number">1</span>] - root[<span class="string">'data'</span>][split_axis])</span><br><span class="line">        neighbor_max = <span class="number">-1</span> * heapq.nsmallest(<span class="number">1</span>, self.neighbors)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> split_dist &gt; neighbor_max:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x[split_axis] &lt; root[<span class="string">'data'</span>][split_axis]:</span><br><span class="line">            self._search_tree(root[<span class="string">'right'</span>], x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._search_tree(root[<span class="string">'left'</span>], x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @run_time</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_train, y_train, X_test, kd_tree=False)</span>:</span></span><br><span class="line">        <span class="string">"""用训练集来预测测试集</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X_train: 训练特征数据</span></span><br><span class="line"><span class="string">            y_train: 训练标签数据</span></span><br><span class="line"><span class="string">            X_test: 测试特征数据</span></span><br><span class="line"><span class="string">            kd_tree: True代表使用kd树搜索，False代表使用线性扫描</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">            pred: 针对X_test的预测结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred = np.empty(X_test.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> kd_tree:</span><br><span class="line">            data = np.insert(X_train, <span class="number">0</span>, y_train, axis=<span class="number">1</span>)</span><br><span class="line">            n_features = np.array(data).shape[<span class="number">1</span>]</span><br><span class="line">            self.split_order = random.sample(range(<span class="number">1</span>, n_features), n_features - <span class="number">1</span>)</span><br><span class="line">            root = self._build_kd_tree(data, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">                <span class="comment"># print(f'processing sample &#123;i + 1&#125; / &#123;len(X_test)&#125;')</span></span><br><span class="line">                self.neighbors.clear()</span><br><span class="line">                self._search_tree(root, sample)</span><br><span class="line">                neighbors = np.array([x[<span class="number">0</span>] <span class="keyword">for</span> d, c, x <span class="keyword">in</span> self.neighbors])</span><br><span class="line">                pred[i] = self._vote(neighbors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">                <span class="comment"># print(f'processing sample &#123;i + 1&#125; / &#123;len(X_test)&#125;')</span></span><br><span class="line">                idx = np.argsort([euclidean_distance(x, sample) <span class="keyword">for</span> x <span class="keyword">in</span> X_train])[:self.k]</span><br><span class="line">                neighbors = np.array([y_train[j] <span class="keyword">for</span> j <span class="keyword">in</span> idx])</span><br><span class="line">                pred[i] = self._vote(neighbors)</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><p>这里还是采用sklearn自带的digits数据集来测试我们的k近邻算法，分别用暴力搜索和kd树的方式来测试以上的实现，运行结果如下图。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlgy1fqlem0k5cnj30co074gm8.jpg" alt="kd_result"></p><p>可以看到kd树的实现方式略占优势，数据集更大的话，kd树应该能更快。完整代码在<a href="https://github.com/rearcher/template/blob/master/ml/k_nearest_neighbors.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k近邻算法虽然简单，但是在实现过程中还是学到了些知识和技巧，例如python中的heapq优先队列是最小堆，如果想要最大堆则将队列中元素的值乘以-1即可；再比如用cProfile来分析程序的性能，发现自己实现的欧式距离计算耗时很长，更换为numpy自带的实现后程序运行速度有了很大提升。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;k近邻算法是一种非常简单、直观的算法：给定一个数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现逻辑回归</title>
    <link href="https://rearcher.github.io/logistic_regression.html"/>
    <id>https://rearcher.github.io/logistic_regression.html</id>
    <published>2018-04-18T04:56:42.000Z</published>
    <updated>2018-04-19T12:17:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>逻辑回归主要用于解决分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。<br>\begin{align}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align}</p><p>基本形式如下：<br>\begin{align}&amp; h_\theta (x) =  g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align}</p><p>可以看到逻辑回归其实就是在线性回归上套了一个$g$函数，也称为Sigmoid函数，形式如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy310ss7lj30my03k74g.jpg" alt="sigmoid"></p><p>可以看到，当$z$的值大于0的时候，$g(z)$是大于0.5的，当$z$小于0时，$g(z)$是小于0.5的。所以如果假设$h_\theta(x) \geq 0.5$时有$y = 1$（即当输出概率大于等于0.5时，判定为类别1），就等价于$\theta^T x \geq 0 \Rightarrow y = 1$。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><p>求解模型首先要了解模型的损失函数，损失函数可以用来衡量模型对数据的拟合程度。逻辑回归采用对数损失函数，如下：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = \begin{cases}  -log(h_{\theta}(x))  & \text {if y=1} \\ -log(1-h_{\theta}(x))  & \text{if y=0} \end{cases}</script><blockquote><p>当$y=1$时，即这个样本是正类：</p><ol><li>如果此时$h_\theta(x) = 1$，则对于这个样本而言是分类正确的，此时$cost=-log(1)=0$，表示对于分类正确的样本没有惩罚；</li><li>如果此时$h_\theta(x) = 0$，则对于这个样本而言是分类错误的，此时$cost=-log(0) \to \infty$，表示对于分类错误的样本要给予很大的惩罚。</li></ol><p>$y=0$时同理，不再赘述。</p></blockquote><p>可以将上述损失函数简化合并，变成如下形式：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = -y^{(i)}log(h_{\theta}(x)) - (1-y^{(i)})log(1-h_{\theta}(x))</script><p>考虑所有的样本，就变成了如下的形式：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = \frac{1}{m} \sum_{i=1}^{m} -y^{(i)}log(h_{\theta}(x)) - (1-y^{(i)})log(1-h_{\theta}(x))</script><p>逻辑回归模型的求解过程就是最小化上面的损失函数。这里还是采用梯度下降法来求解，还是用损失函数对每个参数$\theta_j$求偏导，求导过程不再涉及，参数$\theta_j$的梯度（偏导）为$\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$，梯度下降算法就是对每个参数进行如下更新：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}</script><p>为了便于实现，这里还是给出损失函数和梯度下降的向量化形式，如下：<br>\begin{align}<br>&amp; h = g(X\theta)\newline<br>&amp; J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)\newline<br>&amp; \theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})<br>\end{align}</p><h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><p>有了上面的公式，再使用python自带的numpy矩阵运算库，可以很方便地实现逻辑回归模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""逻辑回归"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        <span class="string">"""sigmoid函数"""</span></span><br><span class="line">        g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""损失函数</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据</span></span><br><span class="line"><span class="string">            y: 标签数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            J: 模型当前的损失</span></span><br><span class="line"><span class="string">            grad: 损失函数的梯度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples = len(y)</span><br><span class="line">        h = self.sigmoid(X.dot(self.theta))</span><br><span class="line">        J = (<span class="number">1</span> / n_samples) * (<span class="number">-1</span> * y.T.dot(np.log(h)) - (<span class="number">1</span> - y).T.dot(np.log(<span class="number">1</span> - h)))</span><br><span class="line">        grad = (<span class="number">1</span> / n_samples) * X.T.dot(h - y)</span><br><span class="line">        <span class="keyword">return</span> J, grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="string">"""训练模型</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据</span></span><br><span class="line"><span class="string">            y: 标签数据</span></span><br><span class="line"><span class="string">            n_iterations: 梯度下降算法迭代次数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        self.theta = np.zeros([n_features, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">            J, grad = self.costFunction(X, y)</span><br><span class="line">            self.theta -= learning_rate * grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""用模型进行预测"""</span></span><br><span class="line">        y_pred = np.round(self.sigmoid(X.dot(self.theta))).astype(int)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p><p>通过可视化训练过程中损失函数的变化，可以判断梯度下降算法是否正常工作，如下图所示：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqgu51x8afj30zk0qojry.jpg" alt="lr_gd"><br>可以看到模型的损失随着迭代次数不断下降。下面使用sklearn自带的数据集iris来粗略地测试一下我们的分类器的效果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用sklearn自带的测试数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:<span class="number">100</span>]</span><br><span class="line">y = np.array(iris.target[:<span class="number">100</span>]).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 分割训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练集训练模型，用测试集测试模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型的各项评分</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">precision = precision_score(y_test, y_pred)</span><br><span class="line">recall = recall_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">f'accuracy <span class="subst">&#123;accuracy&#125;</span>, precision <span class="subst">&#123;precision&#125;</span>, recall <span class="subst">&#123;recall&#125;</span>'</span>)</span><br></pre></td></tr></table></figure></p><p>可以看到最后模型的accuracy、precision、recall都为1。感兴趣的读者还可以使用其他数据集来测试，完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/logistic_regression.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文用梯度下降算法实现了逻辑回归模型，由于梯度下降过程中考虑了所有的训练样本，所以是批量梯度下降。当数据量很大时，应该采用随机梯度下降或者小批量梯度下降来加快训练速度。或许读者会看到有的博客或者书中求解逻辑回归模型用的是梯度上升算法，这取决于损失函数的定义。如果原始的问题转化为求某个损失函数的极小值，那么就应该采取梯度下降算法，如本文中所示；如果原始的问题转化为求某个损失函数的最大值，那么就应该用梯度上升算法。梯度上升与梯度下降算法的区别仅仅在于更新参数时是加上梯度还是减去梯度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基本形式&quot;&gt;&lt;a href=&quot;#基本形式&quot; class=&quot;headerlink&quot; title=&quot;基本形式&quot;&gt;&lt;/a&gt;基本形式&lt;/h2&gt;&lt;p&gt;逻辑回归主要用于解决分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。&lt;br&gt;\begin{alig
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现线性回归</title>
    <link href="https://rearcher.github.io/linear_regression.html"/>
    <id>https://rearcher.github.io/linear_regression.html</id>
    <published>2018-04-16T13:05:42.000Z</published>
    <updated>2018-04-19T12:00:44.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>线性回归基本形式：</p><script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n</script><p>其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &gt; 1 $时是多元线性回归。<br><a id="more"></a></p><p>在实际实现中，常常采取如下的向量化:</p><script type="math/tex; mode=display">h_\theta(X) = X \theta</script><p>其中$ X $代表的是训练数据，并且每个训练数据是一行。假设每条训练数据有n个特征（包含$x_0$），总共有m条训练数据，那么$ X $就是$m \times n$的矩阵。$ \theta $代表的是参数矩阵，因为训练数据有n个特征，所以$ \theta $是$n \times 1$的列向量。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><p>根据以上的公式，模型求解的目标就是要求出参数$\theta$。最常见的有两种方法，一种是梯度下降法，另一种是正规方程（normal equation)。</p><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>介绍梯度下降算法之前需要先介绍一下线性回归模型的损失函数，常用的平方损失函数，定义如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2</script><p>一个直观的解释是，线性回归的损失函数计算的是预测点到实际点的距离的平方和。至于为什么要除以2，是因为在计算损失函数偏导的时候，会乘以2，所以刚好能够抵消，最后系数只剩下 $\frac{1}{m}$。</p><p>损失函数的向量化形式如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><p>向量化的意义在于用向量的形式实现，就可以使用矩阵来进行运算，相比于普通的循环来说更加高效，特别是当数据量很大的时候。</p><p>在知道损失函数之后，我们的目标就转换为，<strong>求解参数$\theta$使得损失函数的值达到最小</strong>。梯度下降法就是对损失函数求每个参数$\theta_j$的偏导，偏导就称为梯度，然后通过将当前的$\theta_j$减去其梯度的这种方式来更新参数，直至算法收敛（算法收敛的条件可以是损失函数的值小于某个值）。公式如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}</script><p>其中$\alpha$是学习率，$\frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$就是$\theta_j$对应的梯度。所以梯度下降法是一个迭代的算法，每轮迭代同时更新所有参数；另外还需要注意学习率$\alpha$的选择，$\alpha$太大的话，可能导致算法不收敛，$\alpha$太小可能导致迭代次数过多，算法收敛慢。</p><p>下面是梯度下降法更新参数的向量化形式：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>梯度下降法是个迭代的算法，需要迭代很多次，而正规方程则可以直接计算出线性回归的参数。这里先举个直观的例子，求某个二次函数的极值，例如$y = x^2 - 2x + 1$，要求它的极小值，可以对$x$求导，得到$2x - 2$，令$2x - 2 = 0$，得到$x = 1$是极小值点。正规方程也是基于这个思想，回顾前文所提到的线性回归模型损失函数的向量化形式，正规方程是根据损失函数对参数矩阵$\theta$求导（这里涉及到矩阵求导，不再介绍，具体推导过程可以参考<a href="https://blog.csdn.net/ZhikangFu/article/details/51315542" target="_blank" rel="noopener">这里</a>），令求导式子等于0，直接得出参数矩阵$\theta$的值。正规方程如下：</p><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>正规方程相比于梯度下降法，不需要迭代，也不需要选择学习率$\alpha$，只需要对矩阵求逆。当特征数量很少时，可以采用<strong>normal equation</strong>的方式，更加直接；当特征数量很大时，矩阵求逆会耗费大量的时间，采用梯度下降法更佳。另外，使用<strong>normal equation</strong>时，可能出现矩阵$X^T X$不可逆的情况，可能的原因是存在多余的特征，特征与特征之间依赖比较严重，例如线性相关；或者特征数太多，远大于样本数量，此时应当删去不必要的特征或者采取正则化（Regularization）。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>线性回归算法的核心就是参数的求解，这里给出梯度下降算法的实现和正规方程的python实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">0.01</span>, gradient_descent=True)</span>:</span></span><br><span class="line">        <span class="string">"""初始化.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            n_iterations: 梯度下降算法的迭代次数.</span></span><br><span class="line"><span class="string">            learning_rate: 梯度下降算法的学习速率.</span></span><br><span class="line"><span class="string">            gradient_descent: True代表使用梯度下降算法求解，False代表使用normal equation求解.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.n_iterationns = n_iterations</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.gradient_descent = gradient_descent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""计算当前模型对于训练数据的损失(平方损失)</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据中的特征</span></span><br><span class="line"><span class="string">            y: 训练数据中的目标值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            当前模型对于训练数据的损失</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples = len(y)</span><br><span class="line">        <span class="keyword">return</span> (X.dot(self.theta) - y).T.dot(X.dot(self.theta) - y) / <span class="number">2</span> / n_samples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""用训练数据来训练模型</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据中的特征</span></span><br><span class="line"><span class="string">            y: 训练数据中的目标值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            theta: 线性回归模型的参数</span></span><br><span class="line"><span class="string">            costHistory: 如果使用梯度下降算法求解，梯度下降算法过程中模型的历史损失会被返回</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入常量1作为偏置bias特征</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        y = np.array(y).reshape(len(y), <span class="number">1</span>)</span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.theta = np.zeros(n_features).reshape(n_features, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        costHistory = np.zeros(self.n_iterationns)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.gradient_descent:</span><br><span class="line">            <span class="comment"># normal equation</span></span><br><span class="line">            self.theta = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iterationns):</span><br><span class="line">                tmp = X.T.dot(X.dot(self.theta) - y)</span><br><span class="line">                self.theta -= tmp * self.learning_rate / n_samples</span><br><span class="line">                costHistory[i] = self.computeCost(X, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.theta, costHistory</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X.dot(self.theta)</span><br></pre></td></tr></table></figure></p><p>在实现梯度算法的过程中，我们可以计算每一步当前模型的损失，用来判断梯度下降法是否正常工作，因此在上面的fit方法实现时，返回了梯度下降算法过程中模型的历史损失，可以通过绘图的方式来观察其变化，如下图所示：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqetwfiqr6j315s10475s.jpg" alt="cost_history"></p><p>可以看到模型的损失随着迭代次数的增加而不断减少，证明梯度下降算法是正常工作的。</p><p>下面使用sklearn自带的数据diabetes来测试我们的线性回归模型，用scatterplot画出数据点的分布范围，用不同的直线画出不同迭代次数时的模型拟合情况以及使用正规方程时的模型拟合情况，如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqetwfjguyj315s104q53.jpg" alt="linear_regression_result"></p><p>可以看到随着迭代次数不断增加，拟合的效果越来越好，越来越接近正规方程的拟合效果。图中的模型拟合时学习速率设置相同，都为0.05，读者还可以尝试不同学习速率导致的拟合效果变化，完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/linear_regression.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>这里有必要提一下多项式回归，其实多项式回归的本质也是线性回归，只不过添加了高维的特征。要实现多项式回归，上面的线性回归代码都无需改动，只需对特征$X$进行改动，添加高阶特征即可，感兴趣的读者可以自行实现并寻找一些训练数据来查看拟合效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文用python实现了简单的线性回归模型，实现了梯度下降算法和正规方程（normal equation）。由于梯度下降的过程中我们考虑了所有的样本点，这种梯度下降方式也称为批量梯度下降，在训练数据量较小时可以使用，但训练数据很大时会导致训练时间太长，不可取，此时可以使用随机梯度下降算法或者小批量梯度下降算法。</p><p>本文还没有涉及到模型的正则化（Regularization），用于解决模型的过拟合问题，会在以后实现其他模型的时候涉及，敬请期待！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本形式&quot;&gt;&lt;a href=&quot;#基本形式&quot; class=&quot;headerlink&quot; title=&quot;基本形式&quot;&gt;&lt;/a&gt;基本形式&lt;/h2&gt;&lt;p&gt;线性回归基本形式：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n&lt;/script&gt;&lt;p&gt;其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &amp;gt; 1 $时是多元线性回归。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng《machine learning》小结</title>
    <link href="https://rearcher.github.io/coursera-ml-summary.html"/>
    <id>https://rearcher.github.io/coursera-ml-summary.html</id>
    <published>2018-04-01T07:35:27.000Z</published>
    <updated>2018-04-19T12:13:54.652Z</updated>
    
    <content type="html"><![CDATA[<p>花了两周时间学完了吴恩达的机器学习课程，做完了所有作业。虽然编程作业大多是照着公式写代码，框架都给你搭好，几乎是保姆式的服务，但还是或多或少有所收获。因为有点基础，所以看得比较快，十一周的课程只花了两周的时间。总的来说还是值得一看，涉及到了很多方面，在此做个小结。<br><a id="more"></a></p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><p>线性回归基本形式：</p><script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n</script><p>其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &gt; 1 $时是多元线性回归。线性回归可以很方便地改成多项式回归，只需要把特征$ x_n $变换成对应的多项式特征即可。因此掌握了最基本的一元线性回归，就掌握了多元线性回归、多项式回归。</p><p>在实际实现中，常常采取如下的向量化:</p><script type="math/tex; mode=display">h_\theta(X) = X \theta</script><p>其中$ X $代表的是训练数据，并且每个训练数据是一行。假设每条训练数据有n个特征（包含$x_0$），总共有m条训练数据，那么$ X $就是$m \times n$的矩阵。$ \theta $代表的是参数矩阵，因为训练数据有n个特征，所以$ \theta $是$n \times 1$的列向量。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>线性回归的损失函数是平方损失，定义如下所示：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2</script><p>一个直观的解释是，线性回归的损失函数计算的是预测点到实际点的距离的平方和。至于为什么要除以2，是因为在计算损失函数偏导的时候，会乘以2，所以刚好能够抵消，最后系数只剩下 $\frac{1}{m}$。</p><p>损失函数的向量化形式如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><p>这里又提到了向量化，向量化的意义在于用向量的形式实现，就可以使用矩阵来进行运算，相比于普通的循环来说更加高效，特别是当数据量很大的时候。</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练就是拟合模型参数的过程，线性回归有两种求解模型参数的办法，一种是将问题转化为损失函数最小化，通过梯度下降法来不断更新要求的参数，使损失函数达到最优；另一种是用<strong>Normal Equation</strong>直接求出。</p><p>梯度下降法就是对损失函数求每个参数$\theta_j$的偏导，偏导就称为梯度，然后通过将当前的$\theta_j$减去其梯度的这种方式来更新参数，直至算法收敛（算法收敛的条件可以是损失函数的值小于某个值）。公式如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}</script><p>其中$\alpha$是学习率，$\frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$就是$\theta_j$对应的梯度。所以梯度下降法是一个迭代的算法，每轮迭代同时更新所有参数；另外还需要注意学习率$\alpha$的选择，$\alpha$太大的话，可能导致算法不收敛，$\alpha$太小可能导致迭代次数过多，算法收敛慢。</p><p>下面是梯度下降法的向量化形式：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><p><strong>Normal Equation</strong>也是求解线性回归参数的一种方法，方程如下：</p><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>相比于梯度下降法，不需要迭代，也不需要选择学习率$\alpha$，只需要对矩阵求逆。当特征数量很少时，可以采用<strong>Normal Equation</strong>的方式，更加直接；当特征数量很大时，矩阵求逆会耗费大量的时间，采用梯度下降法更佳。另外，使用<strong>Normal Equation</strong>时，可能出现矩阵$X^T X$不可逆的情况，可能的原因是存在多余的特征，特征与特征之间依赖比较严重，例如线性相关；或者特征数太多，远大于样本数量，此时应当删去不必要的特征或者采取正则化（Regularization）。</p><p><strong>Normal Equation</strong>的推导过程可见<a href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression" target="_blank" rel="noopener">这里</a>。</p><h3 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h3><p>特征归一化（Normalization）主要是为了加速梯度下降算法，能够减少梯度下降法的迭代次数。</p><script type="math/tex; mode=display">x_i := \dfrac{x_i - \mu_i}{s_i}</script><p>其中$x_i$是原始的特征，$\mu_i$是特征$x_i$的均值，$s_i$是$x_i$的范围，实际情况中也用$x_i$的标准差来替代$s_i$。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="基本形式-1"><a href="#基本形式-1" class="headerlink" title="基本形式"></a>基本形式</h3><p>逻辑回归（Logistic Regression）虽然名字上带了“回归”，但主要用于二分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。<br>\begin{align}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align}</p><p>基本形式如下：<br>\begin{align}&amp; h_\theta (x) =  g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align}</p><p>可以看到逻辑回归其实就是在线性回归上套了一个$g$函数，也称为Sigmoid函数，形式如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy310ss7lj30my03k74g.jpg" alt="sigmoid"></p><p>可以看到，当$z$的值大于0的时候，$g(z)$是大于0.5的，当$z$小于0时，$g(z)$是小于0.5的。所以如果假设$h_\theta(x) \geq 0.5$时有$y = 1$（即当输出概率大于等于0.5时，判定为类别1），就等价于$\theta^T x \geq 0 \Rightarrow y = 1$。</p><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>逻辑回归的损失函数是基于最大似然估计推导得到的，推导可见<a href="https://blog.csdn.net/ZhikangFu/article/details/51315542" target="_blank" rel="noopener">这里</a>。</p><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]</script><p>这里损失函数的意义是度量分类错误的代价，当分类正确时，损失函数中对应的部分为0。损失函数的向量化形式：<br>\begin{align}<br>&amp; h = g(X\theta)\newline<br>&amp; J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)<br>\end{align}</p><h3 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h3><p>采用梯度下降法，本质上也是对损失函数求偏导，迭代如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}</script><p>向量化形式如下：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})</script><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>由于逻辑回归是用于二分类问题，当需要应用到多分类问题时，采用的是OneVsAll策略，即对于某一类，训练其与其他类的分类器，所以n类需要训练n个分类器。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化（Regularization）主要用来过拟合问题，通常的做法是在损失函数上增加一项关于参数$\theta$的惩罚项。带正则化的逻辑回归损失函数如下：</p><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2</script><p>其中$ \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$就是惩罚项，$\lambda$是正则化系数。当$\lambda$过大时，可能造成模型欠拟合。当损失函数带有正则化时，梯度下降时对损失函数求偏导也会带有惩罚项，但需要注意不对$\theta_0$进行惩罚。</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="基本形式-2"><a href="#基本形式-2" class="headerlink" title="基本形式"></a>基本形式</h3><p>神经网络由神经元组成，单个的神经元的作用是通过若干个输入来输出一个值，如下图所示。通常会添加一个偏置单元bias作为输入。每条输入边上都有相应的参数$\theta$，神经元上有一个激活函数（课程中采用的是sigmoid函数），因此这里$h_\theta(x) = sigmoid(\theta^TX)$。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsb343j31vk10itju.jpg" alt="neuron"></p><p>神经网络由多层神经元组成，每层神经元的数量不等。基本的神经网络由输入层和输出层组成，输入层神经元的个数与特征数量相同，输出层神经元的个数与求解的具体问题相关，如果求解一个二分类问题，输出层的神经元个数就是2个。处于输入层和输出层之间网络层称之为隐层（Hidden Layer）。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsf1g2j31sg11a4d5.jpg" alt="neural_network"></p><p>神经网络求最后输出层的值采用的是前向传播算法，从前往后分别计算每一层每一个神经元上的值（用前一层神经元的输出值以及对应的权重和神经元上的激活函数来计算）。</p><h3 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h3><p>神经网络的损失函数与逻辑回归的损失函数类似，不过需要考虑网络每条边上的权重。具体形式如下：</p><script type="math/tex; mode=display">J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2</script><p>反向传播算法是用来最小化神经网络损失函数的一种算法，其本质上也是对损失函数求每条边上参数$\theta$的偏导，不过其采取从后往前一层一层计算的方式。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2ws5ui1j31uw10qdto.jpg" alt="bp_1"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrv0uwj31sw10atkb.jpg" alt="bp_2"></p><blockquote><p>训练神经网络的一般步骤：</p><ol><li>随机初始化所有参数。如果将参数初值都设为0，当反向传播时，那么所有节点的更新值都会变成一样，所以一般采用随机初始化的形式；</li><li>采用前向算法计算每一层神经元的输出值；</li><li>实现计算损失和用反向传播计算偏导的函数；</li><li>检查反向传播算法是否正常工作；</li><li>用梯度下降或者成熟的优化器来优化损失函数，例如在octave中用fmincg来函数优化损失函数，只需要传入计算损失和梯度的函数。</li></ol></blockquote><h2 id="应用机器学习的一些建议"><a href="#应用机器学习的一些建议" class="headerlink" title="应用机器学习的一些建议"></a>应用机器学习的一些建议</h2><blockquote><ol><li>将训练数据分为训练集、交叉验证集、测试集。用训练集来训练模型，交叉验证集来测试模型的性能，测试集来测试模型的泛化能力；</li><li>解决欠拟合：增加特征数量，增加多项式特征，降低正则化系数$\lambda$；</li><li>解决过拟合：增加训练数据，尝试更少的特征，增加正则化系数$\lambda$；</li><li>对于二分类问题，当正负样本数不均衡时，采用精确度accuracy来度量模型的性能是不合理的，此时应该采取准确率precision、召回率recall、F1。</li></ol></blockquote><h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="基本形式-3"><a href="#基本形式-3" class="headerlink" title="基本形式"></a>基本形式</h3><p>支持向量机要求函数输出0或者1，而不像逻辑回归是概率。</p><script type="math/tex; mode=display">h_\theta(x) =\begin{cases}    1 & \text{if} \ \Theta^Tx \geq 0 \\    0 & \text{otherwise}\end{cases}</script><h3 id="损失函数-3"><a href="#损失函数-3" class="headerlink" title="损失函数"></a>损失函数</h3><p>课程中通过逻辑回归的损失函数引出支持向量机的损失函数。对于逻辑回归，如果$y = 1$，要求$\theta^TX \geq 0$，而支持向量机则要求$\theta^TX \gg 0$。基于此，对逻辑回归的损失函数进行一定改变，就得到了支持向量机的损失函数。</p><script type="math/tex; mode=display">J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j</script><p>损失函数中的$C$是支持向量机的正则化系数，其作用相当于$\frac{1}{\lambda}$。支持向量机寻找能够区分不同类别样本点的最佳决策边界，最佳决策边界到最近的样本点的距离称为间隔，由于支持向量机的本质是最大化这个间隔，所以又称为最大间隔分类器。最大间隔只有当C很大时才会取得，如果存在某些异常点并且我们不想让这些异常点影响最大间隔的决策边界，可以适当减少C。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>不采用核函数的支持向量机（也称为采用线性核linear kernel的支持向量机）对于线性可分数据具有较好的性能，如果数据线性不可分，可以采用核函数来实现非线性支持向量机。所谓核函数，是指某种相似度函数。Andrew Ng对于于核函数的解释非常直观，在输入空间中选取几个代表性的点，将原始特征转化成与这几个代表性的点的相似度的向量作为新的特征，然后用新的特征来最小化损失函数。这样做的效果最后就是，决策边界在这几个代表性的点附近会弯曲，形成非线性的决策面。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrnthgj31t0120n3l.jpg" alt="kernel_1"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrxg9vj31wi11ik04.jpg" alt="kernel_2"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wry9unj31xy11awpa.jpg" alt="kernel_3"></p><p>课程中提到的一个核函数是高斯核函数（Gaussian），其有个$\sigma^2$参数需要设置，$\sigma^2$太大会导致新特征变化缓慢，容易造成欠拟合；$\sigma^2$过小会导致新特征变化剧烈，容易造成过拟合。所以$\sigma^2$的作用与$C$相反，与$\lambda$相同。如果使用高斯核，建议对特征进行归一化。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsvee7j31rw11g7pi.jpg" alt="gaussin_kernel_sigma"></p><p>接着上面说，实际使用过程中，对于有代表性的点的选取，常常是选择所有的输入数据。所以新的特征维数n与之前的训练数据量m相同。关于逻辑回归和支持向量机使用：</p><blockquote><ol><li>如果n远大于m，例如$n=10000$，$m = 100$，建议采用逻辑回归或者线性核支持向量机；</li><li>如果n很小，m也不大，例如$n = 100$， $m = 1000$，建议使用带高斯核的支持向量机；</li><li>如果n很小，m很大，例如$n = 100$， $m = 50000+$，建议增加更多特征，然后采用逻辑回归或者线性核支持向量机。</li></ol></blockquote><h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><p>主要介绍了聚类方法k-means和降维方法PCA。</p><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>k-means主要分为两步：</p><blockquote><ol><li>随机选取k个中心点；</li><li>将所有的数据点归类到与其距离最近的某个中心点，然后更新每一类的中心点。</li></ol></blockquote><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>PCA是将高维数据映射到低维的方法，其步骤如下：</p><blockquote><ol><li>计算协方差矩阵$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$；</li><li>计算协方差矩阵的特征向量；</li><li>选取前k个特征向量，组成$n \times k$的矩阵$Ureduce$，对于某个原始特征数据$x^{(i)}$，其降维后的特征$z^{(i)} = Ureduce^T \cdot x^{(i)}$</li></ol></blockquote><p>所以PCA的本质是将高维特征通过某个投影矩阵投影到低维。</p><p>原始特征的还原：$x_{approx}^{(i)} = U_{reduce} \cdot z^{(i)}$</p><p>如何选取k的值来构建投影矩阵？这里定义平方投影误差为$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$，选择最小的k满足如下条件：</p><script type="math/tex; mode=display">\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01</script><p>但这种方式计算非常缓慢，对于某个k，需要计算所有的还原后的特征向量。现实实现中常常采取如下方式：</p><script type="math/tex; mode=display">\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99</script><p>其中$S_{ii}$是矩阵$S$里的值，而矩阵S在octave中可以通过如下代码求出：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] = svd(Sigma)</span><br></pre></td></tr></table></figure></p><p>PCA主要的用处是对特征数据进行降维处理。或者将高维数据映射到2维或者3维使其易于可视化。不要将PCA用来解决过拟合问题，并且不要一开始就对特征进行降维处理，应该先尝试原始特征的效果。</p><h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>异常检测（Anomaly Detection）是为了检测出数据中的异常点。这里我们定义一个模型$p(x)$来告诉我们一个样本是正常样本的概率，然后用一个阈值$\epsilon$来区分正常样本和异常样本（$p(x) &lt; \epsilon$）。</p><p>我们假设特征符合高斯分布。课程中提到了两种，一种是对于特征的每一维，都符合一个一维高斯分布；另一种是多维特征符合一个多维高斯分布。多维高斯分布相比于多个一维高斯分布，能够自动地捕捉到特征之间的联系，但计算量较大。然后根据训练数据来估计高斯分布的均值和方差。对于样本点$x$，只需计算其在估计出的模型$p(x)$上的值，并与$\epsilon$比较即可。</p><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>关于推荐系统介绍了协同过滤算法。假设我们的推荐系统要用于电影的推荐，那么关于电影其实可以用一个特征向量$x$来表述，例如<code>[动作，爱情，悬疑，恐怖]</code>等，然后用户对电影的喜爱其实是由一个参数向量$\theta$决定的，这样就可以根据评分数据在已知电影特征向量的情况下，为每个用户拟合一个线性回归模型，估计出每个用户的参数向量$\theta$，这就是<strong>基于内容的推荐系统</strong>。即最小化如下损失函数：</p><script type="math/tex; mode=display">min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u}  \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2</script><p>然而很多时候，电影的特征其实不是很好定义，一部电影的动作值是多少？爱情值是多少？这时候可以采用<strong>基于用户的推荐系统</strong>， 给出用户的参数向量$\theta$（即用户对动作电影的喜欢程度是多少，对爱情电影的喜欢程度是多少等），来拟合电影的特征数据，即最小化如下损失函数：</p><script type="math/tex; mode=display">min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m}  \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2</script><p>而<strong>协同过滤算法</strong>，就是整合了基于内容的推荐系统和基于用户的推荐系统，同时求解用户参数$\theta$和电影特征$x$，其损失函数如下：</p><script type="math/tex; mode=display">J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2</script><p>参数向量$\theta$和电影特征向量$x$的初始化可以采取随机值，在参数估计完之后，想要预测某用户$j$对某部没有看过电影$i$的评分，可以用$(\theta^{(j)})^T x^{(i)}$来获得。用于电影推荐的推荐系统，在具体实现时，常常对评分矩阵进行均值归一化（即对于每部电影的评分，减去所有用户对其评分的均值），这样对于某位一部电影都没看过的用户，会将其对其他电影的评分预测为平均分，而不全都预测为0分，预测时采用$(\theta^{(j)})^T x^{(i)} + \mu_i$公式来计算。</p><h2 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h2><p>当训练数据量非常大时，传统的梯度下降法（批量梯度下降）会非常耗时，因为每次求梯度都需要遍历所有的训练数据，课程中介绍了两种替代方案：随机梯度下降和小批量梯度下降。</p><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>对样本进行随机排序，然后按序遍历样本，每次只用当前样本来计算梯度，进行梯度下降。</p><blockquote><p>对样本进行随机排序；<br>对于$i = 1 … m$，$\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j$</p></blockquote><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>每次只用小批量样本来计算梯度，假设小批量为10，算法如下：</p><blockquote><p>$For$  $i=1,11,21,31,…,991$<br>$\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>洋洋洒洒写了这么多，也算是对课程的一个回顾吧。总的来说，这门课讲得比较形象，虽然涉及到很多公式，但并没有深入去讲公式如何推导等，而是侧重于阐述直观上的理解，非常适合入门。这篇博文虽然涉及到了很多方面，但很多也只是列个公式，并没有做深入的讲解，只是列了一些课程中提到的注意点，说再多也显得晦涩，感兴趣的读者不如直接去看课程的视频吧。</p><p>后面准备开始动手实现一遍机器学习中的各种模型，加深对各个模型的理解，敬请期待！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了两周时间学完了吴恩达的机器学习课程，做完了所有作业。虽然编程作业大多是照着公式写代码，框架都给你搭好，几乎是保姆式的服务，但还是或多或少有所收获。因为有点基础，所以看得比较快，十一周的课程只花了两周的时间。总的来说还是值得一看，涉及到了很多方面，在此做个小结。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="coursera" scheme="https://rearcher.github.io/tags/coursera/"/>
    
  </entry>
  
  <entry>
    <title>从Solr源码看自动机的实现</title>
    <link href="https://rearcher.github.io/solr-automation.html"/>
    <id>https://rearcher.github.io/solr-automation.html</id>
    <published>2016-11-23T12:43:13.000Z</published>
    <updated>2018-04-02T04:11:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在公司被分配了一个任务，看一下Solr通配符查询的实现，主要是因为发现Solr对于通配符查询很慢，于是就哼哧哼哧跑去看源码了。源码看起来很累，大量调用的嵌套、委托模式，一层套一层，不过还是很有收获，今天不谈Solr具体查询的逻辑，谈一谈其查询的实现方式——自动机。对于想自己实现用自动机进行字符串匹配、正则表达式匹配的同学，还是值得一看的。(基于Solr_5.0.0)</p><h2 id="自动机的定义"><a href="#自动机的定义" class="headerlink" title="自动机的定义"></a>自动机的定义</h2><blockquote><p>An automaton is a self-operating machine, or a machine or control mechanism designed to follow automatically a predetermined sequence of operations, or respond to predetermined instructions.<br><a id="more"></a></p></blockquote><p>“自动机就是自我运行的机器，它的运行机制被设计为自动执行一些预定义好的操作序列，或者对一些预定义好的指令做出反映。”维基百科如是说。此时的自动机，仅仅是停留在机器的字面意思，指的就是物理的机器，例如上发条的机器，上了发条之后，会按照预定义好的逻辑行动。</p><h2 id="有限状态自动机"><a href="#有限状态自动机" class="headerlink" title="有限状态自动机"></a>有限状态自动机</h2><p>在计算机上运用比较普遍的就是有限状态自动机，简称状态机。所谓状态机，是一种表示若干个状态以及在这些状态之间的转移和动作等行为的数学模型。可以发现，状态自动机为什么称之为自动机，是因为其上的状态和状态之间的转移操作也都是预定义好的。而状态，在计算机程序中，就可以被抽象成很多东西，从而实现很多应用。状态自动机通常包含以下几个部分：</p><blockquote><ol><li>唯一初始状态</li><li>中间状态(可能是一个集合)</li><li>接收状态(可能是一个集合)</li><li>输入符号(可能是一个集合)</li><li>转换函数(从一个状态到另一个状态的转换)</li></ol></blockquote><p>来看一个简单的自动机：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy4yyr9czj30gi06h3yu.jpg" alt="nfa_1"><br>其中，初始状态是0，中间状态是1，接收状态是2，{a,b}是输入符号。自动机最主要的一个应用就是用于正则表达式，上面的自动机对应的正则表达式是(a|b)*ab。其实到这里，自动机如何用于正则表达式匹配，就已经可见一斑了。一般过程如下：</p><blockquote><ol><li>对正则表达式生成相应的有限状态自动机</li><li>对输入的字符串，根据字符串中的字符，和状态机上的状态转移函数，进行状态转移，直到无法进行转移，或者到达输入的字符串的末尾</li><li>无法进行转移或者到达输入字符串的末尾时，判断当前状态是不是可接收状态，如果是的话，则代表该输入的字符串被正则表达式接收，反之则不接收</li></ol></blockquote><h2 id="确定的有限状态自动机-DFA-和不确定的有限状态自动机-NFA"><a href="#确定的有限状态自动机-DFA-和不确定的有限状态自动机-NFA" class="headerlink" title="确定的有限状态自动机(DFA)和不确定的有限状态自动机(NFA)"></a>确定的有限状态自动机(DFA)和不确定的有限状态自动机(NFA)</h2><p>上面举的自动机就是一个NFA，NFA和DFA的主要区别在于：</p><blockquote><ol><li>DFA没有输入空串上的转换操作</li><li>对于DFA，一个特定的符号输入，有且只能得到一个状态，而NFA可能得到一个状态集</li></ol></blockquote><p>上面举的NFA的例子，对于状态0，输入符号a，可能得到状态1，也可能得到状态0。至于输入空串上的转换，就是输入空，可以转到下一个状态。输入空通常用希腊字母Epsilon表示，如下图中状态0到状态1的转换。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy4yyib3wj30go08q0tc.jpg" alt="此处输入图片的描述"><br>NFA和DFA具体的区别也很多，NFA也能转化成DFA，这里都不再展开了。</p><h2 id="Solr中的自动机实现"><a href="#Solr中的自动机实现" class="headerlink" title="Solr中的自动机实现"></a>Solr中的自动机实现</h2><p>好了，终于到了实现自动机的时候了。从上面的自动机概念的介绍可以发现，实现自动机重要的两个组成部分，就是状态以及状态之间的转移。Solr中状态之间的转换用Transition类来表示。Transition类如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Transition</span> </span>&#123;</span><br><span class="line">  <span class="comment">/** Sole constructor. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Transition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Source state. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> source;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Destination state. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> dest;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Minimum accepted label (inclusive). */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> min;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Maximum accepted label (inclusive). */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> max;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">int</span> transitionUpto = -<span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> source + <span class="string">" --&gt; "</span> + dest + <span class="string">" "</span> + (<span class="keyword">char</span>) min + <span class="string">"-"</span> + (<span class="keyword">char</span>) max;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到，成员变量主要有代表源状态的source，目标状态的dest，以及转换边上的label。可能会有同学觉得label的概念不是很好理解，可以看一下上面的NFA的实例图，label就是每条转换边上的字母。Solr中用min和max两个变量来指定了label的范围，而且都是int型变量，这是因为Solr会把字符转换成在Unicode字符集里的编号，即Java中的CodePoint。比如某个状态可以接受任意字符，那么用Transition类来表示的话，<code>min=Character.MIN_CODE_POINT</code>, <code>max=Character.MAX_CODE_POINT</code>。</p><p>下面就可以看一下自动机类Automaton的实现了，按照上面的思路，Automaton类里面，得包含一个整型数组，保存所有的状态，还得包含一个Transition数组，保存所有状态间的转换。但Solr源码可不是这样实现的，而是用到了一些小trick。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Index in the transitions array, where this states</span></span><br><span class="line"><span class="comment"> *  leaving transitions are stored, or -1 if this state</span></span><br><span class="line"><span class="comment"> *  has not added any transitions yet, followed by number</span></span><br><span class="line"><span class="comment"> *  of transitions. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span>[] states;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> BitSet isAccept;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Holds toState, min, max for each transition. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span>[] transitions;</span><br></pre></td></tr></table></figure></p><p>可以看到，代码中用transitions整型数组保存状态间的转移，用states整型数组保存状态信息。transitions数组用三个位置保存一个转换信息，依次是toState、min、max，这样三个一组三个一组，依次保存每一个转换的信息。你可能会好奇，为什么没有保存源状态信息，这就要说到states数组。states数组是两个一组，作为一个状态的信息，第一个位置表示该状态上的转移在transitions数组中的下标，第二个位置表示在该状态上的转移边的数量。这就容易理解了，假设我要知道状态i(假设从0开始)的第j个转移,那么直接访问<code>transitions[states[i*2] + j*3]</code>就可以了。</p><p>除此之外，我们还需要一个step函数，用于一步一步执行自动机，源码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Performs lookup in transitions, assuming determinism.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> state starting state</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> label codepoint to look up</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> destination state, -1 if no matching outgoing transition</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">step</span><span class="params">(<span class="keyword">int</span> state, <span class="keyword">int</span> label)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> state &gt;= <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">assert</span> label &gt;= <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> trans = states[<span class="number">2</span>*state];</span><br><span class="line">  <span class="keyword">int</span> limit = trans + <span class="number">3</span>*states[<span class="number">2</span>*state+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> we could do bin search; transitions are sorted</span></span><br><span class="line">  <span class="keyword">while</span> (trans &lt; limit) &#123;</span><br><span class="line">    <span class="keyword">int</span> dest = transitions[trans];</span><br><span class="line">    <span class="keyword">int</span> min = transitions[trans+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> max = transitions[trans+<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span> (min &lt;= label &amp;&amp; label &lt;= max) &#123;</span><br><span class="line">      <span class="keyword">return</span> dest;</span><br><span class="line">    &#125;</span><br><span class="line">    trans += <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到，基本逻辑就是根据输入的label，判断label在不在该状态的Transition的min和max之间，在的话，就可以跳到该Transition的dest状态。</p><p>其实到这里，这个自动机已经可以跑了，如果你想用于字符串匹配，那么就根据字符串长度构造出相等数量的状态，再根据字符串中的每个字符，添加Transition，初始化好states和transiitons数组，匹配的时候通过step函数一步一步执行即可。Solr中还考虑了很多细节，不再展开叙述。</p><h2 id="Solr中自动机的快速执行"><a href="#Solr中自动机的快速执行" class="headerlink" title="Solr中自动机的快速执行"></a>Solr中自动机的快速执行</h2><p>由于Solr对于查询的匹配，需要从索引文件中读出很多Term在自动机上进行匹配，所以能够快速执行自动机匹配能够有效降低结果返回的延迟。基本的思想还是用空间换时间。Solr中把每个Automaton类转换成RunAutomaton类，从而快速执行，看一下RunAutomaton类中的变量，就可以了解其实现方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] transitions; </span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] points; <span class="comment">// char interval start points</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] classmap; <span class="comment">// map from char number to class class</span></span><br></pre></td></tr></table></figure></p><p>先说一下这个points整型数组的作用，用于保存自动机的所有转换边上的输入，即所有转换边上输入字符的CodePoint，并且是从小到大排序好的。这里的transitions数组不同于Automaton类中的Transition数组，这里的transitions数组看似是一个一维数组，实则是当二维数组来用的，用于保存自动机上的状态经过points数组中保存的输入字符的转换能够到达的状态。如果说状态数是n，那么transitions数组的长度就是<code>n * points.length</code>。如果不存在该输入字符的转换，那么transitions数组上对应位置的元素是-1。可以看到，这样用一个很大的数组保存所有的转换，具有很大的冗余。不过优势在于，只需要生成一次RunAutomaton实例，就可以重复使用。</p><p>现在假设我们在状态i，输入字符c，想要知道下一个状态。那么首先需要从points数组中找出字符c对应的CodePoint在points数组中的下标j，那么<code>transitions[i*points.length + j]</code>即代表下一个状态。现在已经很快了，不过还有一个不足的地方，倘若points数组很大，从头到尾遍历需要很长的时间，一种方式是采用二分查找（因为是排好序的，不过Solr并未采用），另一种方式是更极端的用空间换时间，启用classmap整型数组。classmap数组保存的是某个输入字符的CodePoint在points数组中的下标，也就是说，比如输入字符c，其对应的CodePoint是d，那么<code>points[classmap[d]] = d</code>。现在只需要直接访问<code>transitiosn[i*points.length + classmap[d]]</code>就可以知道下一个状态了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在公司被分配了一个任务，看一下Solr通配符查询的实现，主要是因为发现Solr对于通配符查询很慢，于是就哼哧哼哧跑去看源码了。源码看起来很累，大量调用的嵌套、委托模式，一层套一层，不过还是很有收获，今天不谈Solr具体查询的逻辑，谈一谈其查询的实现方式——自动机。对于想自己实现用自动机进行字符串匹配、正则表达式匹配的同学，还是值得一看的。(基于Solr_5.0.0)&lt;/p&gt;
&lt;h2 id=&quot;自动机的定义&quot;&gt;&lt;a href=&quot;#自动机的定义&quot; class=&quot;headerlink&quot; title=&quot;自动机的定义&quot;&gt;&lt;/a&gt;自动机的定义&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;An automaton is a self-operating machine, or a machine or control mechanism designed to follow automatically a predetermined sequence of operations, or respond to predetermined instructions.&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="solr" scheme="https://rearcher.github.io/tags/solr/"/>
    
      <category term="automaton" scheme="https://rearcher.github.io/tags/automaton/"/>
    
  </entry>
  
  <entry>
    <title>基于Netty的HTTP代理服务器</title>
    <link href="https://rearcher.github.io/netty-http-proxy.html"/>
    <id>https://rearcher.github.io/netty-http-proxy.html</id>
    <published>2016-09-09T10:17:14.000Z</published>
    <updated>2018-04-02T03:24:16.131Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看netty，光看看书也是挺无聊的，就想着写点东西。Netty的<a href="https://github.com/netty/netty" target="_blank" rel="noopener">Github</a>上面有很多example可以学习，想到之前做的项目用Javascript写了一个http/https代理服务器来获取某些请求的参数，就想着自己再用netty来实现一下。思想很简单，实现的时候还是有很多细节的地方要注意。而且暂时https的代理还没能实现，遇到了一些问题，暂时避开这方面。<br><a id="more"></a><br>首先看一张架构图吧，代码实现基于下图的流程：<br> <img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy508njcgj30ux0ds3z2.jpg" alt="Netty http proxy"><br>有几点需要解释一下：</p><ol><li>代理服务器通过bind()操作，与本机地址的某个端口绑定，bind()操作会产生一个Channel，属于本地的Channel。这个Channel没有在上图中画出来;</li><li>代理服务器通过与客户端建立连接，为每个客户端生成一个Channel，即图中名为inbound的通道;</li><li>这个时候，客户端的请求发送到了我们的代理服务器，代理服务器要做的就是解析请求的host，然后与真正的server建立连接，此时建立的即为图中名为outbound的通道，然后将客户端的请求发送过去;</li><li>outbound中收到Server的回复时，再写入inbound通道，客户端就可以看见真正的服务器返回的数据。</li></ol><p>代码中值得一提的是，什么时候建立outbound通道？因为建立outbound通道需要知道Server的地址，所以在读取到第一个FullHttpRequest的时候时候，解析Request中host和port，然后建立。还有一点需要注意的是，outbound通道所依赖的EventLoop应该与inbound一致，这样避免了新建额外的线程，也省去了一些线程切换的开销。</p><p>完整代码在我的<a href="https://github.com/Rearcher/netty-demos/tree/master/src/main/java/com/renhuanhuan/proxy" target="_blank" rel="noopener">Github</a>上，基于maven构建，写的比较糙，欢迎交流、批评指正。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看netty，光看看书也是挺无聊的，就想着写点东西。Netty的&lt;a href=&quot;https://github.com/netty/netty&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;上面有很多example可以学习，想到之前做的项目用Javascript写了一个http/https代理服务器来获取某些请求的参数，就想着自己再用netty来实现一下。思想很简单，实现的时候还是有很多细节的地方要注意。而且暂时https的代理还没能实现，遇到了一些问题，暂时避开这方面。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="netty" scheme="https://rearcher.github.io/tags/netty/"/>
    
      <category term="maven" scheme="https://rearcher.github.io/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>网易面试小记</title>
    <link href="https://rearcher.github.io/netease-interview.html"/>
    <id>https://rearcher.github.io/netease-interview.html</id>
    <published>2016-09-03T11:36:19.000Z</published>
    <updated>2018-04-02T03:24:04.959Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间有幸拿到了网易校招内推批次的内推码，于是就抱着试一试的心态投了杭研的Java职位。首先是笔试，笔试过程也是充满了戏剧性。第一次笔试由于平台的原因导致我提前交卷，没能答好，于是就有了第二次笔试的机会。鉴于第一次笔试的经验，第二次做的比较顺利，编程题有的很快也就有了思路，所以最后笔试也是过了，还是挺高兴的。<br><a id="more"></a><br>笔试结果出来的一个多礼拜之后，赶赴杭州面试。先说结果，比较遗憾，没能通过，不过也在预料之中。当天有免费的午餐，第一次身处那么多程序员的环境，感觉还挺奇妙的。总共进行了两次技术面试，一次HR面试。感觉技术面试还是很轻松的，主要是讲一讲做过的项目，之前参加比赛的经历等，还有一些技术上的问题，跟技术人员谈话，总有莫名的亲切感，一开始的紧张也就烟消云散了。不过HR面就没那么轻松了，也是我自己的原因，之前并没有在春招的时候找公司实习，项目经验也不是很多，简历显得非常精简，入不得HR的法眼吧。我也问HR，没有实习经验怎么弥补，他说，“有些东西是弥补不了的”，确实很无奈，如果大二那时候的我有我现在的觉悟就好啦……</p><p>不过多去经历总是好的，总要一步一步来嘛。比如笔试的编程题，由于平时在LeetCode上坚持刷了一段时间，就能很快有些思路。感觉找到了些学习的路子，还是继续努力吧，积少成多，聚沙成塔，总是这么个道理。</p><p>最后分享一下技术面问的问题(Java开发工程师)：</p><ul><li>Object类有哪些方法</li><li>hashmap的实现原理，散列冲突的解决方案</li><li>文件每行包含一个字符串，对这些字符串进行排序，但是文件大小大于可用内存</li><li>多线程，线程启动的方式，线程间资源冲突怎么解决</li><li>设计模式，应用场景</li><li>网络相关，TCP建立连接、释放连接</li><li>JVM内存布局，自动内存管理，垃圾收集</li><li>Java多态实现原理</li><li>求第K个最小数</li><li>最后就是个人项目，技术面都会问</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前段时间有幸拿到了网易校招内推批次的内推码，于是就抱着试一试的心态投了杭研的Java职位。首先是笔试，笔试过程也是充满了戏剧性。第一次笔试由于平台的原因导致我提前交卷，没能答好，于是就有了第二次笔试的机会。鉴于第一次笔试的经验，第二次做的比较顺利，编程题有的很快也就有了思路，所以最后笔试也是过了，还是挺高兴的。&lt;br&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://rearcher.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="java" scheme="https://rearcher.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Java自动内存管理</title>
    <link href="https://rearcher.github.io/java-memory-management.html"/>
    <id>https://rearcher.github.io/java-memory-management.html</id>
    <published>2016-08-11T02:55:15.000Z</published>
    <updated>2018-04-02T03:23:49.347Z</updated>
    
    <content type="html"><![CDATA[<p>最近找了两本Java虚拟机方面的书，看了看其中对于Java自动内存管理的章节，写的都大同小异，在此总结一下，主要是三个方面：内存划分、内存分配、内存回收。<br><a id="more"></a></p><h2 id="内存划分（运行时数据区）"><a href="#内存划分（运行时数据区）" class="headerlink" title="内存划分（运行时数据区）"></a>内存划分（运行时数据区）</h2><p><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy51fqxsbj310u0p9dgk.jpg" alt="JVM运行时数据区"></p><p>从线程的角度来分，可分为线程私有和线程共享的，上图中左边的灰色区域就是线程共享的区域，包括堆、方法区、运行时常量池。而右边的区域则是线程私有的，包括程序计数器、虚拟机栈。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>堆是虚拟机管理的内存中最大的一块，是被线程共享的一块区域，主要用于存放对象实例，<code>但并不是所有对象都是在堆上分配的</code>。同时堆也是垃圾收集器管理的主要区域。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区与堆一样，也是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区虽然逻辑上与堆独立，但物理上属于堆。</p><h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><p>运行时常量池属于方法区的一部分，用于存放class文件中的常量池信息，主要是各种字面值和符号引用。另外，运行时常量池并不要求常量一定只能在编译期产生，运行期间也可能将新的常量放入池中，例如String类的intern()方法。</p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>类似于操作系统中的程序计数器，不过这里的程序计数器指示的是正在执行的字节码指令的地址。字节码解释器的执行完一条指令后，会改变程序计数器的值，指向下一条需要执行的指令地址。之所以需要每个线程都使用一个独立的程序计数器，是因为能够让多线程程序正确执行，各条线程之间的计数器互不影响。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>虚拟机栈描述的是Java方法执行的内存模型，其基本单位是栈帧，每个方法执行的时候都会创建一个栈帧。虚拟机一直在执行栈顶的栈帧所对应的方法，当一个方法中调用另一个方法时，就会新建一个被调用方法的栈帧，push进虚拟机栈，被调用方法执行结束，会将返回值写入调用他的栈帧，并将自己的栈帧从栈中弹出。<br>而方法的栈帧中，存放了局部变量表、操作数栈、动态链接、方法出口等信息。局部变量表所需的内存空间都是在编译器就能够确定的，用于存放方法内部的本地变量;操作数栈则是用来进行运算操作，将两个操作数从栈顶弹出，计算结果，压入栈。</p><p>还有一个没有提及的是<code>本地方法栈</code>，与虚拟机栈相似，不过是为本地方法服务的，虚拟机规范中对其没有强制规定，可由虚拟机具体实现。</p><h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><p><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy51fqgspj30k80bxgll.jpg" alt="java堆分代"></p><p>上面说到，堆是内存管理的主要区域，堆中存放了各种各样的对象，进一步可以划分为新生代和老年代。其中，新生代里有Eden空间、From Survivor空间、To Survivor空间。这样划分主要是为了方便内存回收。具体各个空间的用途，到内存回收就会知道。</p><p>从Java代码中new一个对象说起，JVM首先会检查这个new指令的参数能够在常量池中定位到一个类的符号引用，然后检查与这个符号引用相对应的类是否已经成功经历过加载、解析、初始化等步骤，当类完成装载之后，就可以完全确定创建该类实例所需要的空间大小。然后JVM就会为该实例进行内存分配。</p><p>下面就是分配在哪的问题。一般会分配在堆中的Eden空间，如果启动了本地线程分配缓冲，会优先在TLAB(Tread Local Allocation Buffer,即本地线程分配缓冲区)中分配，TLAB是Eden空间中线程私有的部分，大约占据Eden总空间的1%。 如果分配到Eden空间失败，就会进行一次新生代的垃圾收集工作。对于需要大量连续内存的大对象，会直接分配到老年代。</p><p>另外涉及到的一个概念是<code>逃逸分析</code>。上文也提到，并不是所有的对象都在堆中分配，其中有一部分对象是在栈上分配的，这里说的栈就是指虚拟机栈帧中的局部变量表部分。逃逸分析是JVM执行性能优化之前的一种分析技术，具体目标是分析出对象的作用域。如果一个对象的作用于仅限于方法体内部，就会在栈上为其分配内存，栈帧随着方法退出而销毁，不需要参与到垃圾收集中去。但一旦方法内部的对象被外部对象引用，这个对象就因此发生了逃逸，就不会在栈上分配。</p><h2 id="内存回收"><a href="#内存回收" class="headerlink" title="内存回收"></a>内存回收</h2><p>内存回收涉及到几个方面：哪些内存需要回收？什么时候回收？如何回收？</p><h3 id="可回收对象判定"><a href="#可回收对象判定" class="headerlink" title="可回收对象判定"></a>可回收对象判定</h3><p>常用的有<code>引用计数算法</code>和<code>根搜索算法</code>。</p><p>引用计数就是为每一个对象添加一个引用计数器，每当有一个地方引用它时，就将计数器的值加1，当引用失效时，计数器的值减1。任何时刻计数器值为0,说明对象不再被使用。此方法的缺陷在于，很难解决对象之间相互循环引用，如果两个需要回收的对象分别引用彼此，就无法被垃圾收集器回收。</p><p>根搜索算法通过一系列名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连，就证明此对象是不可用的。GC Roots对象包括栈帧中本地变量表中引用的对象、方法区中类静态属性引用的对象、方法区中常量引用的对象、本地方法栈中引用的对象。</p><h3 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h3><p>主要有标记-清除算法、复制算法、标记-压缩算法。</p><p>标记-清楚算法：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。标记和清除过程的效率都不高，而且标记清除之后会产生大量不连续的内存碎片，碎片太多会导致需要分配大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集操作。空闲的内存碎片可以用空闲列表来表示，从而提供下一次分配对象的内存地址。</p><p>复制算法：将内存划分为大小相等的两块，每次只使用其中的一块，当一块内存用完了，就将还活着的对象复制到另外一块上面，然后把已使用过的内存空间一次清理掉。运行高效，代价是损失了一般的内存空间。在堆中的新生代垃圾收集算法中，就使用了复制算法。将Eden空间、From Survivor空间中存活的对象复制到To Survivor空间，然后将From Survivor空间和To Survivor空间互换。(如果Eden空间和From Survivor空间的存活对象的分代年龄大于一定阈值或者To Survivor空间已满，会直接被分配到老年代）Eden空间和两个Survivor空间的缺省比例是8:1:1。之所以可以在新生代使用复制算法，是因为大多数新生代对象的生命周期都非常短暂。</p><p>标记-整理算法：与标记-清除算法差不多，不过此算法将所有存活对象向内存的一端移动，然后直接清理掉另一端的内存。此算法应用于老年代的垃圾收集。由于能够整理出一大块连续的空闲内存区域，所以用一个指针指向空闲内存区域的起点，用于指向下一次内存分配的位置。</p><h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><p>垃圾收集器有很多，而且虚拟机里整合了很多种垃圾收集器，本文不再赘述，值得一提的是<code>Stop-the-World</code>机制，通俗来说，垃圾收集进行的时候，工作线程必须停止一段时间，无论以哪种收集器进行垃圾收集，都会有或多或少的Stop-the-World时间。</p><p>另外一个是程序吞吐量与低延迟的权衡。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值。可通过<code>-XX:MaxGcPauseMillis</code>设置垃圾收集造成的Stop-the-World的时间，但为了低延迟而将该值调小之后，会导致相应的新生代内存空间变小，内存空间越小越容易被耗尽，会导致GC更加频繁，总的用于GC的时间可能反而会变多，导致程序吞吐量下降。</p><p>暂时就写这么多吧，下面是我看的两本书：</p><ul><li><a href="https://book.douban.com/subject/24722612/" target="_blank" rel="noopener">深入理解Java虚拟机</a>  </li><li><a href="https://book.douban.com/subject/26353219/" target="_blank" rel="noopener">Java虚拟机精讲</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近找了两本Java虚拟机方面的书，看了看其中对于Java自动内存管理的章节，写的都大同小异，在此总结一下，主要是三个方面：内存划分、内存分配、内存回收。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="jvm" scheme="https://rearcher.github.io/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>Java线程池的简单实现</title>
    <link href="https://rearcher.github.io/simple-java-thread-pool.html"/>
    <id>https://rearcher.github.io/simple-java-thread-pool.html</id>
    <published>2016-07-20T08:02:36.000Z</published>
    <updated>2018-04-02T03:08:34.069Z</updated>
    
    <content type="html"><![CDATA[<p>最近在写Java程序的时候，接触到一些多线程方面的东西，用到了Java中的线程池。JDK中对线程池的支持比较完善，在<code>java.util.concurrent</code>包中，用<code>ThreadPoolExecuter</code>类表示一个线程池，同时还有一个<code>Executor</code>类扮演着线程池工厂的角色。例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newSingleThreadExecutor</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newCachedThreadPool</span><span class="params">()</span></span></span><br><span class="line"><span class="function">...</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>这些工厂方法，从本质上，都是调用了<code>ThreadPoolExecutor</code>类的构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">int</span> maximumPoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">long</span> keepAliveTime,</span></span></span><br><span class="line"><span class="function"><span class="params">                    TimeUnit unit,</span></span></span><br><span class="line"><span class="function"><span class="params">                    BlockingQueue&lt;Runnable&gt; workQueue,</span></span></span><br><span class="line"><span class="function"><span class="params">                    ThreadFactory threadFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">                    RejectedExecutionHandler handler)</span></span></span><br></pre></td></tr></table></figure><p>参数含义如下：</p><ul><li>corePoolSize：线程池中应该保持的线程数量，即使线程空闲</li><li>maximumPoolSize：线程池中最大线程的数量</li><li>keepAliveTime：当线程数量大于corePoolSize时，指定空闲线程存在多久会被销毁</li><li>unit：keepAliveTime的单位</li><li>workQueue：任务队列，被提交但还没有执行</li><li>threadFactory：线程工厂</li><li>handler：拒绝策略</li></ul><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>从<code>ThreadPoolExecutor</code>的构造函数中，我们大概知道实现一个线程池需要哪些东西，如果完全按照构造函数中的参数来的话，太麻烦，有太多地方需要考虑，因此实现一个简单版的。</p><ol><li>首先需要考虑线程池中存放多少线程。可以简单用一个变量来指定，并且这些线程要放在一个容器里，便于销毁，也便于知道他们的状态。</li><li>然后我们要考虑一个作为任务队列的容器。假如线程池中有5个线程，如果5个线程都处于工作状态的话，这时候送来的任务就需要放在任务队列中等待。</li><li>最后是线程池中工作线程的形式。工作线程在创建时开始就应该启动，其所做的工作主要是：<code>从任务队列中取出任务-执行任务</code>这样的无限循环。</li></ol><p>工作线程的一种实现方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Boolean isRunning = <span class="keyword">true</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            Runnable task = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                task = taskQueue.take();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            task.run();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>WorkerThread是线程池的内部类，其中，<code>taskQueue</code>是任务队列，这里采用了<code>BlockingQueue</code>接口的一种实现，其<code>put</code>和<code>take</code>方法都是阻塞的。提交任务和销毁线程池如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">(Runnable task)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (currentWorking() &lt; limits) &#123;</span><br><span class="line">        WorkerThread worker = <span class="keyword">new</span> WorkerThread();</span><br><span class="line">        worker.start();</span><br><span class="line">        workers.add(worker);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        taskQueue.put(task);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!taskQueue.isEmpty());</span><br><span class="line">    <span class="keyword">for</span> (WorkerThread worker : workers) &#123;</span><br><span class="line">        worker.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完整代码在<a href="https://github.com/Rearcher/template/blob/master/Java/tools/SimpleThreadPool.java" target="_blank" rel="noopener">这里</a>。</p><p>线程池看似简单，其实很复杂，因为如果真要到一个应用级别的话，要考虑的东西还有很多，例如何时该启动一个线程，何时线程应该中止与挂起，任务队列的阻塞与超时，任务拒绝策略，线程生命周期等。至于本文实现的线程池，just for fun~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在写Java程序的时候，接触到一些多线程方面的东西，用到了Java中的线程池。JDK中对线程池的支持比较完善，在&lt;code&gt;java.util.concurrent&lt;/code&gt;包中，用&lt;code&gt;ThreadPoolExecuter&lt;/code&gt;类表示一个线程池，同时还有一个&lt;code&gt;Executor&lt;/code&gt;类扮演着线程池工厂的角色。例如：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newFixedThreadPool&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; nThreads)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newSingleThreadExecutor&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newCachedThreadPool&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>我理解中的JVM</title>
    <link href="https://rearcher.github.io/myjvm.html"/>
    <id>https://rearcher.github.io/myjvm.html</id>
    <published>2016-07-18T07:49:14.000Z</published>
    <updated>2018-04-02T03:08:34.070Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子amazon搞优惠活动，随便浏览了一下，发现了几本想看的书，其中一本就是<a href="https://book.douban.com/subject/26802084/" target="_blank" rel="noopener">《自己动手写Java虚拟机》</a>，一是最近正好在学java，二是对go语言比较好奇，于是就下单了。书挺薄的，300页不到，期末考完花了一个多星期的时间看完了，本文算是对这本书的一点总结吧。<br><a id="more"></a><br>全书主要讨论了以下几个方面：</p><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>顾名思义，类加载器就是把类从文件加载到内存中。Java程序通常是一个类放一个文件，并且会被编译成平台无关的字节码，成为一个个独立的class文件。类加载器工作步骤如下：</p><ol><li>获得类名，通常是<code>java/lang/Object.class</code>的形式</li><li>从<code>path/to/jre/lib</code>、<code>path/to/jre/lib/ext</code>和用户指定的<code>classpath</code>搜索类文件，找到就读入到内存中，否则出错</li><li><p>读入内存中的是原始的二进制数据，然后就需要对这一串数据进行解析，将其解析成一个结构体，大致如下：(书中先解析成class_file结构体，然后又进一步解析成了class结构体)</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ClassFile &#123;</span><br><span class="line">    u4             magic;</span><br><span class="line">    u2             minor_version;</span><br><span class="line">    u2             major_version;</span><br><span class="line">    u2             constant_pool_count;</span><br><span class="line">    cp_info        constant_pool[constant_pool_count<span class="number">-1</span>];</span><br><span class="line">    u2             access_flags;</span><br><span class="line">    u2             this_class;</span><br><span class="line">    u2             super_class;</span><br><span class="line">    u2             interfaces_count;</span><br><span class="line">    u2             interfaces[interfaces_count];</span><br><span class="line">    u2             fields_count;</span><br><span class="line">    field_info     fields[fields_count];</span><br><span class="line">    u2             methods_count;</span><br><span class="line">    method_info    methods[methods_count];</span><br><span class="line">    u2             attributes_count;</span><br><span class="line">    attribute_info attributes[attributes_count];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>最后进行链接。链接包括验证和准备阶段，验证阶段对类进行验证以确保安全性，准备阶段给类变量分配空间并且给予初值。</p></li></ol><p>这一部分比较容易，涉及到一点设计模式，例如组合模式。但有一个需要注意的地方是，数组和字符串相比于普通的类有点不一样。<br>数组的类由JVM在运行时生成，并且创建数组的方式和创建普通对象的方式不同，以及数组和普通对象存放的数据也是不同的。加载数组类的时候，需要自行完善class结构体的各个部分。<br>对于字符串，在class文件中是以MUTF-8格式保存的，在JVM运行期间，字符串被表示成java.lang.String对象的形式存在，在String对象内部，字符串又是以UTF-16格式保存的。新建字符串对象时，首先加载java.lang.String类，调用其构造函数，然后将读取到的数据以UTF-16格式存进去。</p><h2 id="运行时数据区"><a href="#运行时数据区" class="headerlink" title="运行时数据区"></a>运行时数据区</h2><p>JVM在运行的时候，需要一个地方，来存放各种各样的数据，比如类信息、各种变量等，这部分功能由运行时数据区来实现。运行时数据区可以分成两类：<strong>多线程共享的</strong>和<strong>线程私有的</strong>。</p><ol><li><p>多线程共享的运行时数据区<br>这一部分主要存放两类数据，<strong>类数据</strong>和<strong>类实例</strong>。类实例存放在堆中，类数据存放在方法区中。根据书上的图来理解的话，多线程共享的运行时数据区就是一个堆，里面包含了方法区，同时还有其他对象实例。堆由垃圾收集器定期清理。</p></li><li><p>线程私有的运行数据区<br>线程私有的运行时数据区用于辅助执行字节码。这里以线程为单位，每个线程拥有自己的JVM栈和PC寄存器，PC寄存器用来保存当前正在执行的指令的地址，JVM栈中保存了一个个JVM栈帧，一个帧中包含了一个局部变量表和操作数栈。<br><strong>操作数栈</strong>主要用来进行一些运算，有相应的指令把函数传入的参数和局部变量表中的数据推入操作数栈，进行运算时，就从操作数栈中弹出相应数量的操作数，进行运算，同时中间结果继续压入操作数栈，最后的结果会保存到局部变量表中。<br><strong>局部变量表</strong>主要用来存放局部变量的值。需要用到局部变量值的时候，会根据索引将其取出，并压入操作数栈。</p></li></ol><p>这一部分占了很多代码，一开始看很乱，来来回来看了好多遍。大致理解了一下，其实有了线程私有的运行数据区就可以执行单个方法了，只要从类数据中找到某个方法，然后直接从那部分字节码开始执行就可以。顺序执行还是跳转，都由相应的指令控制，指令会设置线程的PC寄存器。方法区是为了方法调用准备的。<br>同时这一部分还讲到了类和字段符号引用的解析。因为在定义变量的时候，需要用到很多类名和字段等。类符号引用的解析，除了得到类数据外，还要考虑当前执行代码的类是否有权限访问那个类。字段也是如此，但还要考虑继承关系，找不到的情况下要在超类中递归查找。</p><h2 id="方法调用"><a href="#方法调用" class="headerlink" title="方法调用"></a>方法调用</h2><p>方法调用是很有意思的一部分。用作者的话来说，实现了方法调用，这个JVM“从只能在地上爬的baby变成了能够到处跑”。<br>从调用的角度来看，方法可以分为静态方法和实例方法，静态方法是静态绑定的，实例方法是动态绑定的。从实现的角度来看，用Java语言实现的方法叫做Java方法，其他无法用Java语言实现而用本地语言实现的方法叫做本地方法。</p><h3 id="Java方法调用的过程"><a href="#Java方法调用的过程" class="headerlink" title="Java方法调用的过程"></a>Java方法调用的过程</h3><ol><li>调用方法首先涉及到的一个问题就是方法符号引用的解析。其中，<strong>非接口方法符号引用</strong>和<strong>接口方法符号引用</strong>的解析又是不同的。<strong>非接口方法符号引用</strong>的解析首先需要解析得到相应的类，根据方法名和描述符查找方法，考虑继承关系、接口以及访问权限。<strong>接口方法符号引用</strong>的解析则先在接口中查找，查不到再去超接口。</li><li>解析完成，就需要做<strong>方法调用和参数传递</strong>。JVM需要给这个方法创建一个新的JVM栈帧并推入栈顶，然后传递参数。传递的参数首先放在调用者的操作数栈中，只需要计算出参数的个数然后将其放到新JVM栈帧的局部变量表中就可以了。</li><li>方法执行完毕需要<strong>返回</strong>。返回指令通常把要返回的结果存入调用者的操作数栈，并且将之前新建的用于调用方法的栈帧从栈中弹出。</li></ol><h3 id="本地方法"><a href="#本地方法" class="headerlink" title="本地方法"></a>本地方法</h3><ol><li>本地方法注册，通过一个Map将key(类名+方法名+描述符)和实现函数建立一一对应的关系。</li><li>本地方法查找，从Map中查找本地方法。</li><li>本地方法调用。Java虚拟机规范并没有规定如何实现和调用本地方法，书的作者是通过定义虚拟机规范未定义的操作码指令来实现本地方法调用的。可以通过方法的访问标志来判断是不是本地方法，如果是本地方法的话，在新建方法对象放入方法区的时候就注入字节码（操作码+若干信息字节），在执行本地方法的时候，会跳到我们注入的字节码，其实就是一条指令，通过这条指令来调用本地方法。</li></ol><h2 id="指令集和解释器"><a href="#指令集和解释器" class="headerlink" title="指令集和解释器"></a>指令集和解释器</h2><p>解释器其实以上就有涉及，字节码的执行就是解释器来完成的。书的作者通过实现各种指令集来完成指令的解码。在读取字节码的过程中，先读取操作码，然后通过操作码生成新的指令，执行，计算下一条指令地址，重复上述过程，直至程序结束。书的作者实现了约200条指令，是个复杂并且需要细心耐心的工作。</p><p>总结差不多就到这吧，感谢作者。对我来说，这本书算是对JVM的一个启蒙教育吧。感觉时间越来越不够用，还是要不断学习呀。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前阵子amazon搞优惠活动，随便浏览了一下，发现了几本想看的书，其中一本就是&lt;a href=&quot;https://book.douban.com/subject/26802084/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《自己动手写Java虚拟机》&lt;/a&gt;，一是最近正好在学java，二是对go语言比较好奇，于是就下单了。书挺薄的，300页不到，期末考完花了一个多星期的时间看完了，本文算是对这本书的一点总结吧。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="go" scheme="https://rearcher.github.io/tags/go/"/>
    
      <category term="jvm" scheme="https://rearcher.github.io/tags/jvm/"/>
    
  </entry>
  
</feed>
