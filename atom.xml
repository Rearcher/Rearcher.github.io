<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Life&#39;s monolog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rearcher.github.io/"/>
  <updated>2019-03-22T14:53:45.564Z</updated>
  <id>https://rearcher.github.io/</id>
  
  <author>
    <name>Rahul</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅谈Dropout与Batch Normalization</title>
    <link href="https://rearcher.github.io/dropout-and-batch-normalization.html"/>
    <id>https://rearcher.github.io/dropout-and-batch-normalization.html</id>
    <published>2019-03-22T14:40:10.000Z</published>
    <updated>2019-03-22T14:53:45.564Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout是一种防止神经网络过拟合的技术，它是指神经网络在训练的过程中，里面的节点会以概率p失活。所以Dropout相当于同时训练了很多个子网络，最后的结果是多个模型平均的结果，增强了模型的泛化能力，因此能够解决过拟合问题。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g1bylkc068j30h2094mym.jpg" alt="dropout"></p><p>对于Dropout层，假设其中的神经元被保留的概率是p，比较直观的实现是：对于训练过程，根据概率p生成一个与输入向量X大小相同的符合伯努利分布的向量mask（里面都是0和1），然后将X与mask相乘；对于测试过程，因为测试过程没有神经元失活，所以对于某一层，其在测试过程的输出会比在训练过程大很多，所以需要进行rescale，常见的做法就是将这一层的输出再乘以p。</p><blockquote><p>为什么测试过程要把输出乘以p？<br>假设当前Dropout层某个神经元的输出是x，则训练过程中，此神经元的期望输出是$px + (1 - p)0$，如果要测试过程时保持与训练过程一致，那么就需要把此神经元的输出x变为px。</p></blockquote><p>上述直观实现的问题在于，增大了模型测试阶段的计算时间。比较常用的实现是inverted dropout，即在训练过程中进行scale，对神经元的输出除以p，来使得训练过程和测试过程神经元的输出保持一致。</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>深度学习模型训练时，往往需要尝试不同的参数初始化方法、学习率等来使得模型加速收敛。模型参数变化会导致每一层的输入分布发生改变，并且随着网络层数的增加，这种变化会加剧，因此会导致模型训练变得困难。</p><blockquote><p>由于网络中参数变化而引起数据分布发生变化的这一过程被称作Internal Covariate Shift。</p></blockquote><p>Internal Covariate Shift带来问题主要有：上层网络需要不断调整来适应数据分布的变化，导致网络学习速率降低，同时网络的训练容易陷入梯度饱和区，减缓网络收敛速度。</p><p>白化（Whiten）是机器学习中常用的一种规范化数据分布的方法，白化能够降低特征之间的相关性，并且使得特征具有相同的方差。但是在网络的每一层都进行白化操作，计算量太大，并且由于改变了网络每一层数据的分布，因此也降低了网络每层数据的表达能力。批量归一化就是为了解决普通白化的计算量大、改变数据分布的两个问题而提出来的。</p><p>深度学习模型训练往往采取mini batch的形式，批量归一化也是基于mini batch实现的。对每个batch的数据，会进行如下变换：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g1bymbhlslj30uk0kiwhf.jpg" alt="bn"></p><p>反向传播的时候，梯度根据链式法则计算：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g1bymb9009j30to0goace.jpg" alt="bn-back-propgate"></p><p>一般来说，批量归一化应用在激活函数之前；对于卷积层，如果卷积计算输出多个通道，需要分别对这些通道分别做批量归一化，且每个通道都拥有独立的参数。</p><blockquote><p>批量归一化的优点：</p><ol><li>使得网络中每层输入数据的分布相对稳定，加快模型训练速度；</li><li>使得模型对网络中的参数不再那么敏感，可以使用更大的学习率，简化调参过程，是网络学习更稳定；</li><li>具有一定的正则化效果，不同batch的均值与方差不同，相当于加入了一定的随机噪声。</li></ol></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zh.d2l.ai/chapter_convolutional-neural-networks/batch-norm.html" target="_blank" rel="noopener">动手学深度学习</a></li><li><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a></li><li><a href="http://ufldl.stanford.edu/wiki/index.php/Whitening" target="_blank" rel="noopener">Whitening</a></li><li><a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener">cs231n</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Dropout&quot;&gt;&lt;a href=&quot;#Dropout&quot; class=&quot;headerlink&quot; title=&quot;Dropout&quot;&gt;&lt;/a&gt;Dropout&lt;/h2&gt;&lt;p&gt;Dropout是一种防止神经网络过拟合的技术，它是指神经网络在训练的过程中，里面的节点会以概率p失活
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://rearcher.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>微软（苏州）暑期实习面试小记</title>
    <link href="https://rearcher.github.io/microsoft-summer-intern.html"/>
    <id>https://rearcher.github.io/microsoft-summer-intern.html</id>
    <published>2019-03-21T14:57:13.000Z</published>
    <updated>2019-03-21T15:58:06.278Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间春招都陆陆续续开始了，我也投了一些公司，反馈都比较慢，反而之后投的微软安排很快，两周之内就收到了onsite面试通知，当然是选择接受。投递的岗位是微软官网挂出来的Data Mining/Algorithm/Machine Learning Scientist Intern，因为秋招想找算法岗，所以实习也想找一找相关的，奈何现在又缺乏拿得出手的算法经历，不过好在听说微软面试只问算法题，这也是我投递微软的一个原因吧。</p><p>辗转到了苏州之后，直奔微软。在旁边随便找了家餐厅解决了午饭，随后随便逛了逛，看时间差不多就去微软前台报到了，随后面试官下来带我去面试房间。第一面是一个非常和蔼可亲的小哥，和他说话有一种很舒服的感觉，随便聊了聊简历，然后开始做算法题。因为看我简历上写了自己实现过各种机器学习算法，然后说到了线性回归，问我熟不熟悉最小二乘，我说熟悉。然后他出的第一道题目是：</p><blockquote><p>在二维平面上给出一堆点，要求用分段最小二乘的形式，求出最优的拟合方案。即可以用多条直线拟合这些点，每多增加一条直线，总的损失会增加一个常数C。</p></blockquote><p>一开始确实大脑一片空白，然后给出了一个贪心的解法，面试官提出这种解法不一定能得到最优值，然后想到了用动态规划来求解，面试官听完比较满意，然后就是手写代码。第一次手写代码偷懒，有很多不规范的地方，面试官也给我指出了。后来就是出了第二道题：</p><blockquote><p>有G个组，给出每组的人数，同时有T张桌子，每张桌子最多可以坐10个人，并且同一组的人不能坐在同一张桌子上，问如何判断是否有解？</p></blockquote><p>这道题我想了一段时间没什么思路，然后面试官提示可以往图论方向思考，我说图论不太熟，他就换了个比较简单的搜索题：</p><blockquote><p>给定二维平面上的一堆点，给定距离d，距离不超过d的点可以归为一类，要求输出总共有几类点，并且给出每一类里有哪些点。</p></blockquote><p>这道题用简单的深度优先搜索就可以求解，跟面试官说了思路之后还是要手写代码，顺利通过。然后一面所剩时间也不多了，面试官还善意地提醒了我一些要注意的点，主要是代码规范方面的问题，说有的面试官可能比较在意（后来视频三面才深切体会到这位面试官的良苦用心）。</p><p>然后是第二面，第二面的面试官有点高冷的感觉，同样是上来随便聊了聊简历，然后开始做题。二面只做了一道题：</p><blockquote><p>给定一个二维数组，代表一个滑雪场。二维数组中的数字代表滑雪场对应地方的海拔高度。规定滑雪只能从高处滑向低处（两处相等也不能滑），求最长的滑雪距离。</p></blockquote><p>咋一看是一道搜索题，跟面试官讨论了一下可以直接遍历深搜，但是时间复杂度比较大。面试官说可以先认为数据量很小，然后就让我手写了最原始的暴搜版本。接着分析了时间复杂度和空间复杂度，然后提出对当前版本的优化。思考了一下可以用记忆化搜索，面试官听了思路表示赞同，然后又是手写代码，因为是对上一版本的修修补补，所以很乱，实现的也不够优雅，有点“曲线救国”的感觉，后来又写了一版较优雅的版本，面试官看了也点头表示赞同。不知不觉二面的一个小时就这样过去了。最后问了面试官所在部门，以及实习生一般做什么工作，多久会有后续通知，onsite两面至此结束。</p><p>然后找苏州的小伙伴一起吃晚饭，路上一直觉得二面没发挥好，有点遗憾。正和小伙伴在餐厅闲聊，就收到了微软的三面邮件通知，是线上面试。三面前向之前在微软实习的学姐打听了一下，说三面是manager面，一般就是聊一下，然后也会问算法题，也会问开放性问题。我早早的来到了微软的网上面试房间，等待面试开始。后来面试官来了，听声音也是很和蔼，一直在笑，问我有没有准备英文面试，有点突然，也只好硬着头皮上，跟面试官进行了简单的英文对话，问题类似于爱好、对微软的了解之类的，现在回想起来当时答得都不太好，应该多聊几句。然后面试官给了一个链接用于在线写代码，他那边可以同步看到。面试官给的问题非常简单：</p><blockquote><p>给出年月日三个数字，要求输出是那一年的第几天。</p></blockquote><p>我三下五除二就写好了代码，面试官直接说你这个代码有问题，会crash。我找了半天，急的直冒汗，然后问他输入是否一直合法，他反问道“你说呢？”当时就有种不好的预感，平时刷算法题输入输出范围都是规定好的，突然来这么一出还确实有点不适应。然后加了输入合法性判断，并且在面试官的提醒下改了判断是否是闰年的bug。然后面试官说这个代码能用，但是不太好，还可以改进。我急的直冒汗，想了一会儿问他是不是代码风格方面的问题，他说算是吧。之后又是修修补补，他都说没改到点子上。就这样过去了40分钟，面试官说时间也不多了，第二题也来不及问了，我们就结束吧，我连忙问他我有哪些需要改进的地方（内心戏是想死的明明白白），他说是代码规范的问题，比如函数的命名、返回的类型等，并且建议我多去看一看不同语言的编码规范。</p><p>三面面完心情极度低落，但晚上六点多却收到了通过面试的邮件，不得不感叹微软效率之高，自己的运气还不错。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g1autscj3ij30ku0uxmyw.jpg" alt="interview-passed"></p><p>总的来说，微软的面试体验还是很棒的，这次面试也学到了不少东西，希望自己以后与人交流的时候能少点紧张，展现出自己应有的水平，秋招有个满意的结果吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前段时间春招都陆陆续续开始了，我也投了一些公司，反馈都比较慢，反而之后投的微软安排很快，两周之内就收到了onsite面试通知，当然是选择接受。投递的岗位是微软官网挂出来的Data Mining/Algorithm/Machine Learning Scientist Int
      
    
    </summary>
    
      <category term="求职" scheme="https://rearcher.github.io/categories/%E6%B1%82%E8%81%8C/"/>
    
    
      <category term="算法" scheme="https://rearcher.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习比赛中常用的Target Encoding</title>
    <link href="https://rearcher.github.io/ml-target-encoding.html"/>
    <id>https://rearcher.github.io/ml-target-encoding.html</id>
    <published>2019-03-17T13:10:04.000Z</published>
    <updated>2019-03-21T14:56:49.695Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做机器学习比赛的时候，遇到了Target Encoding。所谓Target Encoding，是一种特征工程方式，根据训练集中的标签信息生成特征，来提高模型的性能。比较常见的是对于二分类问题（即需要预测的标签是0和1），根据训练集中的某一列特征对训练集进行groupby操作，然后计算每个分组内标签的均值，作为新的特征。</p><p>例如下图中，根据原始特征<code>id</code>，生成了<code>id_target_mean</code>这一列均值特征。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g163u4u9cvj311y0smacr.jpg" alt="example"></p><p>上图展示的是最朴素的一种办法，这种办法非常直观，但是常常不work，带来的最明显的问题就是过拟合，训练集分数会飙升，而验证集的分数会剧烈下降。但Target Encoding确实是一种非常好的特征工程，只是需要一些额外的操作（即Regularization）来防止过拟合，下面就介绍几种带Regularization的Target Encoding方式。</p><h2 id="CV-Loop"><a href="#CV-Loop" class="headerlink" title="CV Loop"></a>CV Loop</h2><p>CV即cross validation，这种方式有点类似于交叉验证，利用交叉验证的思路来进行Target Encoding。具体地：</p><blockquote><ol><li>将训练集分成几份（例如5份）；</li><li>对于每一份训练集，该训练集上的均值特征通过在其他份训练集上进行groupby mean等操作得到；</li><li>对于测试集（或验证集），用全部的训练集进行groupby mean等操作得到。</li></ol></blockquote><p>这种方式能够很好地避免过拟合，并且通常使用4折、5折交叉验证就能够得到很好的效果。下面给出python的实现代码，也非常简单：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_kfold_mean</span><span class="params">(X_train, X_test, cols, target, folds=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X_train: 训练集</span></span><br><span class="line"><span class="string">            X_test: 测试集或验证集</span></span><br><span class="line"><span class="string">            cols: 特征列表，表示根据这些特征做groupby</span></span><br><span class="line"><span class="string">            target: 标签列名</span></span><br><span class="line"><span class="string">            folds: 交叉验证的折数</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            df: pandas.DataFrame, df.shape[0] = X_train.shape[0] + X_test.shape[0]</span></span><br><span class="line"><span class="string">                包含根据cols生成的target均值特征</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    skf = StratifiedKFold(n_splits=folds, shuffle=<span class="keyword">True</span>, random_state=<span class="number">918</span>)</span><br><span class="line">    train, test = pd.DataFrame(), pd.DataFrame()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">        train[new_col] = np.zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> tr_idx, val_idx <span class="keyword">in</span> skf.split(X_train, X_train[target]):</span><br><span class="line">        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">            new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">            tmp_means = X_val[col].map(X_tr.groupby(col)[target].mean())</span><br><span class="line">            train[new_col][val_idx] = tmp_means</span><br><span class="line">            </span><br><span class="line">    prior = X_train[target].mean()</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">        train[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">        target_map = X_train.groupby(col)[target].mean()</span><br><span class="line">        test[new_col] = X_test[col].map(target_map)</span><br><span class="line">        test[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pd.concat([train, test], axis=<span class="number">0</span>).reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>一个使用的例子：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1g164ciff3gj30x00csmy1.jpg" alt="kfold-example"></p><p>但是这种方式在极端情况下也存在leakage的风险，例如LOO（Leave One Out），只有5个样本，用5折交叉，就相当于每次都剔除当前行的样本，所以叫Leave One Out。因此也衍生出一种LOO的Target Encoding方式，即剔除当前行的target信息，python实现也很简单，如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loo_mean</span><span class="params">(X_train, X_test, cols, target)</span>:</span></span><br><span class="line">    prior = X_train[target].mean()</span><br><span class="line">    train, test = pd.DataFrame(), pd.DataFrame()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">        </span><br><span class="line">        target_sum = X_train.groupby(col)[target].transform(<span class="string">'sum'</span>)</span><br><span class="line">        n_objects = X_train.groupby(col)[target].transform(<span class="string">'count'</span>)</span><br><span class="line">        </span><br><span class="line">        train[new_col] = (target_sum - X_train[target]) / (n_objects - <span class="number">1</span>)</span><br><span class="line">        train[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line">        test[new_col] = X_test[col].map(X_train.groupby(col)[target].mean())</span><br><span class="line">        test[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> pd.concat([train, test], axis=<span class="number">0</span>).reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><h2 id="Add-Random-Noise"><a href="#Add-Random-Noise" class="headerlink" title="Add Random Noise"></a>Add Random Noise</h2><p>对Target Encoding出的特征加入随机噪声，这个不太好控制，噪声太大会让特征失效，噪声太小还是会过拟合。这种方法通常与LOO一起使用。</p><h2 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h2><p>这种方式通过超参数$\alpha$来控制regularization的程度，如果$\alpha$等于0，相当于没有regularization，如果$\alpha$等于正无穷，则相当于用global mean。通常需要与其他regularization方法一起使用。</p><script type="math/tex; mode=display">\frac{mean(target) * nrows + globalmean * alpha}{nrows + alpha}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_smooth_mean</span><span class="params">(X_train, X_test, cols, target, m=<span class="number">300</span>)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_smooth_mean_map</span><span class="params">(df, by, on, m=<span class="number">300</span>)</span>:</span></span><br><span class="line">        mean = df[on].mean()</span><br><span class="line">        agg = df.groupby(by)[on].agg([<span class="string">'count'</span>, <span class="string">'mean'</span>])</span><br><span class="line">        counts = agg[<span class="string">'count'</span>]</span><br><span class="line">        means = agg[<span class="string">'mean'</span>]</span><br><span class="line">        smooth = (counts * means + m * mean) / (counts + m)</span><br><span class="line">        <span class="keyword">return</span> smooth</span><br><span class="line">    </span><br><span class="line">    prior = X_train[target].mean()</span><br><span class="line">    train, test = pd.DataFrame(), pd.DataFrame()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">        target_map = get_smooth_mean_map(X_train, by=col, on=target, m=m)</span><br><span class="line">        train[new_col] = X_train[col].map(target_map)</span><br><span class="line">        test[new_col] = X_test[col].map(target_map).fillna(prior)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pd.concat([train, test], axis=<span class="number">0</span>).reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Expanding-mean"><a href="#Expanding-mean" class="headerlink" title="Expanding mean"></a>Expanding mean</h2><p>根据累积sum和累积count来生成均值特征。这种方式的好处在于不需要调整超参，但是生成的均值特征的质量可能有高有低，而且取决于数据的顺序（因为是用的累积sum和累积count），具体实现可看代码。CatBoost中内置了这种算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_expanding_mean</span><span class="params">(X_train, X_test, cols, target)</span>:</span></span><br><span class="line">    prior = X_train[target].mean()</span><br><span class="line">    train, test = pd.DataFrame(), pd.DataFrame()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">        new_col = col + <span class="string">'_'</span> + target + <span class="string">'_mean'</span></span><br><span class="line">        </span><br><span class="line">        cumsum = X_train.groupby(col)[target].cumsum() - X_train[target]</span><br><span class="line">        cumcnt = X_train.groupby(col)[target].cumcount()</span><br><span class="line">        train[new_col] = cumsum / cumcnt</span><br><span class="line">        train[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">        </span><br><span class="line">        test[new_col] = X_test[col].map(X_train.groupby(col)[target].mean())</span><br><span class="line">        test[new_col].fillna(prior, inplace=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> pd.concat([train, test], axis=<span class="number">0</span>).reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在实际使用过程中，没有哪一种方式是Silver Bullet，有时间可以都试一试。但总的来说，还是推荐使用CV loop或者Expanding mean。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://www.coursera.org/learn/competitive-data-science" target="_blank" rel="noopener">How to Win a Data Science Competition: Learn from Top Kagglers</a></li><li><a href="https://maxhalford.github.io/blog/target-encoding-done-the-right-way/" target="_blank" rel="noopener">Target Encoding Done The Right Way</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在做机器学习比赛的时候，遇到了Target Encoding。所谓Target Encoding，是一种特征工程方式，根据训练集中的标签信息生成特征，来提高模型的性能。比较常见的是对于二分类问题（即需要预测的标签是0和1），根据训练集中的某一列特征对训练集进行group
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现支持向量机</title>
    <link href="https://rearcher.github.io/svm-implemented-in-python.html"/>
    <id>https://rearcher.github.io/svm-implemented-in-python.html</id>
    <published>2018-12-29T14:55:24.000Z</published>
    <updated>2018-12-29T15:59:06.593Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇博客中主要整理了支持向量机的数学原理，本文主要致力于用SMO算法来求解支持向量机最后的凸二次规划问题。</p><h2 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h2><p>SMO算法的全称是Sequential minimal optimization，又名序列最小最优化算法，是用于快速求解支持向量机凸二次规划问题的算法。其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了，因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，进行求解。SMO不断将原问题分解为子问题进行求解。整个SMO算法包括两个部分，求解两个变量二次规划的解析方法和选择变量的启发式方法。</p><h3 id="两个变量二次规划问题求解"><a href="#两个变量二次规划问题求解" class="headerlink" title="两个变量二次规划问题求解"></a>两个变量二次规划问题求解</h3><p>假设选择两个变量$\alpha_1$和$\alpha_2$，其他变量$\alpha_i$是固定的，于是SMO的最优化问题的子问题可以写成：</p><script type="math/tex; mode=display">\begin{aligned}\min_{\alpha_1,\alpha_2} \quad  & W(\alpha_1, \alpha_2) =  \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1 + \alpha_2) + y_1\alpha_1\sum_{i=1}^N y_i \alpha_i K_{i1} + y_2\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{i2} \\s.t. \quad & \alpha_1 y_1 + \alpha_2 y_2 = - \sum_{i=3}^N y_i \alpha_i = \varsigma \\& 0 \leq \alpha_i \leq C, \quad i = 1,2\end{aligned}</script><p>其中$\varsigma$是常数，$K_{ji} = K(x_i, x_j)$，目标函数中省略了不含$\alpha_1$和$\alpha_2$的常数项。因为$\alpha_1 y_1 + \alpha_2 y_2 = \varsigma$是常数，所以可以化为$\alpha_1 + \alpha_1\alpha_2y_2 = y1\varsigma = C$，所以得到$\alpha_1 = C - \alpha_1\alpha_2y_2$，将此式子代入上面的目标函数，就将目标函数转为了关于单变量$\alpha_2$的优化问题。用转化后的目标函数对$\alpha_2$求偏导，可以得到</p><script type="math/tex; mode=display">\alpha_2^{new} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{K_{11} + K_{22} - 2K_{12}}</script><p>其中</p><script type="math/tex; mode=display">E_i = g(x_i) - y_i = \sum_{j=1}^N \alpha_j y_j K(x_j, x_i) + b - y_i</script><p>还有一个需要注意的地方是，$\alpha_2^{new}$必须落在区间$[L, H]$之内，即</p><script type="math/tex; mode=display">\alpha_2^{new} = \begin{cases} H, \quad \alpha_2^{new} > H \\\alpha_2^{new}, \quad L \leq \alpha_2^{new} \leq H \\L, \quad \alpha_2^{new} < L\end{cases}</script><p>如果$y_1 \not = y_2$，则</p><script type="math/tex; mode=display">L = \max(0, \alpha_2^{old} - \alpha_1^{old}), \quad H = \min(C, C + \alpha_2^{old} - \alpha_1^{old})</script><p>如果$y_1 = y_2$，则</p><script type="math/tex; mode=display">L = \max(0, \alpha_2^{old} + \alpha_1^{old} - C), \quad H = \min(C, \alpha_2^{old} + \alpha_1^{old})</script><p>至于为什么$L$和$H$是这样得到的，其实画个图就可以出来了。然后根据$\alpha_2^{new}$的值可以求得$\alpha_1^{new}$的值：</p><script type="math/tex; mode=display">\alpha_1^{new} = \alpha_1^{old} + y_1 y_2(\alpha_2^{old} - \alpha_2^{new})</script><h3 id="变量的选择"><a href="#变量的选择" class="headerlink" title="变量的选择"></a>变量的选择</h3><p>选择第一个变量的过程称为外层循环，外层循环在训练样本中选取违反KKT条件最严重的样本点，具体的，检查训练样本点是否满足KKT条件，即</p><script type="math/tex; mode=display">\begin{gathered}\alpha_i = 0 \Leftrightarrow y_i g(x_i) \geq 1 \\0 < \alpha_i < C \Leftrightarrow y_i g(x_i) = 1 \\\alpha_i = C \Leftrightarrow y_i g(x_i) \leq 1\end{gathered}</script><p>第二个变量选择使得$|E_1 - E_2|$最大的。</p><p>在每次完成两个变量的优化后，都要重新计算阈值$b$。当$0 &lt; \alpha_i^{new} &lt; C$时，由KKT条件可知：</p><script type="math/tex; mode=display">\sum_{i=1}^N \alpha_i y_i K_{i1} + b = y_1</script><p>于是可以得到</p><script type="math/tex; mode=display">b_1^{new} = y_1 - \sum_{i=3}^N \alpha_i y_i K_{i1} - \alpha_1^{new} y_1 K_{11} - \alpha_2^{new} y_2 K_{21}</script><p>又因为</p><script type="math/tex; mode=display">\begin{gathered}E_1 = \sum_{i=3}^N \alpha_i y_i K_{i1} + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_2 K{21} + b^{old} - y_i \\y_1 - \sum_{i=3}^N \alpha_i y_i K_{i1}  = -E_1 + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_2 K{21} + b^{old} \end{gathered}</script><p>所以</p><script type="math/tex; mode=display">b_1^{new} = -E_1 - y_1 K_{11}(\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{21}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p>同理</p><script type="math/tex; mode=display">b_2^{new} = -E_2 - y_1 K_{12}(\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{22}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p>如果$\alpha_1^{new}$和$\alpha_2^{new}$同时满足$0 &lt; \alpha_i^{new} &lt; C$，那么$b_1^{new} = b_2^{new}$。如果$\alpha_1^{new}$和$\alpha_2^{new}$是0或者C，那么$\alpha_1^{new}$，$\alpha_2^{new}$和它们之间的数都符合，这时选择它们的中点。在每次完成两个变量的优化之后，还需要更新$E_i$，并且要用$b^{new}$来更新，</p><script type="math/tex; mode=display">E_i^{new} = \sum_S y_j \alpha_j K(x_i, x_j) + b^{new} - y_i</script><p>其中$S$是所有支持向量$x_j$的集合。</p><h2 id="SMO算法实现"><a href="#SMO算法实现" class="headerlink" title="SMO算法实现"></a>SMO算法实现</h2><p>下面用python来实现SMO算法求解支持向量机。本文实现的是一个带高斯核的支持向量机，代码架构大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_rbf_kernel</span><span class="params">(self, x1, x2, gamma)</span>:</span></span><br><span class="line">        <span class="string">"""径向基核函数"""</span></span><br><span class="line">        distance = np.linalg.norm(x1 - x2) ** <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> np.exp(-gamma * distance)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_cal_E_i</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">"""计算Ei，注意只用到支持向量"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_select_j</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">"""SMO算法的第二层循环，根据选择的第一个点来选择第二个点"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update</span><span class="params">(self, i, j)</span>:</span></span><br><span class="line">        <span class="string">"""两个变量的二次规划算法"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, C=<span class="number">1.0</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>, n_iteration=<span class="number">1000</span>, tol=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对数据进行拟合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 特征矩阵</span></span><br><span class="line"><span class="string">            y: 标签矩阵</span></span><br><span class="line"><span class="string">            C: 对误差的惩罚项</span></span><br><span class="line"><span class="string">            gamma: rbf核函数的参数</span></span><br><span class="line"><span class="string">            n_iteration: 迭代次数</span></span><br><span class="line"><span class="string">            tol: 停止训练的误差精度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""预测"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>对于支持向量机的求解，也就是fit函数，其代码主要分为以下几个步骤：</p><blockquote><ol><li>外层循环，选取第一个点</li><li>内层循环，根据第一个点选取第二个点</li><li>根据选取的两个点进行二变量问题的求解，更新参数</li></ol></blockquote><p>外层循环本文采取简单的遍历，内层循环采取上文所说的选择使得$|E_1 - E_2|$最大的样本，具体实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_select_j</span><span class="params">(self, i)</span>:</span></span><br><span class="line">    <span class="string">"""SMO算法的第二层循环，根据选择的第一个点来选择第二个点"""</span></span><br><span class="line">    max_delta_E, j = <span class="number">0</span>, <span class="number">-1</span></span><br><span class="line">    n, _ = self.X.shape</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> k == i:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        delta_E = abs(self.E[i, <span class="number">0</span>] - self.E[k, <span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> delta_E &gt; max_delta_E:</span><br><span class="line">            max_delta_E = delta_E</span><br><span class="line">            j = k</span><br><span class="line">    <span class="keyword">return</span> j</span><br></pre></td></tr></table></figure></p><p>二变量规划问题的求解，则按照上文所说的步骤一步步来，具体实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update</span><span class="params">(self, i, j)</span>:</span></span><br><span class="line">    <span class="string">"""两个变量的二次规划算法"""</span></span><br><span class="line">    alpha_i_old, alpha_j_old = self.alpha[i, <span class="number">0</span>], self.alpha[j, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> self.y[i, <span class="number">0</span>] != self.y[j, <span class="number">0</span>]:</span><br><span class="line">        L = max(<span class="number">0</span>, self.alpha[j, <span class="number">0</span>] - self.alpha[i, <span class="number">0</span>])</span><br><span class="line">        H = min(self.C, self.C + self.alpha[j, <span class="number">0</span>] - self.alpha[i, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        L = max(<span class="number">0</span>, self.alpha[i, <span class="number">0</span>] + self.alpha[j, <span class="number">0</span>] - self.C)</span><br><span class="line">        H = min(self.C, self.alpha[i, <span class="number">0</span>] + self.alpha[j, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> L == H:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    eta = self.K[i, i] + self.K[j, j] - <span class="number">2</span> * self.K[i, j]</span><br><span class="line">    <span class="keyword">if</span> eta &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    alpha_j_new = alpha_j_old + self.y[j, <span class="number">0</span>] * (self.E[i, <span class="number">0</span>] - self.E[j, <span class="number">0</span>]) / eta</span><br><span class="line">    <span class="comment"># 更新第一个变量 </span></span><br><span class="line">    <span class="keyword">if</span> alpha_j_new &gt; H:</span><br><span class="line">        self.alpha[j, <span class="number">0</span>] = H</span><br><span class="line">    <span class="keyword">elif</span> alpha_j_new &lt; L:</span><br><span class="line">        self.alpha[j, <span class="number">0</span>] = L</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.alpha[j, <span class="number">0</span>] = alpha_j_new</span><br><span class="line">    <span class="keyword">if</span> abs(self.alpha[j, <span class="number">0</span>] - alpha_j_old) &lt; <span class="number">1e-5</span>: <span class="comment"># 更新量太小</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新第二个变量 </span></span><br><span class="line">    self.alpha[i, <span class="number">0</span>] = alpha_i_old + self.y[i, <span class="number">0</span>] * self.y[j, <span class="number">0</span>] * (alpha_j_old - self.alpha[j, <span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 更新b</span></span><br><span class="line">    b1 = self.b - self.E[i, <span class="number">0</span>] - self.y[i, <span class="number">0</span>] * self.K[i, i] * (self.alpha[i, <span class="number">0</span>] - alpha_i_old) \</span><br><span class="line">            - self.y[j, <span class="number">0</span>] * self.K[j, i] * (self.alpha[j, <span class="number">0</span>] - alpha_j_old)</span><br><span class="line">    b2 = self.b - self.E[j, <span class="number">0</span>] - self.y[i, <span class="number">0</span>] * self.K[i, j] * (self.alpha[i, <span class="number">0</span>] - alpha_i_old) \</span><br><span class="line">            - self.y[j, <span class="number">0</span>] * self.K[j, j] * (self.alpha[j, <span class="number">0</span>] - alpha_j_old)</span><br><span class="line">    <span class="keyword">if</span> self.alpha[i, <span class="number">0</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> self.alpha[i, <span class="number">0</span>] &lt; self.C:</span><br><span class="line">        self.b = b1</span><br><span class="line">    <span class="keyword">elif</span> self.alpha[j, <span class="number">0</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> self.alpha[j, <span class="number">0</span>] &lt; self.C:</span><br><span class="line">        self.b = b2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.b = (b1 + b2) / <span class="number">2</span></span><br><span class="line">    <span class="comment"># 更新E</span></span><br><span class="line">    self.E[i, <span class="number">0</span>] = self._cal_E_i(i)[<span class="number">0</span>]</span><br><span class="line">    self.E[j, <span class="number">0</span>] = self._cal_E_i(j)[<span class="number">0</span>]        </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>最后fit函数的具体实现为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, C=<span class="number">1.0</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>, n_iteration=<span class="number">1000</span>, tol=<span class="number">1e-3</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对数据进行拟合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X: 特征矩阵</span></span><br><span class="line"><span class="string">        y: 标签矩阵</span></span><br><span class="line"><span class="string">        C: 对误差的惩罚项</span></span><br><span class="line"><span class="string">        gamma: rbf核函数的参数</span></span><br><span class="line"><span class="string">        n_iteration: 迭代次数</span></span><br><span class="line"><span class="string">        tol: 停止训练的误差精度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n, m = X.shape</span><br><span class="line">    <span class="keyword">if</span> gamma == <span class="string">'auto'</span>: <span class="comment"># 初始化gamma</span></span><br><span class="line">        self.gamma = <span class="number">1</span> / m</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算Gram相似性矩阵</span></span><br><span class="line">    self.K = np.zeros((n, n)) </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, n):</span><br><span class="line">            self.K[i, j] = self.K[j, i] = self._rbf_kernel(X[i], X[j], self.gamma)</span><br><span class="line">    self.X, self.y = X, y</span><br><span class="line">    self.C, self.tol = C, tol</span><br><span class="line">    <span class="comment"># alpha和b都初始化为0</span></span><br><span class="line">    self.alpha, self.b = np.zeros((n, <span class="number">1</span>)), <span class="number">0</span></span><br><span class="line">    <span class="comment"># 初始化E</span></span><br><span class="line">    self.E = (np.dot((self.alpha * y).T, self.K) + self.b).reshape(<span class="number">-1</span>, <span class="number">1</span>) - y</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iteration):</span><br><span class="line">        pairs_changed = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            Ei = self.E[i, <span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 选取不满足KKT条件的第一个点</span></span><br><span class="line">            <span class="keyword">if</span> ((y[i, <span class="number">0</span>] * Ei &lt; -tol) <span class="keyword">and</span> (self.alpha[i, <span class="number">0</span>] &lt; C)) <span class="keyword">or</span> ((y[i, <span class="number">0</span>] * Ei &gt; tol) <span class="keyword">and</span> (self.alpha[i, <span class="number">0</span>] &gt; <span class="number">0</span>)):</span><br><span class="line">                j = self._select_j(i)</span><br><span class="line">                <span class="keyword">if</span> j != <span class="number">-1</span>:</span><br><span class="line">                    pairs_changed += self._update(i, j)</span><br><span class="line">        <span class="keyword">if</span> pairs_changed == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">f'no more pairs to change, stop at iteration <span class="subst">&#123;iteration + <span class="number">1</span>&#125;</span>'</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><p>最后用数据测试本文支持向量机的实现，在默认参数的情况下，训练集和测试集的Precision都能达到0.98以上，完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/support_vector_machine.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文实现了SMO算法求解带高斯核的支持向量机，采取的两个变量的选择方法是《统计学习方法》一书中提到的简化版，所以还有待改进。另外测试算法性能的时候发现，参数C和参数gamma对性能的影响还是挺大的，它们的默认值参考了sklearn的支持向量机实现，分别是1.0和1/n_features。参数C是对支持向量机对误差的容忍度，比较好理解，而gamma直观来说会影响高斯核的分布情况，但通常情况下为什么要设置成这样一个值，还有待探究。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li><li><a href="https://book.douban.com/subject/24703171/" target="_blank" rel="noopener">《机器学习实战》</a></li><li><a href="https://zh.wikipedia.org/wiki/%E5%BA%8F%E5%88%97%E6%9C%80%E5%B0%8F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">维基百科-序列最小优化算法</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一篇博客中主要整理了支持向量机的数学原理，本文主要致力于用SMO算法来求解支持向量机最后的凸二次规划问题。&lt;/p&gt;
&lt;h2 id=&quot;SMO算法&quot;&gt;&lt;a href=&quot;#SMO算法&quot; class=&quot;headerlink&quot; title=&quot;SMO算法&quot;&gt;&lt;/a&gt;SMO算法&lt;/h
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机原理整理</title>
    <link href="https://rearcher.github.io/svm-fundamentals.html"/>
    <id>https://rearcher.github.io/svm-fundamentals.html</id>
    <published>2018-12-28T06:28:32.000Z</published>
    <updated>2018-12-29T16:01:46.749Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。</p><h2 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h2><p>支持向量机的求解过程用到了拉格朗日对偶性。考虑原始的带约束的最优化问题，具有如下形式：</p><script type="math/tex; mode=display">\begin{aligned} \min_w \quad & f(w) \\s.t. \quad & g_i(w) \leq 0, \, \, i=1,\cdots,k \\& h_i(w) = 0, \, \, i=1,\cdots,l.\end{aligned}</script><p>为了解决上述问题，定义广义的拉格朗日函数：</p><script type="math/tex; mode=display">\mathcal{L}(w,\alpha,\beta) = f(w) + \sum_{i=1}^k\alpha_ig_i(w) + \sum_{i=1}^l \beta_ih_i(w)</script><p>其中的$\alpha_i$和$\beta_i$称作拉格朗日乘子，然后考虑$w$的函数：</p><script type="math/tex; mode=display">\theta_{\mathcal{P}}(w) = \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)</script><p>其中的下标$\mathcal{P}$代表英文中的“primal”，意为原始问题。如果$w$满足限制条件的话，可以发现$\theta_{\mathcal{P}}(w) = f(w)$，因此最初的带约束的最优化问题等价于下面的式子：</p><script type="math/tex; mode=display">\min_w \theta_{\mathcal{P}} (w) = \min_w \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)</script><p>下面定义对偶优化问题：</p><script type="math/tex; mode=display">\theta_{\mathcal{D}}(\alpha, \beta) = \min_w \mathcal{L}(w, \alpha, \beta)</script><script type="math/tex; mode=display">\max_{\alpha,\beta,\alpha_i \geq 0} \theta_{\mathcal{D}}(\alpha, \beta) = \max_{\alpha,\beta,\alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta)</script><p>对偶问题和原始问题仅仅交换了$\min$和$\max$的顺序。由于一个函数的最小值的最大值（max min）一定小于等于其最大值的最小值（min max），所以有：</p><script type="math/tex; mode=display">\max_{\alpha,\beta,\alpha_i \geq 0} \min_w \mathcal{L}(w, \alpha, \beta) \leq \min_w \max_{\alpha,\beta:\alpha_i \geq 0} \mathcal{L}(w, \alpha, \beta)</script><p>然而在满足约束条件时，它们两者是相等的，所以可以通过求解对偶问题来求解原始问题。</p><p>原始问题和对偶问题的解$\alpha^\star, \beta^\star, w^\star$满足<strong>KKT条件</strong>，同时满足<strong>KKT条件</strong>的$\alpha^\star, \beta^\star, w^\star$也是原始问题和对偶问题的解，<strong>KKT条件</strong>如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial w_i} \mathcal{L}(w^\star, \alpha^\star, \beta^\star) & = 0, \quad i = 1,\cdots,n \\\frac{\partial}{\partial \beta_i} \mathcal{L}(w^\star, \alpha^\star, \beta^\star) & = 0, \quad i = 1,\cdots,l \\\alpha_i^\star g_i(w^\star) & = 0, \quad i = 1,\cdots,k \\g_i(w^\star) & \leq 0, \quad i = 1,\cdots,k \\\alpha^\star & \geq 0,  \quad i = 1,\cdots,k\end{aligned}</script><h2 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><p>给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为$w^\star \cdot x + b^\star = 0$，以及相应的分类决策函数$f(x) = sign(w^\star \cdot x + b^\star)$称为线性可分支持向量机。</p><p><strong>函数间隔</strong>：函数间隔可以表示分类预测的正确性及确信度。对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i, y_i)$的函数间隔为</p><script type="math/tex; mode=display">\hat{\gamma_i} = y_i(w \cdot x_i + b)</script><p>定义超平面$(w, b)$关于训练数据集T的函数间隔为超平面$(w, b)$关于T中所有样本点$(x_i, y_i)$的函数间隔之最小值，即</p><script type="math/tex; mode=display">\hat{\gamma} = \min_{i=1,\cdots,N} \hat{\gamma_i}</script><p><strong>几何间隔</strong>：函数间隔的问题在于，如果同时将$w$和$b$扩大两倍，函数间隔会变成原来的两倍，但是超平面却没有改变。此时可以对超平面的法向量$w$加一些约束，如规范化，$|w| = 1$，此时间隔就是确定的，这时函数间隔就变成了几何间隔。对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i, y_i)$的几何间隔为</p><script type="math/tex; mode=display">\gamma_i = y_i(\frac{w}{\|w\|} \cdot x_i + \frac{b}{\|w\|})</script><p>定义超平面$(w, b)$关于训练数据集T的几何间隔为超平面$(w, b)$关于T中所有样本点$(x_i, y_i)$的几何间隔之最小值，即</p><script type="math/tex; mode=display">\gamma = \min_{i=1,\cdots,N} \gamma_i</script><p>函数间隔与几何间隔的关系：</p><script type="math/tex; mode=display">\gamma_i = \frac{\hat{\gamma_i}}{\|w\|}, \quad \gamma = \frac{\hat{\gamma}}{\|w\|}</script><p>线性可分支持向量机的求解就是要求几何间隔最大，可以表示成如下的最优化问题：</p><script type="math/tex; mode=display">\begin{aligned}\max_{w,b} \quad & \gamma \\s.t. \quad & y_i(\frac{w}{\|w\|} \cdot x_i + \frac{b}{\|w\|}) \geq \gamma, \quad i=1,2,\cdots,N\end{aligned}</script><p>考虑到几何间隔和函数间隔的关系，可以改写成如下形式：</p><script type="math/tex; mode=display">\begin{aligned}\max_{w,b} \quad & \frac{\hat{\gamma}}{\|w\|} \\s.t. \quad & y_i(w \cdot x_i + b) \geq \hat{\gamma}, \quad i=1,2,\cdots,N\end{aligned}</script><p>注意到函数间隔$\hat{\gamma}$的取值并不影响最优化问题的解，所以可以取1，并且最大化$\frac{1}{|w|}$等价于最小化$\frac{1}{2}|w|^2$，所以得到如下形式的凸二次规划问题：</p><script type="math/tex; mode=display">\begin{aligned}\min_{w,b} \quad & \frac{1}{2}\|w\|^2 \\s.t. \quad & y_i(w \cdot x_i + b) - 1 \geq 0, \quad i=1,2,\cdots,N\end{aligned}</script><p>该凸二次规划问题的拉格朗日函数为：</p><script type="math/tex; mode=display">\mathcal{L}(w, b, \alpha) = \frac{1}{2}\|w\|^2 - \sum_{i=1}^N \alpha_i y_i (w \cdot x_i + b) + \sum_{i=1}^N \alpha_i</script><p>根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题，因此需要先求$\mathcal{L}(w,b,\alpha)$对$w$和$b$的极小，再求对$\alpha$的极大。求$w$和$b$的极小通过偏导为0来求得：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_w \mathcal{L}(w, b, \alpha) & = w - \sum_{i=1}^N \alpha_i y_i x_i = 0 \\\nabla_b \mathcal{L}(w, b, \alpha) & = - \sum_{i=1}^N \alpha_i y_i = 0\end{aligned}</script><p>将求导结果代入其拉格朗日函数，可以得到：</p><script type="math/tex; mode=display">\mathcal{L}(w, b, \alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j(x_i \cdot x_j)</script><p>然后对其求极大，转化为下面的对偶问题：</p><script type="math/tex; mode=display">\begin{aligned}\max_\alpha  \quad & \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j(x_i \cdot x_j) \\s.t. \quad & \sum_{i=1}^N \alpha_i y_i = 0 \\& \alpha_i \geq 0, \, \, i = 1,2,\cdots,N\end{aligned}</script><p>然后将对偶问题转化为凸优化问题，便于求解：</p><script type="math/tex; mode=display">\begin{aligned}\min_\alpha  \quad & \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j(x_i \cdot x_j) - \sum_{i=1}^N \alpha_i\\s.t. \quad & \sum_{i=1}^N \alpha_i y_i = 0 \\& \alpha_i \geq 0, \, \, i = 1,2,\cdots,N\end{aligned}</script><h2 id="软间隔最大化"><a href="#软间隔最大化" class="headerlink" title="软间隔最大化"></a>软间隔最大化</h2><p>某些样本点无法满足函数间隔大于等于1的约束条件，此时可以引入松弛变量$\xi$，这样约束条件就变为$y_i(w \cdot x_i + b) \geq 1 - \xi_i$，同时对每个松弛变量$\xi_i$需要支付一定的代价，求解软间隔最大化就变成下述凸二次规划问题：</p><script type="math/tex; mode=display">\begin{aligned}\min_{w,b,\xi} \quad & \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i \\s.t. \quad & y_i(w \cdot x_i = b) \geq 1 - \xi_i, \quad i = 1,2,\cdots,N \\& \xi_i \geq 0, \quad i = 1,2,\cdots,N\end{aligned}</script><p>可以采取相同的方式，转化为对偶问题来求解（先列出其拉格朗日方程，使其极小化，分别对$w$、$b$、$\xi_i$求偏导，使它们的偏导为0，然后代入拉格朗日方程，再使其极大化，然后转换成凸二次规划问题，就得到了下面的形式）：</p><script type="math/tex; mode=display">\begin{aligned}\min_\alpha \quad & \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j(x_i \cdot x_j) - \sum_{i=1}^N \alpha_i \\s.t. \quad & \sum_{i=1}^N \alpha_i y_i = 0 \\& 0 \leq \alpha_i \leq C, \quad i=1,2,\cdots,N\end{aligned}</script><h2 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h2><script type="math/tex; mode=display">\begin{gathered}L(y(w \cdot x + b)) = [1 - y(w \cdot x + b)]_+ \\[z]_+= \begin{cases} z, &z > 0 \\ 0, &z \leq 0 \end{cases}\end{gathered}</script><p>线性支持向量机的原始最优化问题，等价于以下最优化问题（具体证明可看《统计学习方法》）：</p><script type="math/tex; mode=display">\min_{w,b} \quad \sum_{i=1}^N[1 - y_i(w \cdot x_i + b)]_+ + \lambda \|w\|^2</script><h2 id="带核函数非线性支持向量机"><a href="#带核函数非线性支持向量机" class="headerlink" title="带核函数非线性支持向量机"></a>带核函数非线性支持向量机</h2><p>所谓核函数，具有如下的形式：</p><script type="math/tex; mode=display">K(x,z) = \phi(x) \cdot \phi(z)</script><p>其中$\phi(x)$为映射函数，$\phi(x) \cdot \phi(z)$是$\phi(x)$和$\phi(z)$的内积。核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$，而不显示地定义映射函数$\phi$。核函数需要满足一定的条件。</p><p>在求解支持向量机的对偶问题中，只用到了实例与实例之间的内积，因此可以将核技巧应用在支持向量机中，此时求解的对偶问题变成：</p><script type="math/tex; mode=display">\begin{aligned}\min_\alpha \quad & \frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(x_i \cdot x_j) - \sum_{i=1}^N \alpha_i \\s.t. \quad & \sum_{i=1}^N \alpha_i y_i = 0 \\& 0 \leq \alpha_i \leq C, \quad i=1,2,\cdots,N\end{aligned}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文梳理了支持向量机的数学原理，最后都是转化为一个凸二次规划问题来求解。解决凸二次规划问题有现成的优化工具，但针对于支持向量机的求解还专门有一个名叫SMO的算法，会在下篇用python实现支持向量机的博客中详解。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li><li><a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener">Support Vector Machines - CS229</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;支持向量机是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。&lt;/p&gt;
&lt;h2 id=&quot;拉
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现决策树</title>
    <link href="https://rearcher.github.io/decision-tree.html"/>
    <id>https://rearcher.github.io/decision-tree.html</id>
    <published>2018-12-14T14:57:21.000Z</published>
    <updated>2019-02-12T10:32:43.942Z</updated>
    
    <content type="html"><![CDATA[<h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p>随机变量X的<strong>熵</strong>定义为：</p><script type="math/tex; mode=display">H(X) = -\sum_{i=1}^n p_i \log p_i</script><p>因为随机变量X的熵与X的取值没关系，只取决于X的分布，所以也可以记作$H(p)$。如果对数以2为底，此时熵的单位是比特；如果对数以e为底，此时熵的单位是奈特。</p><p><strong>条件熵</strong>：</p><script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^n p_i H(Y|X=x_i)</script><p><strong>信息增益</strong>：特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即</p><script type="math/tex; mode=display">g(D, A) = H(D) - H(D|A)</script><p>熵$H(D)$与条件熵$H(D|A)$之差也称为互信息。设训练数据集为D，$|D|$表示为样本容量；设有K个类，$|C_k|$表示属于第k类的样本个数；设特征A的取值有n个，根据特征A的取值可以将D划分为n个不同的子集$D_1,D_2,…,D_n$，$|D_i|$为子集$D_i$中的样本个数；记子集$D_i$中属于第k类的样本集合为$D_{ik}$，信息增益可以通过如下公式计算：</p><script type="math/tex; mode=display">\begin{gathered}H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} \\H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) \\g(D, A) = H(D) - H(D|A)\end{gathered}</script><p><strong>信息增益比</strong>：以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这一问题进行校正，信息增益比通过如下公式计算：</p><script type="math/tex; mode=display">\begin{gathered}H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|} \\g_R(D, A) = \frac{g(D, A)}{H_A(D)}\end{gathered}</script><p><strong>基尼指数</strong>：分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布p的基尼指数可以定义为：</p><script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2</script><p>对于二类分类问题，若样本点属于第一个类的概率是p，则该概率分布的基尼指数为：</p><script type="math/tex; mode=display">Gini(p) = 2p(1 - p)</script><p>对于给定的样本集合D，其基尼指数为：</p><script type="math/tex; mode=display">Gini(D) = 1 - \sum_{k=1}^K(\frac{|C_k|}{|D|})^2</script><p>如果样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分，则在特征A的条件下，集合D的基尼指数为：</p><script type="math/tex; mode=display">Gini(D, A=a) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)</script><p>（$Gini(D,A)$的计算方式与条件熵$H(D,A)$有点类似）</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>决策树的生成方式采用递归的方式，对于初始数据集D，计算所有特征对数据集D的信息增益（或信息增益比），选择信息增益最大（或信息增益比最大）的特征，根据该特征的取值对数据集进行划分（可能不止两个子节点），然后对划分的数据集递归使用这种生成算法。使用信息增益作为特征选择指标的算法叫<strong>ID3</strong>算法；使用信息增益比作为特征选择指标的算法叫做<strong>C4.5</strong>算法；<strong>ID3</strong>算法不能直接处理连续型特征，<strong>C4.5</strong>对其进行了改进，对于连续型特征，采用分桶的方式处理成离散型特征；<strong>CART</strong>算法与<strong>ID3</strong>和<strong>C4.5</strong>类似，但只生成二叉树，而且可以处理回归问题，回归时的拟合误差采用均方误差。</p><p>在实现之前需要考虑写的决策树具有什么样的功能，按照哪种算法来实现。查阅sklearn的决策树<a href="https://scikit-learn.org/stable/modules/tree.html" target="_blank" rel="noopener">官方文档</a>发现，sklearn里实现的是优化过的<strong>CART</strong>算法，并且无法处理离散型特征。为了便于实现，本文实现的决策树算法生成的是一棵二叉分类树，并且只能处理连续型特征，采用信息增益作为分裂的准则，每次分裂通过遍历所有特征的所有分裂点来选择最优的分裂特征，并且通过控制树的最大深度、最小信息增益量来控制过拟合。决策树的每一个内部节点需要保存该节点分列时用的哪一个特征以及分裂值，还需要保存孩子节点的指针，如果是叶子节点，需要保存当前叶子节点对应的标签（叶子 标签的计算采用投票法，即当前叶子节点的样本中出现次数最多的标签）。做预测时，只要根据输入的样本从树的根节点走到叶子节点即可。</p><p>代码的框架大致如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_idx=None, threshold=None, target=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                 target_proba=None, left_child=None, right_child=None)</span>:</span></span><br><span class="line">        self.feature_idx = feature_idx    <span class="comment"># 用于分裂的特征编号</span></span><br><span class="line">        self.threshold = threshold        <span class="comment"># 用于分裂的特征值</span></span><br><span class="line">        self.left_child = left_child      <span class="comment"># 左子树</span></span><br><span class="line">        self.right_child = right_child    <span class="comment"># 右子树</span></span><br><span class="line">        self.target = target              <span class="comment"># 如果是叶子节点，则保存对应的label</span></span><br><span class="line">        self.target_proba = target_proba  <span class="comment"># 如果是叶子节点，则保存每一个label的概率</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecisionTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_samples_split=<span class="number">2</span>, min_info_gain=<span class="number">1e-7</span>, max_depth=float<span class="params">(<span class="string">"inf"</span>)</span>)</span>:</span></span><br><span class="line">        self.root = <span class="keyword">None</span></span><br><span class="line">        self.min_samples_split = min_samples_split</span><br><span class="line">        self.min_info_gain = min_info_gain</span><br><span class="line">        self.max_depth = max_depth</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_entropy</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        <span class="string">"""计算信息熵"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_info_gain</span><span class="params">(self, y, y1, y2)</span>:</span></span><br><span class="line">        <span class="string">"""计算信息增益"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_leaf_calculation</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        <span class="string">"""计算叶子节点对应的标签、对应标签的概率"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_data_split</span><span class="params">(self, X, feature_idx, threshold)</span>:</span></span><br><span class="line">        <span class="string">"""将数据集根据第feature_idx个特征分成两部分：该特征值小于等于threshold的分为一部分，剩下的为另一部分"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_tree</span><span class="params">(self, X, y, current_depth=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""构建决策树"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_search_tree</span><span class="params">(self, x, proba=False, tree=None)</span>:</span></span><br><span class="line">        <span class="string">"""搜索树"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""训练决策树"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""预测"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict_proba</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""预测概率"""</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p><p>用sklearn自带的iris数据集测试实现的决策树算法，accuracy可以达到0.96。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = datasets.load_iris()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">model = DecisionTree()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'Accuracy = <span class="subst">&#123;accuracy&#125;</span>'</span>)</span><br></pre></td></tr></table></figure></p><p>完整代码在<a href="https://github.com/rearcher/template/blob/master/ml/decision_tree.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文实现了可以处理连续型特征的二叉分类树，采用信息增益作为分裂时特征选择的标准。还有许多有趣的问题可以考虑，例如加上离散特征的处理、增加回归树的实现等。还可以基于当前的实现的决策树来实现随机森林算法，因为随机森林包含多棵基本决策树，每棵基本决策树都对输入样本进行采样，并且使用特征集的某个子集来进行分裂，最后的结果由所有基本决策树的结果来共同决定（投票、平均等）。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://github.com/eriklindernoren/ML-From-Scratch" target="_blank" rel="noopener">ML-From-Scratch</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li><li><a href="https://book.douban.com/subject/24703171/" target="_blank" rel="noopener">《机器学习实战》</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;相关概念&quot;&gt;&lt;a href=&quot;#相关概念&quot; class=&quot;headerlink&quot; title=&quot;相关概念&quot;&gt;&lt;/a&gt;相关概念&lt;/h2&gt;&lt;p&gt;随机变量X的&lt;strong&gt;熵&lt;/strong&gt;定义为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现感知机算法</title>
    <link href="https://rearcher.github.io/perceptron.html"/>
    <id>https://rearcher.github.io/perceptron.html</id>
    <published>2018-12-06T08:12:21.000Z</published>
    <updated>2018-12-08T10:20:59.659Z</updated>
    
    <content type="html"><![CDATA[<script type="math/tex; mode=display">\begin{gathered}f(x) = sign(w \cdot x + b) \\sign(x) = \begin{cases} +1, x \geq 0 \\ -1, x < 0 \end{cases}\end{gathered}</script><p>感知机是一种线性分类模型，属于判别模型。几何解释：$w \cdot x + b = 0$可以看成是一个超平面，其中$w$是超平面的法向量，$b$是超平面的截距，这个超平面把特征空间划分为两部分。</p><h2 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h2><p>因为感知机是一个线性分类器，所以学习的目标是求得能将两类点分离的超平面。这需要定义一个损失函数，并且将损失函数极小化。直观来说，损失函数可以定义为误分类的点的数量，但是这样的损失函数不是参数$w$、$b$的连续可导函数，无法优化。损失函数的另一个选择是<strong>误分类点到超平面的总距离</strong>。输入空间中任意一点$x_0$到分离超平面的距离为$\frac{1}{|w|}|w \cdot x_0 + b|$，考虑到对于误分类的数据点$(x_i, y_i)$来说，有$-y_i(w \cdot x_i + b) &gt; 0$，因此所有误分类点到超平面的总距离可以写成$-\frac{1}{|w|} \sum_{x_i \in M} y_i(w \cdot x_i + b)$，不考虑$\frac{1}{|w|}$，损失函数就可以写成</p><script type="math/tex; mode=display">L(w, b) = -\sum_{x_i \in M} y_i(w \cdot x_i + b)</script><p>其中$M$是误分类点的集合。</p><h2 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h2><h3 id="原始形式"><a href="#原始形式" class="headerlink" title="原始形式"></a>原始形式</h3><p>根据损失函数分别计算参数$w$和$b$的梯度：</p><script type="math/tex; mode=display">\begin{gathered}\nabla_w L(w,b) = -\sum_{x_i \in M} y_i x_i \\\nabla_b L(w,b) = -\sum_{x_i \in M} y_i\end{gathered}</script><p>随机梯度下降算法：随机选取误分类点$(x_i, y_i)$，对$w$，$b$进行更新（$w$和$b$的初值都为0）：</p><script type="math/tex; mode=display">\begin{gathered}w \gets w + \eta y_i x_i \\b \gets b + \eta y_i\end{gathered}</script><h3 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h3><p>从上面更新$w$和$b$的式子可以看到，最终的$w$和$b$的值取决于误分类点。因此最终的$w$和$b$可以写成如下形式：</p><script type="math/tex; mode=display">\begin{gathered}w = \sum_{i=1}^N \alpha_i y_i x_i \\b = \sum_{i=1}^N \alpha_i y_i\end{gathered}</script><p>其中$\alpha_i = n_i \eta_i$，其中$n_i$是点$i$用于更新的次数。因此求解$w$和$b$转变成了求解参数参数向量$\alpha$。初始时可以设置$\alpha$向量为0向量，b也设置为0，然后对于随机选取的点$(x_i, y_i)$，如果有$y_i(\sum_{j=1}^N \alpha_j y_j x_j \cdot x_i + b) \leq 0$，则进行以下更新：</p><script type="math/tex; mode=display">\begin{gathered}\alpha_i \gets \alpha_i + \eta \\b \gets b + \eta y_i\end{gathered}</script><p>对偶形式中，需要计算向量$x$之间的内积，因此可以提前算出来，并以矩阵的形式存储，这个矩阵也称为Gram矩阵。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面用python实现三维空间上的感知机算法，即输入空间是3维的，感知机方程则表示三维空间中的一个平面。代码中随机生成一个三维平面，接着随机生成平面两侧的数据点用于感知机算法的拟合，然后分别用accuracy、precision、recall来评判拟合的效果，以及用可视化的方式来展示拟合的状态。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""感知机训练算法的原始形式</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 输入的特征</span></span><br><span class="line"><span class="string">            y: 输入的标签&#123;-1, 1&#125;</span></span><br><span class="line"><span class="string">            n_iterations: 算法迭代次数</span></span><br><span class="line"><span class="string">            learning_rate: 学习速率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n, m = X.shape</span><br><span class="line">        self.w, self.b = np.zeros((<span class="number">3</span>, <span class="number">1</span>)), <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(n_iterations)):</span><br><span class="line">            idx = np.random.randint(<span class="number">0</span>, n)</span><br><span class="line">            <span class="keyword">if</span> y[idx] * (np.vdot(self.w, X[idx].reshape(<span class="number">3</span>, <span class="number">1</span>)) + self.b) &lt;= <span class="number">0</span>:</span><br><span class="line">                self.w += learning_rate * y[idx] * X[idx].reshape(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">                self.b += learning_rate * y[idx]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_dual</span><span class="params">(self, X, y, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""感知机训练算法的对偶形式"""</span></span><br><span class="line">        n, m = X.shape</span><br><span class="line">        self.alpha, self.b = np.zeros((n, <span class="number">1</span>)), <span class="number">0</span></span><br><span class="line">        self.gram = np.zeros((n, n))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算Gram相关性矩阵</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, n):</span><br><span class="line">            self.gram[i][i] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, n):</span><br><span class="line">                self.gram[i][j] = self.gram[j][i] = np.vdot(X[i].reshape(<span class="number">3</span>, <span class="number">1</span>), X[j].reshape(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(range(n_iterations)):</span><br><span class="line">            idx = np.random.randint(<span class="number">0</span>, n)</span><br><span class="line">            <span class="keyword">if</span> y[idx] * (sum(self.alpha * y * self.gram[:, idx].reshape(<span class="number">-1</span>, <span class="number">1</span>)) + self.b) &lt;= <span class="number">0</span>:</span><br><span class="line">                self.alpha[idx][<span class="number">0</span>] += learning_rate</span><br><span class="line">                self.b += learning_rate * y[idx]</span><br><span class="line">        self.w = sum(self.alpha * y * X).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""预测"""</span></span><br><span class="line">        y = (np.dot(X, self.w) + self.b).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        n, _ = y.shape</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            y[i][<span class="number">0</span>] = sign(y[i][<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure></p><p>最后程序输出的accuracy、precision、recall都可以达到1.0，说明拟合效果很好。下图展示了拟合的效果，其中黄色的平面代表用于生成随机数的平面，绿色的平面代表拟合的平面。不同类的数据点都分布于平面的两侧。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fxx48c5medj30p40mujtz.jpg" alt="result"><br>完整代码在<a href="https://github.com/rearcher/template/blob/master/ml/perceptron.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文实现了输入空间是三维的感知机算法，实现了原始形式的学习算法和对偶形式的学习算法。对于更高维的输入，也可以采取类似的方式实现。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{gathered}
f(x) = sign(w \cdot x + b) \\
sign(x) = \begin{cases} +1, x \geq 0 \\ -1, x &lt; 0 \end{
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>有趣的背包问题</title>
    <link href="https://rearcher.github.io/interesting-knapsack-problem.html"/>
    <id>https://rearcher.github.io/interesting-knapsack-problem.html</id>
    <published>2018-09-24T10:29:48.000Z</published>
    <updated>2018-09-24T12:48:46.349Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在写动态规划相关的算法题，遇到了很多背包问题的变种问题，解法也很精彩，趁着中秋有空，记录于此。</p><h2 id="基本的背包问题"><a href="#基本的背包问题" class="headerlink" title="基本的背包问题"></a>基本的背包问题</h2><p>背包问题的定义：</p><blockquote><p>有N种物品，对于第i种物品，其体积是$V_i$，价值是$W_i$，数量是$M_i$。现有容量为C的背包，要求从这N种物品中选择一定数量的物品放入背包，使得背包所装的物品价值总和最大。</p></blockquote><p>根据物品数量的不同，又可以把背包问题分为以下三种类型：</p><blockquote><ol><li>每种物品的数量都是1，这时称为<strong>01背包问题</strong>；</li><li>每种物品的数量都是无限，这时称为<strong>完全背包问题</strong>；</li><li>每种物品的数量都是1个或多个，这时称为<strong>多重背包问题</strong>。</li></ol></blockquote><h3 id="01背包"><a href="#01背包" class="headerlink" title="01背包"></a>01背包</h3><p>最简单的是01背包问题，这里简单定义$f(i,j)$为考虑使用前i种物品和容量为j的背包时，所能获取的最大价值。对于第i种物品有两种策略，一种是取，一种是不取，那么01背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max\{f(i - 1, j),  f(i - 1, j - V_i) + W_i\}</script><p>其中，$f(i - 1, j)$表示不取第i种物品，$f(i - 1, j - V_i) + W_i$表示取第i种物品。此时的代码是一个二重循环，因为递推式中只用到了上一轮的状态，所以可以优化空间复杂度，一维数组就可以求解。代码类似于下面这样：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 01背包</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = C; j &gt;= V[i]; --j)</span><br><span class="line">        dp[j] = max(dp[j], dp[j - V[i]] + W[i]);</span><br></pre></td></tr></table></figure></p><p>注意到第二层循环是从后往前的，因为$f(i, j)$需要用到$f(i, j - V_i)$，只有从后往前，才能保证用到的是上一轮的状态，即“没有选取过第i种物品”的状态。</p><h3 id="完全背包"><a href="#完全背包" class="headerlink" title="完全背包"></a>完全背包</h3><p>完全背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max\{f(i - 1, j),  f(i, j - V_i) + W_i\}</script><p>其中，$f(i-1,j)$代表第i种物品一件都不取的子状态，$f(i, j - V_i) + W_i$代表已经取了第i种物品（多少件不知道）的子状态。代码如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 完全背包</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = V[i]; j &lt;= C; ++j)</span><br><span class="line">        dp[j] = max(dp[j], dp[j - V[i]] + W[i]);</span><br></pre></td></tr></table></figure></p><p>可以看到，与01背包的区别只是第二层的循环顺序不同而已。</p><h3 id="多重背包"><a href="#多重背包" class="headerlink" title="多重背包"></a>多重背包</h3><p>基本思想是每件物品可以取0,1,2,…,$M_i$件，此时多重背包的递推式为：</p><script type="math/tex; mode=display">f(i, j) = \max \{ f(i - 1, j - k * V_i)\}, 0 \le k \le M_i</script><p>时间复杂度是$O(V\Sigma M_i)$。这个时间复杂度在物品件数较高的情况下是较高的，常见的优化为二进制优化，将多重背包问题转化为01背包问题来求解，此时时间复杂度可以下降到$O(Vlog\Sigma M_i)$。</p><p>二进制优化：</p><blockquote><p>将第i种物品分成若干件01背包中的物品，其中每件物品有一个系数，该物品的价值和体积都是原来的价值和体积乘上这个系数。这些系数为$1,2,4,8,…,2^{k-1},M_i - 2^k + 1$，其中k是满足$M_i - 2 ^ k + 1 &gt; 0$的最大整数。</p></blockquote><h2 id="一些变种背包问题"><a href="#一些变种背包问题" class="headerlink" title="一些变种背包问题"></a>一些变种背包问题</h2><h3 id="要求背包装满"><a href="#要求背包装满" class="headerlink" title="要求背包装满"></a>要求背包装满</h3><p>题目链接在<a href="http://poj.org/problem?id=1742" target="_blank" rel="noopener">这里</a>。题目大意是有一堆硬币，每个硬币有不同的面值以及数量，给定一个值m，要求1至m中有多少个值可以被这些硬币凑出来。看成背包问题来做，好像是一个多重背包问题，背包的最大容量是m。但这题与普通的背包问题又有点区别，普通的背包问题要求的是“背包里物品的价值最大”，而这题转换成背包问题是“哪些容量的背包可以被装满”。这区别可以用初始化来体现，如果不要求背包装满，那么所有的初始状态都是0；如果要求装满，那么只将$f(0,0)$设为0，其他设置为一个表示不合法的特殊值即可。</p><p>上面说到多重背包问题可以用二进制优化的思想，转化为01背包问题求解。但对于这道题，这样的时间复杂度还是太高。当背包问题不要求背包里物品价值最大，而仅仅要求装满背包时，可以有$O(NV)$的解法：用$num_j$表示装满容量为j的背包当前物品最多需要多少个，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 对于每一轮，用num[j]表示装满容量为j的背包当前物品最多需要多少个，</span></span><br><span class="line"><span class="comment"> * dp[j] = 1表示能将容量为j的背包装满，dp[j] = 0表示不能，dp[0] = 1，</span></span><br><span class="line"><span class="comment"> * 最后dp[1..V]中为1的都是对应的能被装满的容量。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">    <span class="built_in">memset</span>(num, <span class="number">0</span>, <span class="keyword">sizeof</span>(num));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = V[i]; j &lt;= m; ++j) &#123;</span><br><span class="line">        <span class="comment">// 如果当前容量j的背包还不能被装满，且j-V[i]的背包能被装满，</span></span><br><span class="line">        <span class="comment">// 且装满j-V[i]容量的背包所用的当前物品的数量小于M[i]。</span></span><br><span class="line">        <span class="keyword">if</span> (!dp[j] &amp;&amp; dp[j - V[i]] &amp;&amp; num[j - V[i]] &lt; M[i]) &#123;</span><br><span class="line">            dp[j] = <span class="number">1</span>;</span><br><span class="line">            num[j] = num[j - V[i]] + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="物品的重量为负值"><a href="#物品的重量为负值" class="headerlink" title="物品的重量为负值"></a>物品的重量为负值</h3><p>题目链接在<a href="http://poj.org/problem?id=2184" target="_blank" rel="noopener">这里</a>。题目大意是每头牛都有一个智力值$S_i$和幽默值$F_i$，要求选出一些牛，这些牛的智力值总和和幽默值总和最大，并且智力值总和必须大于0，幽默值总和也必须大于0。</p><p>这题也可以看成背包问题来做，是01背包问题。把智力值当成物品的重量，幽默值当成物品的价值，并且也属于将背包装满的问题。问题在于，物品的重量存在负值。因为数组的索引最小是0，不能存在负值，所以可以人为加一个偏移值offset（比如100000），初始化时令<code>dp[offset]=0</code>，其余位置都设置为一个不合法的值即可。还有一个问题是，01背包问题递推时，要求从后往前，因为$f(i,j)$需要用到$f(i, j-V_i)$的子状态，但是如果$V_i &lt; 0$，此时$j - V_i &gt; j$，应该<strong>从前往后更新</strong>！具体AC代码节选如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> inf = <span class="number">0x3f3f3f3f</span>, offset = <span class="number">100000</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt;= <span class="number">200000</span>; ++i) dp[i] = -inf;</span><br><span class="line">dp[offset] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (s[i] &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">200000</span>; j &gt;= s[i]; --j)</span><br><span class="line">            <span class="keyword">if</span> (dp[j - s[i]] &gt; -inf)</span><br><span class="line">                dp[j] =  max(dp[j], dp[j - s[i]] + f[i]);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;  j - s[i] &lt;= <span class="number">200000</span>; ++j)</span><br><span class="line">            <span class="keyword">if</span> (dp[j - s[i]] &gt; -inf)</span><br><span class="line">                dp[j] = max(dp[j], dp[j - s[i]] + f[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以从前往后更新也不一定就是完全背包哦^_^。</p><h3 id="对于不同物品具有不同的背包上限"><a href="#对于不同物品具有不同的背包上限" class="headerlink" title="对于不同物品具有不同的背包上限"></a>对于不同物品具有不同的背包上限</h3><p>题目链接在<a href="http://poj.org/problem?id=2392`" target="_blank" rel="noopener">这里</a>。题目大意是有各种不同高度和数量的砖块，每种砖块的高度是$h_i$，数量是$c_i$，第i种砖块最高只能用在$a_i$以下高度的地方，要求用这些砖块能达到最大的高度是多少。</p><p>可以看到这也是一个多重背包问题，物品的重量和价值相同，都是砖块的高度$h_i$，同时也属于把背包装满的问题。考虑到能用尽可能多的砖块，所以$a_i$小的砖块应该先用，所以首先需要对砖块进行排序。同时对于砖块i，其背包容量的上限是$a_i$。总的背包容量上限是最大的$a_i$。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>题目总是千变万化，关键是如何将未知的题目转化为已知的题型来求解，这要求对经典的解法有深入的理解。本文所谈及的背包问题变种还只是冰山一角，还有更多未涉及的话题。“我亦无他，唯手熟尔”，很多解题思想并不是只能用在一个地方，而是很多地方都能运用，只有多多练习，以后遇到新的题型才能有更多想法吧，keep moving!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在写动态规划相关的算法题，遇到了很多背包问题的变种问题，解法也很精彩，趁着中秋有空，记录于此。&lt;/p&gt;
&lt;h2 id=&quot;基本的背包问题&quot;&gt;&lt;a href=&quot;#基本的背包问题&quot; class=&quot;headerlink&quot; title=&quot;基本的背包问题&quot;&gt;&lt;/a&gt;基本的背包问
      
    
    </summary>
    
      <category term="算法" scheme="https://rearcher.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="背包问题" scheme="https://rearcher.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"/>
    
      <category term="动态规划" scheme="https://rearcher.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>树状数组区间更新的O(logn)实现</title>
    <link href="https://rearcher.github.io/bit-logn-range-update.html"/>
    <id>https://rearcher.github.io/bit-logn-range-update.html</id>
    <published>2018-07-25T10:47:29.000Z</published>
    <updated>2018-07-27T09:28:27.259Z</updated>
    
    <content type="html"><![CDATA[<p>树状数组常用于高效求解前缀和、区间和，复杂度都在$O(logn)$，同时支持单点更新。但是区间更新效率低下，朴素的实现需要对区间内的每一个元素实行单点更新，时间复杂度在$O(n)$。可以通过维护两个数组，来高效地实现区间更新。</p><p>假设目前有一列数据$a_1, a_2, a_3, …, a_n$，用两个树状数组来高效进行这些数据的区间更新与求和。令<code>sum(bit, i)</code>是树状数组bit的前i项和，构建两个树状数组bit0和bit1，并且假设数列${a_i}$的前i项和为</p><script type="math/tex; mode=display">\Sigma_{j = 1} ^ {i} a_j = sum(bit1, i) * i + sum(bit0, i)</script><p>其中树状数组bit0代表的是原数组，bit1代表的原数组上的区间更新量。那么在$[l, r]$区间上同时加上$x$就可以看成是：</p><blockquote><ul><li>在bit0的$l$位置上加上$-x(l - 1)$</li><li>在bit1的$l$位置上加上$x$</li><li>在bit0的$r+1$位置上加上$xr$</li><li>在bit1的$r+1$位置上加上$-x$</li></ul></blockquote><p>下面考虑三种情况，来验证上面的更新策略，假设要求数组a的前i项和：</p><ul><li>$i &lt; l$：不影响</li><li>$l \le i \le r$：相当于加上了$x * i - x(l - 1) = x * (i - l + 1)$，正确</li><li>$l &gt; r$：相当于加上了$0 * i + xr - x(l - 1) = x * (r - l + 1)$，正确</li></ul><p>基于c++的代码实现：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAX_N = <span class="number">100001</span>;</span><br><span class="line"><span class="keyword">int</span> bit0[MAX_N], bit1[MAX_N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sum</span><span class="params">(<span class="keyword">int</span>* bit, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        res += bit[i];</span><br><span class="line">        i -= i &amp; (-i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span>* bit, <span class="keyword">int</span> i, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (i &lt;= MAX_N) &#123;</span><br><span class="line">        bit[i] += x;</span><br><span class="line">        i += i &amp; (-i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 求和[1, i]</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">sum</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> sum(bit1, i) * i + sum(bit0, i);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 区间更新，[from, to]的元素都加上x，时间复杂度logn</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">rangeUpdate</span><span class="params">(<span class="keyword">int</span> from, <span class="keyword">int</span> to, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">    add(bit0, from, -x * (from - <span class="number">1</span>));</span><br><span class="line">    add(bit1, from, x);</span><br><span class="line">    add(bit0, to + <span class="number">1</span>, x * to);</span><br><span class="line">    add(bit1, to + <span class="number">1</span>, -x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/24749842/" target="_blank" rel="noopener">《挑战程序设计竞赛》</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;树状数组常用于高效求解前缀和、区间和，复杂度都在$O(logn)$，同时支持单点更新。但是区间更新效率低下，朴素的实现需要对区间内的每一个元素实行单点更新，时间复杂度在$O(n)$。可以通过维护两个数组，来高效地实现区间更新。&lt;/p&gt;
&lt;p&gt;假设目前有一列数据$a_1, a
      
    
    </summary>
    
      <category term="算法" scheme="https://rearcher.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="https://rearcher.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>用python实现朴素贝叶斯分类算法</title>
    <link href="https://rearcher.github.io/naive_bayes.html"/>
    <id>https://rearcher.github.io/naive_bayes.html</id>
    <published>2018-04-24T07:08:19.000Z</published>
    <updated>2018-12-06T08:47:32.800Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>朴素贝叶斯算法主要用于分类问题，原理十分简单，主要采用后验概率最大化的方法来判定测试样本的类别。对于某个给定的测试样本$ x = \{x_1,…,x_n\} $，其类别为$y$的概率可以通过贝叶斯公式计算：</p><script type="math/tex; mode=display">P(y|x_1,...,x_n) = \frac{P(y)P(x_1,...,x_n|y)}{P(x_1,...,x_n)}</script><p>其中$P(y)$是类别$y$的先验概率，$P(x_1,…,x_n|y)$是条件概率。后验概率最大化就是找出能够使得$P(y|x_1,…,x_n)$最大的类别$y$。</p><p>由于朴素贝叶斯算法假设各个特征之间相互独立，所以可以得到下面的公式：</p><script type="math/tex; mode=display">P(x_1,...,x_n|y) = \prod_{i=1}^{n}P(x_i|y)</script><p>因为$P(x_1,…,x_n)$是个不变量，所以只要考虑分子，最后朴素贝叶斯算法分类的公式可以表示为：</p><script type="math/tex; mode=display">\hat{y} = \arg\max_y P(y)\prod_{i=1}^{n}P(x_i|y)</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>《统计学习方法》中举了一个关于朴素贝叶斯算法分类的例子，但这个例子中特征是离散特征，所以需要分别对于每一类特征的每一个值计算条件概率$P(x_i | y)$。对于连续型的数值特征，比较常见的做法是假设特征符合某项概率分布，例如高斯分布、伯努利分布等。本文的实现针对连续型的数值特征，假设其符合高斯分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""朴素贝叶斯算法</span></span><br><span class="line"><span class="string">        假设条件概率p(x|y)符合高斯分布</span></span><br><span class="line"><span class="string">        为每一类的样本中特征的每一个维度拟合一个高斯分布</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练特征</span></span><br><span class="line"><span class="string">            y: 训练标签</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.X, self.y = X, y</span><br><span class="line">        self.classes = np.unique(y)</span><br><span class="line">        self.parameters = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.classes):</span><br><span class="line">            tmp_X = X[np.where(y == c)]</span><br><span class="line">            self.parameters.append([])</span><br><span class="line">            <span class="keyword">for</span> col <span class="keyword">in</span> tmp_X.T:</span><br><span class="line">                parameters = &#123;<span class="string">"mean"</span>: col.mean(), <span class="string">"var"</span>: col.var()&#125;</span><br><span class="line">                self.parameters[i].append(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_priori</span><span class="params">(self, c)</span>:</span></span><br><span class="line">        <span class="string">"""根据训练数据计算每一类的先验概率"""</span></span><br><span class="line">        tmp_X = self.X[np.where(self.y == c)]</span><br><span class="line">        <span class="keyword">return</span> len(tmp_X) / len(self.X)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calculate_likelihood</span><span class="params">(self, mean, var, x)</span>:</span></span><br><span class="line">        <span class="string">"""计算条件概率"""</span></span><br><span class="line">        eps = <span class="number">1e-4</span></span><br><span class="line">        coef = <span class="number">1.0</span> / math.sqrt(<span class="number">2.0</span> * math.pi * var + eps)</span><br><span class="line">        exponent = math.exp(-math.pow(x - mean, <span class="number">2</span>) / (<span class="number">2</span> * var + eps))</span><br><span class="line">        <span class="keyword">return</span> coef * exponent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_classify</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        <span class="string">"""采用最大后验概率来分类"""</span></span><br><span class="line">        posteriors = []</span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.classes):</span><br><span class="line">            posterior = self._calculate_priori(c)</span><br><span class="line">            <span class="keyword">for</span> feature_value, params <span class="keyword">in</span> zip(sample, self.parameters[i]):</span><br><span class="line">                likelihood = self._calculate_likelihood(params[<span class="string">'mean'</span>], params[<span class="string">'var'</span>], feature_value)</span><br><span class="line">                posterior *= likelihood</span><br><span class="line">            posteriors.append(posterior)</span><br><span class="line">        <span class="keyword">return</span> self.classes[np.argmax(posteriors)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        pred = [self._classify(x) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><p>下面采用sklearn自带的iris数据集测试实现的模型，可以看到分类的accuracy在0.96。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = NaiveBayes()</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">model.fit(iris.data, iris.target)</span><br><span class="line">y_pred = model.predict(iris.data)</span><br><span class="line">print(<span class="string">f'accuracy = <span class="subst">&#123;accuracy_score(iris.target, y_pred)&#125;</span>'</span>) <span class="comment"># 输出 accuracy = 0.96</span></span><br></pre></td></tr></table></figure></p><p>完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/naive_bayes.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">《统计学习方法》</a></li><li><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/naive_bayes.py" target="_blank" rel="noopener">ML-From-Scratch</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;朴素贝叶斯算法主要用于分类问题，原理十分简单，主要采用后验概率最大化的方法来判定测试样本的类别。对于某个给定的测试样本$ x = \{x_1
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现k近邻、kd树搜索</title>
    <link href="https://rearcher.github.io/k_nearest_neighbors.html"/>
    <id>https://rearcher.github.io/k_nearest_neighbors.html</id>
    <published>2018-04-22T04:51:40.000Z</published>
    <updated>2018-12-04T15:02:32.025Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>k近邻算法是一种非常简单、直观的算法：给定一个数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该新的输入实例分为这个类。</p><p>这里主要涉及到两个问题：一是实例间的距离度量，二是如何找到所有训练数据集中最靠近新的输入实例的k个实例。常用的距离度量有欧氏距离（Euclidean Distance）和Minkowski距离等，本文实现的k近邻算法就采用了欧氏距离。至于如何找到靠近输入实例最近的k个实例，一种比较朴素的实现方式是对整个训练数据集进行线性扫描，维护一个大小为k的优先队列，保存距离输入实例最近的k个实例点，也俗称暴力搜索，但是当训练集非常大时，计算非常耗时，这种方法就有点捉襟见肘了。为了提高搜索的效率，可以使用特殊的数据结构来存储训练数据，以减少搜索时间，常见的有ball-tree、kd-tree等，本文通过实现kd-tree来加快k近邻的搜索。</p><h2 id="kd-tree的构建与搜索"><a href="#kd-tree的构建与搜索" class="headerlink" title="kd-tree的构建与搜索"></a>kd-tree的构建与搜索</h2><p>假设训练数据中的样本属于k维空间，kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每一个节点对应于一个k维超矩形区域。</p><p>构造kd树的算法如下：</p><blockquote><p>假设输入的训练集为T，训练集中每个实例的维度是n；</p><p>构造树节点：假设当前树节点的深度为depth, 以样本的第(depth % n)维为坐标轴，以T中所有实例的第(depth % n)维的中位数为切分点，将训练集分为t1和t2两部分。其中t1是切分点左边的数据集，t2是切分点右边的数据集。将切分点保存在当前树节点，用数据集t1递归构造当前节点的左节点，用数据集t2递归构造当前节点的右节点。当输入的数据集为空时递归终止。</p></blockquote><p>kd树的k近邻搜索算法如下：</p><blockquote><p>假设要搜索实例点x的k近邻，结果保存在L中；</p><ol><li>在kd树中找到距离x最近的叶节点：从根节点出发，递归向下访问kd树，如果x当前维的坐标小于切分点的坐标，则向左子树搜索，反之向右子树搜索，直到到达叶节点为止；</li><li><p>以当前叶节点为起点，开始递归向上搜索，直到到达根节点。对每个节点进行以下操作：</p><p>(a) 如果L中不足k个实例点，或者当前树节点保存的实例与x的距离小于L中的最大距离，用当前树节点保存的实例点替换L中距离最大的点；</p><p>(b) 计算x与当前切分轴的距离，如果此距离小于L中的最大距离，则当前树节点的另一个子节点区域可能存在更近的点，对另一个子节点递归调用k近邻搜索算法。因为是向上回退搜索，如果上一步是从左节点退到父节点，就应该对右节点进行递归搜索，另一种情况同理。</p></li></ol></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, k=<span class="number">3</span>)</span>:</span></span><br><span class="line">        self.k = k</span><br><span class="line">        self.neighbors = []  <span class="comment"># 用于保存kd树搜索过程中k个最近邻居的标签</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_vote</span><span class="params">(self, neighbors)</span>:</span></span><br><span class="line">        <span class="string">"""投票算法，选取k个邻居中出现次数最多的类别"""</span></span><br><span class="line">        counts = np.bincount(neighbors.astype(<span class="string">'int'</span>))</span><br><span class="line">        <span class="keyword">return</span> counts.argmax()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_kd_tree</span><span class="params">(self, data, depth)</span>:</span></span><br><span class="line">        <span class="string">"""建立kd树</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            data: 需要训练的数据</span></span><br><span class="line"><span class="string">            depth: 当前建立的数的深度</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            kd树的节点</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        data = np.array(data)</span><br><span class="line">        n_samples = data.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> n_samples == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            n_features = data.shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">            current_node = dict()</span><br><span class="line">            current_node[<span class="string">'split_axis'</span>] = depth % n_features + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            data = sorted(data, key=<span class="keyword">lambda</span> x: x[current_node[<span class="string">'split_axis'</span>]])</span><br><span class="line">            split_idx = n_samples // <span class="number">2</span></span><br><span class="line">            current_node[<span class="string">'data'</span>] = data[split_idx]</span><br><span class="line">            current_node[<span class="string">'left'</span>] = self._build_kd_tree(data[:split_idx], depth + <span class="number">1</span>)</span><br><span class="line">            current_node[<span class="string">'right'</span>] = self._build_kd_tree(data[split_idx+<span class="number">1</span>:], depth + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> current_node</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_search_tree</span><span class="params">(self, root, x)</span>:</span></span><br><span class="line">        <span class="string">"""搜索kd树</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root: 开始搜索的树节点</span></span><br><span class="line"><span class="string">            x: 需要判定类别的样本</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        split_axis = root[<span class="string">'split_axis'</span>]</span><br><span class="line">        <span class="keyword">if</span> x[split_axis] &lt; root[<span class="string">'data'</span>][split_axis]:</span><br><span class="line">            self._search_tree(root[<span class="string">'left'</span>], x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._search_tree(root[<span class="string">'right'</span>], x)</span><br><span class="line"></span><br><span class="line">        heapq.heappush(self.neighbors, (<span class="number">-1</span> * euclidean_distance(x, root[<span class="string">'data'</span>][<span class="number">1</span>:]), next(counter), root[<span class="string">'data'</span>]))</span><br><span class="line">        <span class="keyword">if</span> len(self.neighbors) &gt; self.k:</span><br><span class="line">            heapq.heappop(self.neighbors)</span><br><span class="line"></span><br><span class="line">        split_dist = abs(x[split_axis] - root[<span class="string">'data'</span>][split_axis])</span><br><span class="line">        neighbor_max = <span class="number">-1</span> * heapq.nsmallest(<span class="number">1</span>, self.neighbors)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> split_dist &gt; neighbor_max:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x[split_axis] &lt; root[<span class="string">'data'</span>][split_axis]:</span><br><span class="line">            self._search_tree(root[<span class="string">'right'</span>], x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self._search_tree(root[<span class="string">'left'</span>], x)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @run_time</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X_train, y_train, X_test, kd_tree=False)</span>:</span></span><br><span class="line">        <span class="string">"""用训练集来预测测试集</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X_train: 训练特征数据</span></span><br><span class="line"><span class="string">            y_train: 训练标签数据</span></span><br><span class="line"><span class="string">            X_test: 测试特征数据</span></span><br><span class="line"><span class="string">            kd_tree: True代表使用kd树搜索，False代表使用线性扫描</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">            pred: 针对X_test的预测结果</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        pred = np.empty(X_test.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> kd_tree:</span><br><span class="line">            data = np.insert(X_train, <span class="number">0</span>, y_train, axis=<span class="number">1</span>)</span><br><span class="line">            n_features = np.array(data).shape[<span class="number">1</span>]</span><br><span class="line">            self.split_order = random.sample(range(<span class="number">1</span>, n_features), n_features - <span class="number">1</span>)</span><br><span class="line">            root = self._build_kd_tree(data, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">                <span class="comment"># print(f'processing sample &#123;i + 1&#125; / &#123;len(X_test)&#125;')</span></span><br><span class="line">                self.neighbors.clear()</span><br><span class="line">                self._search_tree(root, sample)</span><br><span class="line">                neighbors = np.array([x[<span class="number">0</span>] <span class="keyword">for</span> d, c, x <span class="keyword">in</span> self.neighbors])</span><br><span class="line">                pred[i] = self._vote(neighbors)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(X_test):</span><br><span class="line">                <span class="comment"># print(f'processing sample &#123;i + 1&#125; / &#123;len(X_test)&#125;')</span></span><br><span class="line">                idx = np.argsort([euclidean_distance(x, sample) <span class="keyword">for</span> x <span class="keyword">in</span> X_train])[:self.k]</span><br><span class="line">                neighbors = np.array([y_train[j] <span class="keyword">for</span> j <span class="keyword">in</span> idx])</span><br><span class="line">                pred[i] = self._vote(neighbors)</span><br><span class="line">        <span class="keyword">return</span> pred</span><br></pre></td></tr></table></figure><p>这里还是采用sklearn自带的digits数据集来测试我们的k近邻算法，分别用暴力搜索和kd树的方式来测试以上的实现，运行结果如下图。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlgy1fqlem0k5cnj30co074gm8.jpg" alt="kd_result"></p><p>可以看到kd树的实现方式略占优势，数据集更大的话，kd树应该能更快。完整代码在<a href="https://github.com/rearcher/template/blob/master/ml/k_nearest_neighbors.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k近邻算法虽然简单，但是在实现过程中还是学到了些知识和技巧，例如python中的heapq优先队列是最小堆，如果想要最大堆则将队列中元素的值乘以-1即可；再比如用cProfile来分析程序的性能，发现自己实现的欧式距离计算耗时很长，更换为numpy自带的实现后程序运行速度有了很大提升。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;k近邻算法是一种非常简单、直观的算法：给定一个数据集，对于新的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现逻辑回归</title>
    <link href="https://rearcher.github.io/logistic_regression.html"/>
    <id>https://rearcher.github.io/logistic_regression.html</id>
    <published>2018-04-18T04:56:42.000Z</published>
    <updated>2018-04-19T12:17:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>逻辑回归主要用于解决分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。<br>\begin{align}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align}</p><p>基本形式如下：<br>\begin{align}&amp; h_\theta (x) =  g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align}</p><p>可以看到逻辑回归其实就是在线性回归上套了一个$g$函数，也称为Sigmoid函数，形式如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy310ss7lj30my03k74g.jpg" alt="sigmoid"></p><p>可以看到，当$z$的值大于0的时候，$g(z)$是大于0.5的，当$z$小于0时，$g(z)$是小于0.5的。所以如果假设$h_\theta(x) \geq 0.5$时有$y = 1$（即当输出概率大于等于0.5时，判定为类别1），就等价于$\theta^T x \geq 0 \Rightarrow y = 1$。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><p>求解模型首先要了解模型的损失函数，损失函数可以用来衡量模型对数据的拟合程度。逻辑回归采用对数损失函数，如下：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = \begin{cases}  -log(h_{\theta}(x))  & \text {if y=1} \\ -log(1-h_{\theta}(x))  & \text{if y=0} \end{cases}</script><blockquote><p>当$y=1$时，即这个样本是正类：</p><ol><li>如果此时$h_\theta(x) = 1$，则对于这个样本而言是分类正确的，此时$cost=-log(1)=0$，表示对于分类正确的样本没有惩罚；</li><li>如果此时$h_\theta(x) = 0$，则对于这个样本而言是分类错误的，此时$cost=-log(0) \to \infty$，表示对于分类错误的样本要给予很大的惩罚。</li></ol><p>$y=0$时同理，不再赘述。</p></blockquote><p>可以将上述损失函数简化合并，变成如下形式：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = -y^{(i)}log(h_{\theta}(x)) - (1-y^{(i)})log(1-h_{\theta}(x))</script><p>考虑所有的样本，就变成了如下的形式：</p><script type="math/tex; mode=display">cost(h_{\theta}(x),y) = \frac{1}{m} \sum_{i=1}^{m} -y^{(i)}log(h_{\theta}(x)) - (1-y^{(i)})log(1-h_{\theta}(x))</script><p>逻辑回归模型的求解过程就是最小化上面的损失函数。这里还是采用梯度下降法来求解，还是用损失函数对每个参数$\theta_j$求偏导，求导过程不再涉及，参数$\theta_j$的梯度（偏导）为$\frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$，梯度下降算法就是对每个参数进行如下更新：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}</script><p>为了便于实现，这里还是给出损失函数和梯度下降的向量化形式，如下：<br>\begin{align}<br>&amp; h = g(X\theta)\newline<br>&amp; J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)\newline<br>&amp; \theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})<br>\end{align}</p><h2 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h2><p>有了上面的公式，再使用python自带的numpy矩阵运算库，可以很方便地实现逻辑回归模型。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""逻辑回归"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        <span class="string">"""sigmoid函数"""</span></span><br><span class="line">        g = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">        <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costFunction</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""损失函数</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据</span></span><br><span class="line"><span class="string">            y: 标签数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            J: 模型当前的损失</span></span><br><span class="line"><span class="string">            grad: 损失函数的梯度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples = len(y)</span><br><span class="line">        h = self.sigmoid(X.dot(self.theta))</span><br><span class="line">        J = (<span class="number">1</span> / n_samples) * (<span class="number">-1</span> * y.T.dot(np.log(h)) - (<span class="number">1</span> - y).T.dot(np.log(<span class="number">1</span> - h)))</span><br><span class="line">        grad = (<span class="number">1</span> / n_samples) * X.T.dot(h - y)</span><br><span class="line">        <span class="keyword">return</span> J, grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="string">"""训练模型</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据</span></span><br><span class="line"><span class="string">            y: 标签数据</span></span><br><span class="line"><span class="string">            n_iterations: 梯度下降算法迭代次数</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_features = X.shape[<span class="number">1</span>]</span><br><span class="line">        self.theta = np.zeros([n_features, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">            J, grad = self.costFunction(X, y)</span><br><span class="line">            self.theta -= learning_rate * grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""用模型进行预测"""</span></span><br><span class="line">        y_pred = np.round(self.sigmoid(X.dot(self.theta))).astype(int)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></p><p>通过可视化训练过程中损失函数的变化，可以判断梯度下降算法是否正常工作，如下图所示：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqgu51x8afj30zk0qojry.jpg" alt="lr_gd"><br>可以看到模型的损失随着迭代次数不断下降。下面使用sklearn自带的数据集iris来粗略地测试一下我们的分类器的效果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用sklearn自带的测试数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:<span class="number">100</span>]</span><br><span class="line">y = np.array(iris.target[:<span class="number">100</span>]).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 分割训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">19</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练集训练模型，用测试集测试模型</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型的各项评分</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">precision = precision_score(y_test, y_pred)</span><br><span class="line">recall = recall_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">f'accuracy <span class="subst">&#123;accuracy&#125;</span>, precision <span class="subst">&#123;precision&#125;</span>, recall <span class="subst">&#123;recall&#125;</span>'</span>)</span><br></pre></td></tr></table></figure></p><p>可以看到最后模型的accuracy、precision、recall都为1。感兴趣的读者还可以使用其他数据集来测试，完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/logistic_regression.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文用梯度下降算法实现了逻辑回归模型，由于梯度下降过程中考虑了所有的训练样本，所以是批量梯度下降。当数据量很大时，应该采用随机梯度下降或者小批量梯度下降来加快训练速度。或许读者会看到有的博客或者书中求解逻辑回归模型用的是梯度上升算法，这取决于损失函数的定义。如果原始的问题转化为求某个损失函数的极小值，那么就应该采取梯度下降算法，如本文中所示；如果原始的问题转化为求某个损失函数的最大值，那么就应该用梯度上升算法。梯度上升与梯度下降算法的区别仅仅在于更新参数时是加上梯度还是减去梯度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基本形式&quot;&gt;&lt;a href=&quot;#基本形式&quot; class=&quot;headerlink&quot; title=&quot;基本形式&quot;&gt;&lt;/a&gt;基本形式&lt;/h2&gt;&lt;p&gt;逻辑回归主要用于解决分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。&lt;br&gt;\begin{alig
      
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python实现线性回归</title>
    <link href="https://rearcher.github.io/linear_regression.html"/>
    <id>https://rearcher.github.io/linear_regression.html</id>
    <published>2018-04-16T13:05:42.000Z</published>
    <updated>2018-04-19T12:00:44.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>线性回归基本形式：</p><script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n</script><p>其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &gt; 1 $时是多元线性回归。<br><a id="more"></a></p><p>在实际实现中，常常采取如下的向量化:</p><script type="math/tex; mode=display">h_\theta(X) = X \theta</script><p>其中$ X $代表的是训练数据，并且每个训练数据是一行。假设每条训练数据有n个特征（包含$x_0$），总共有m条训练数据，那么$ X $就是$m \times n$的矩阵。$ \theta $代表的是参数矩阵，因为训练数据有n个特征，所以$ \theta $是$n \times 1$的列向量。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><p>根据以上的公式，模型求解的目标就是要求出参数$\theta$。最常见的有两种方法，一种是梯度下降法，另一种是正规方程（normal equation)。</p><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>介绍梯度下降算法之前需要先介绍一下线性回归模型的损失函数，常用的平方损失函数，定义如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2</script><p>一个直观的解释是，线性回归的损失函数计算的是预测点到实际点的距离的平方和。至于为什么要除以2，是因为在计算损失函数偏导的时候，会乘以2，所以刚好能够抵消，最后系数只剩下 $\frac{1}{m}$。</p><p>损失函数的向量化形式如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><p>向量化的意义在于用向量的形式实现，就可以使用矩阵来进行运算，相比于普通的循环来说更加高效，特别是当数据量很大的时候。</p><p>在知道损失函数之后，我们的目标就转换为，<strong>求解参数$\theta$使得损失函数的值达到最小</strong>。梯度下降法就是对损失函数求每个参数$\theta_j$的偏导，偏导就称为梯度，然后通过将当前的$\theta_j$减去其梯度的这种方式来更新参数，直至算法收敛（算法收敛的条件可以是损失函数的值小于某个值）。公式如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}</script><p>其中$\alpha$是学习率，$\frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$就是$\theta_j$对应的梯度。所以梯度下降法是一个迭代的算法，每轮迭代同时更新所有参数；另外还需要注意学习率$\alpha$的选择，$\alpha$太大的话，可能导致算法不收敛，$\alpha$太小可能导致迭代次数过多，算法收敛慢。</p><p>下面是梯度下降法更新参数的向量化形式：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>梯度下降法是个迭代的算法，需要迭代很多次，而正规方程则可以直接计算出线性回归的参数。这里先举个直观的例子，求某个二次函数的极值，例如$y = x^2 - 2x + 1$，要求它的极小值，可以对$x$求导，得到$2x - 2$，令$2x - 2 = 0$，得到$x = 1$是极小值点。正规方程也是基于这个思想，回顾前文所提到的线性回归模型损失函数的向量化形式，正规方程是根据损失函数对参数矩阵$\theta$求导（这里涉及到矩阵求导，不再介绍，具体推导过程可以参考<a href="https://blog.csdn.net/ZhikangFu/article/details/51315542" target="_blank" rel="noopener">这里</a>），令求导式子等于0，直接得出参数矩阵$\theta$的值。正规方程如下：</p><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>正规方程相比于梯度下降法，不需要迭代，也不需要选择学习率$\alpha$，只需要对矩阵求逆。当特征数量很少时，可以采用<strong>normal equation</strong>的方式，更加直接；当特征数量很大时，矩阵求逆会耗费大量的时间，采用梯度下降法更佳。另外，使用<strong>normal equation</strong>时，可能出现矩阵$X^T X$不可逆的情况，可能的原因是存在多余的特征，特征与特征之间依赖比较严重，例如线性相关；或者特征数太多，远大于样本数量，此时应当删去不必要的特征或者采取正则化（Regularization）。</p><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p>线性回归算法的核心就是参数的求解，这里给出梯度下降算法的实现和正规方程的python实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_iterations=<span class="number">1000</span>, learning_rate=<span class="number">0.01</span>, gradient_descent=True)</span>:</span></span><br><span class="line">        <span class="string">"""初始化.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            n_iterations: 梯度下降算法的迭代次数.</span></span><br><span class="line"><span class="string">            learning_rate: 梯度下降算法的学习速率.</span></span><br><span class="line"><span class="string">            gradient_descent: True代表使用梯度下降算法求解，False代表使用normal equation求解.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.n_iterationns = n_iterations</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.gradient_descent = gradient_descent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""计算当前模型对于训练数据的损失(平方损失)</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据中的特征</span></span><br><span class="line"><span class="string">            y: 训练数据中的目标值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            当前模型对于训练数据的损失</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_samples = len(y)</span><br><span class="line">        <span class="keyword">return</span> (X.dot(self.theta) - y).T.dot(X.dot(self.theta) - y) / <span class="number">2</span> / n_samples</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        <span class="string">"""用训练数据来训练模型</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            X: 训练数据中的特征</span></span><br><span class="line"><span class="string">            y: 训练数据中的目标值</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            theta: 线性回归模型的参数</span></span><br><span class="line"><span class="string">            costHistory: 如果使用梯度下降算法求解，梯度下降算法过程中模型的历史损失会被返回</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入常量1作为偏置bias特征</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        y = np.array(y).reshape(len(y), <span class="number">1</span>)</span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line">        self.theta = np.zeros(n_features).reshape(n_features, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        costHistory = np.zeros(self.n_iterationns)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.gradient_descent:</span><br><span class="line">            <span class="comment"># normal equation</span></span><br><span class="line">            self.theta = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iterationns):</span><br><span class="line">                tmp = X.T.dot(X.dot(self.theta) - y)</span><br><span class="line">                self.theta -= tmp * self.learning_rate / n_samples</span><br><span class="line">                costHistory[i] = self.computeCost(X, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.theta, costHistory</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X.dot(self.theta)</span><br></pre></td></tr></table></figure></p><p>在实现梯度算法的过程中，我们可以计算每一步当前模型的损失，用来判断梯度下降法是否正常工作，因此在上面的fit方法实现时，返回了梯度下降算法过程中模型的历史损失，可以通过绘图的方式来观察其变化，如下图所示：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqetwfiqr6j315s10475s.jpg" alt="cost_history"></p><p>可以看到模型的损失随着迭代次数的增加而不断减少，证明梯度下降算法是正常工作的。</p><p>下面使用sklearn自带的数据diabetes来测试我们的线性回归模型，用scatterplot画出数据点的分布范围，用不同的直线画出不同迭代次数时的模型拟合情况以及使用正规方程时的模型拟合情况，如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fqetwfjguyj315s104q53.jpg" alt="linear_regression_result"></p><p>可以看到随着迭代次数不断增加，拟合的效果越来越好，越来越接近正规方程的拟合效果。图中的模型拟合时学习速率设置相同，都为0.05，读者还可以尝试不同学习速率导致的拟合效果变化，完整代码在<a href="https://github.com/Rearcher/template/blob/master/ml/linear_regression.py" target="_blank" rel="noopener">这里</a>。</p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>这里有必要提一下多项式回归，其实多项式回归的本质也是线性回归，只不过添加了高维的特征。要实现多项式回归，上面的线性回归代码都无需改动，只需对特征$X$进行改动，添加高阶特征即可，感兴趣的读者可以自行实现并寻找一些训练数据来查看拟合效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文用python实现了简单的线性回归模型，实现了梯度下降算法和正规方程（normal equation）。由于梯度下降的过程中我们考虑了所有的样本点，这种梯度下降方式也称为批量梯度下降，在训练数据量较小时可以使用，但训练数据很大时会导致训练时间太长，不可取，此时可以使用随机梯度下降算法或者小批量梯度下降算法。</p><p>本文还没有涉及到模型的正则化（Regularization），用于解决模型的过拟合问题，会在以后实现其他模型的时候涉及，敬请期待！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本形式&quot;&gt;&lt;a href=&quot;#基本形式&quot; class=&quot;headerlink&quot; title=&quot;基本形式&quot;&gt;&lt;/a&gt;基本形式&lt;/h2&gt;&lt;p&gt;线性回归基本形式：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n&lt;/script&gt;&lt;p&gt;其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &amp;gt; 1 $时是多元线性回归。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://rearcher.github.io/tags/python/"/>
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng《machine learning》小结</title>
    <link href="https://rearcher.github.io/coursera-ml-summary.html"/>
    <id>https://rearcher.github.io/coursera-ml-summary.html</id>
    <published>2018-04-01T07:35:27.000Z</published>
    <updated>2018-04-19T12:13:54.652Z</updated>
    
    <content type="html"><![CDATA[<p>花了两周时间学完了吴恩达的机器学习课程，做完了所有作业。虽然编程作业大多是照着公式写代码，框架都给你搭好，几乎是保姆式的服务，但还是或多或少有所收获。因为有点基础，所以看得比较快，十一周的课程只花了两周的时间。总的来说还是值得一看，涉及到了很多方面，在此做个小结。<br><a id="more"></a></p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><p>线性回归基本形式：</p><script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n</script><p>其中$ \theta $代表参数， $ \theta_0 $就是通常说的偏置bias。$ x_n $代表特征，如果$ n = 1 $，则是一元线性回归；$ n &gt; 1 $时是多元线性回归。线性回归可以很方便地改成多项式回归，只需要把特征$ x_n $变换成对应的多项式特征即可。因此掌握了最基本的一元线性回归，就掌握了多元线性回归、多项式回归。</p><p>在实际实现中，常常采取如下的向量化:</p><script type="math/tex; mode=display">h_\theta(X) = X \theta</script><p>其中$ X $代表的是训练数据，并且每个训练数据是一行。假设每条训练数据有n个特征（包含$x_0$），总共有m条训练数据，那么$ X $就是$m \times n$的矩阵。$ \theta $代表的是参数矩阵，因为训练数据有n个特征，所以$ \theta $是$n \times 1$的列向量。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>线性回归的损失函数是平方损失，定义如下所示：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2</script><p>一个直观的解释是，线性回归的损失函数计算的是预测点到实际点的距离的平方和。至于为什么要除以2，是因为在计算损失函数偏导的时候，会乘以2，所以刚好能够抵消，最后系数只剩下 $\frac{1}{m}$。</p><p>损失函数的向量化形式如下：</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><p>这里又提到了向量化，向量化的意义在于用向量的形式实现，就可以使用矩阵来进行运算，相比于普通的循环来说更加高效，特别是当数据量很大的时候。</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练就是拟合模型参数的过程，线性回归有两种求解模型参数的办法，一种是将问题转化为损失函数最小化，通过梯度下降法来不断更新要求的参数，使损失函数达到最优；另一种是用<strong>Normal Equation</strong>直接求出。</p><p>梯度下降法就是对损失函数求每个参数$\theta_j$的偏导，偏导就称为梯度，然后通过将当前的$\theta_j$减去其梯度的这种方式来更新参数，直至算法收敛（算法收敛的条件可以是损失函数的值小于某个值）。公式如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}</script><p>其中$\alpha$是学习率，$\frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$就是$\theta_j$对应的梯度。所以梯度下降法是一个迭代的算法，每轮迭代同时更新所有参数；另外还需要注意学习率$\alpha$的选择，$\alpha$太大的话，可能导致算法不收敛，$\alpha$太小可能导致迭代次数过多，算法收敛慢。</p><p>下面是梯度下降法的向量化形式：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><p><strong>Normal Equation</strong>也是求解线性回归参数的一种方法，方程如下：</p><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>相比于梯度下降法，不需要迭代，也不需要选择学习率$\alpha$，只需要对矩阵求逆。当特征数量很少时，可以采用<strong>Normal Equation</strong>的方式，更加直接；当特征数量很大时，矩阵求逆会耗费大量的时间，采用梯度下降法更佳。另外，使用<strong>Normal Equation</strong>时，可能出现矩阵$X^T X$不可逆的情况，可能的原因是存在多余的特征，特征与特征之间依赖比较严重，例如线性相关；或者特征数太多，远大于样本数量，此时应当删去不必要的特征或者采取正则化（Regularization）。</p><p><strong>Normal Equation</strong>的推导过程可见<a href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression" target="_blank" rel="noopener">这里</a>。</p><h3 id="特征归一化"><a href="#特征归一化" class="headerlink" title="特征归一化"></a>特征归一化</h3><p>特征归一化（Normalization）主要是为了加速梯度下降算法，能够减少梯度下降法的迭代次数。</p><script type="math/tex; mode=display">x_i := \dfrac{x_i - \mu_i}{s_i}</script><p>其中$x_i$是原始的特征，$\mu_i$是特征$x_i$的均值，$s_i$是$x_i$的范围，实际情况中也用$x_i$的标准差来替代$s_i$。</p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="基本形式-1"><a href="#基本形式-1" class="headerlink" title="基本形式"></a>基本形式</h3><p>逻辑回归（Logistic Regression）虽然名字上带了“回归”，但主要用于二分类问题。逻辑回归函数的输出是一个处于0-1之间的值，代表输出是1的概率。<br>\begin{align}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align}</p><p>基本形式如下：<br>\begin{align}&amp; h_\theta (x) =  g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align}</p><p>可以看到逻辑回归其实就是在线性回归上套了一个$g$函数，也称为Sigmoid函数，形式如下图：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy310ss7lj30my03k74g.jpg" alt="sigmoid"></p><p>可以看到，当$z$的值大于0的时候，$g(z)$是大于0.5的，当$z$小于0时，$g(z)$是小于0.5的。所以如果假设$h_\theta(x) \geq 0.5$时有$y = 1$（即当输出概率大于等于0.5时，判定为类别1），就等价于$\theta^T x \geq 0 \Rightarrow y = 1$。</p><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>逻辑回归的损失函数是基于最大似然估计推导得到的，推导可见<a href="https://blog.csdn.net/ZhikangFu/article/details/51315542" target="_blank" rel="noopener">这里</a>。</p><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]</script><p>这里损失函数的意义是度量分类错误的代价，当分类正确时，损失函数中对应的部分为0。损失函数的向量化形式：<br>\begin{align}<br>&amp; h = g(X\theta)\newline<br>&amp; J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)<br>\end{align}</p><h3 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h3><p>采用梯度下降法，本质上也是对损失函数求偏导，迭代如下：</p><script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}</script><p>向量化形式如下：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})</script><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>由于逻辑回归是用于二分类问题，当需要应用到多分类问题时，采用的是OneVsAll策略，即对于某一类，训练其与其他类的分类器，所以n类需要训练n个分类器。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化（Regularization）主要用来过拟合问题，通常的做法是在损失函数上增加一项关于参数$\theta$的惩罚项。带正则化的逻辑回归损失函数如下：</p><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2</script><p>其中$ \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$就是惩罚项，$\lambda$是正则化系数。当$\lambda$过大时，可能造成模型欠拟合。当损失函数带有正则化时，梯度下降时对损失函数求偏导也会带有惩罚项，但需要注意不对$\theta_0$进行惩罚。</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="基本形式-2"><a href="#基本形式-2" class="headerlink" title="基本形式"></a>基本形式</h3><p>神经网络由神经元组成，单个的神经元的作用是通过若干个输入来输出一个值，如下图所示。通常会添加一个偏置单元bias作为输入。每条输入边上都有相应的参数$\theta$，神经元上有一个激活函数（课程中采用的是sigmoid函数），因此这里$h_\theta(x) = sigmoid(\theta^TX)$。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsb343j31vk10itju.jpg" alt="neuron"></p><p>神经网络由多层神经元组成，每层神经元的数量不等。基本的神经网络由输入层和输出层组成，输入层神经元的个数与特征数量相同，输出层神经元的个数与求解的具体问题相关，如果求解一个二分类问题，输出层的神经元个数就是2个。处于输入层和输出层之间网络层称之为隐层（Hidden Layer）。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsf1g2j31sg11a4d5.jpg" alt="neural_network"></p><p>神经网络求最后输出层的值采用的是前向传播算法，从前往后分别计算每一层每一个神经元上的值（用前一层神经元的输出值以及对应的权重和神经元上的激活函数来计算）。</p><h3 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h3><p>神经网络的损失函数与逻辑回归的损失函数类似，不过需要考虑网络每条边上的权重。具体形式如下：</p><script type="math/tex; mode=display">J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2</script><p>反向传播算法是用来最小化神经网络损失函数的一种算法，其本质上也是对损失函数求每条边上参数$\theta$的偏导，不过其采取从后往前一层一层计算的方式。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2ws5ui1j31uw10qdto.jpg" alt="bp_1"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrv0uwj31sw10atkb.jpg" alt="bp_2"></p><blockquote><p>训练神经网络的一般步骤：</p><ol><li>随机初始化所有参数。如果将参数初值都设为0，当反向传播时，那么所有节点的更新值都会变成一样，所以一般采用随机初始化的形式；</li><li>采用前向算法计算每一层神经元的输出值；</li><li>实现计算损失和用反向传播计算偏导的函数；</li><li>检查反向传播算法是否正常工作；</li><li>用梯度下降或者成熟的优化器来优化损失函数，例如在octave中用fmincg来函数优化损失函数，只需要传入计算损失和梯度的函数。</li></ol></blockquote><h2 id="应用机器学习的一些建议"><a href="#应用机器学习的一些建议" class="headerlink" title="应用机器学习的一些建议"></a>应用机器学习的一些建议</h2><blockquote><ol><li>将训练数据分为训练集、交叉验证集、测试集。用训练集来训练模型，交叉验证集来测试模型的性能，测试集来测试模型的泛化能力；</li><li>解决欠拟合：增加特征数量，增加多项式特征，降低正则化系数$\lambda$；</li><li>解决过拟合：增加训练数据，尝试更少的特征，增加正则化系数$\lambda$；</li><li>对于二分类问题，当正负样本数不均衡时，采用精确度accuracy来度量模型的性能是不合理的，此时应该采取准确率precision、召回率recall、F1。</li></ol></blockquote><h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h3 id="基本形式-3"><a href="#基本形式-3" class="headerlink" title="基本形式"></a>基本形式</h3><p>支持向量机要求函数输出0或者1，而不像逻辑回归是概率。</p><script type="math/tex; mode=display">h_\theta(x) =\begin{cases}    1 & \text{if} \ \Theta^Tx \geq 0 \\    0 & \text{otherwise}\end{cases}</script><h3 id="损失函数-3"><a href="#损失函数-3" class="headerlink" title="损失函数"></a>损失函数</h3><p>课程中通过逻辑回归的损失函数引出支持向量机的损失函数。对于逻辑回归，如果$y = 1$，要求$\theta^TX \geq 0$，而支持向量机则要求$\theta^TX \gg 0$。基于此，对逻辑回归的损失函数进行一定改变，就得到了支持向量机的损失函数。</p><script type="math/tex; mode=display">J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j</script><p>损失函数中的$C$是支持向量机的正则化系数，其作用相当于$\frac{1}{\lambda}$。支持向量机寻找能够区分不同类别样本点的最佳决策边界，最佳决策边界到最近的样本点的距离称为间隔，由于支持向量机的本质是最大化这个间隔，所以又称为最大间隔分类器。最大间隔只有当C很大时才会取得，如果存在某些异常点并且我们不想让这些异常点影响最大间隔的决策边界，可以适当减少C。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>不采用核函数的支持向量机（也称为采用线性核linear kernel的支持向量机）对于线性可分数据具有较好的性能，如果数据线性不可分，可以采用核函数来实现非线性支持向量机。所谓核函数，是指某种相似度函数。Andrew Ng对于于核函数的解释非常直观，在输入空间中选取几个代表性的点，将原始特征转化成与这几个代表性的点的相似度的向量作为新的特征，然后用新的特征来最小化损失函数。这样做的效果最后就是，决策边界在这几个代表性的点附近会弯曲，形成非线性的决策面。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrnthgj31t0120n3l.jpg" alt="kernel_1"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wrxg9vj31wi11ik04.jpg" alt="kernel_2"><br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wry9unj31xy11awpa.jpg" alt="kernel_3"></p><p>课程中提到的一个核函数是高斯核函数（Gaussian），其有个$\sigma^2$参数需要设置，$\sigma^2$太大会导致新特征变化缓慢，容易造成欠拟合；$\sigma^2$过小会导致新特征变化剧烈，容易造成过拟合。所以$\sigma^2$的作用与$C$相反，与$\lambda$相同。如果使用高斯核，建议对特征进行归一化。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy2wsvee7j31rw11g7pi.jpg" alt="gaussin_kernel_sigma"></p><p>接着上面说，实际使用过程中，对于有代表性的点的选取，常常是选择所有的输入数据。所以新的特征维数n与之前的训练数据量m相同。关于逻辑回归和支持向量机使用：</p><blockquote><ol><li>如果n远大于m，例如$n=10000$，$m = 100$，建议采用逻辑回归或者线性核支持向量机；</li><li>如果n很小，m也不大，例如$n = 100$， $m = 1000$，建议使用带高斯核的支持向量机；</li><li>如果n很小，m很大，例如$n = 100$， $m = 50000+$，建议增加更多特征，然后采用逻辑回归或者线性核支持向量机。</li></ol></blockquote><h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><p>主要介绍了聚类方法k-means和降维方法PCA。</p><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>k-means主要分为两步：</p><blockquote><ol><li>随机选取k个中心点；</li><li>将所有的数据点归类到与其距离最近的某个中心点，然后更新每一类的中心点。</li></ol></blockquote><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>PCA是将高维数据映射到低维的方法，其步骤如下：</p><blockquote><ol><li>计算协方差矩阵$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$；</li><li>计算协方差矩阵的特征向量；</li><li>选取前k个特征向量，组成$n \times k$的矩阵$Ureduce$，对于某个原始特征数据$x^{(i)}$，其降维后的特征$z^{(i)} = Ureduce^T \cdot x^{(i)}$</li></ol></blockquote><p>所以PCA的本质是将高维特征通过某个投影矩阵投影到低维。</p><p>原始特征的还原：$x_{approx}^{(i)} = U_{reduce} \cdot z^{(i)}$</p><p>如何选取k的值来构建投影矩阵？这里定义平方投影误差为$\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$，选择最小的k满足如下条件：</p><script type="math/tex; mode=display">\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01</script><p>但这种方式计算非常缓慢，对于某个k，需要计算所有的还原后的特征向量。现实实现中常常采取如下方式：</p><script type="math/tex; mode=display">\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99</script><p>其中$S_{ii}$是矩阵$S$里的值，而矩阵S在octave中可以通过如下代码求出：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] = svd(Sigma)</span><br></pre></td></tr></table></figure></p><p>PCA主要的用处是对特征数据进行降维处理。或者将高维数据映射到2维或者3维使其易于可视化。不要将PCA用来解决过拟合问题，并且不要一开始就对特征进行降维处理，应该先尝试原始特征的效果。</p><h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>异常检测（Anomaly Detection）是为了检测出数据中的异常点。这里我们定义一个模型$p(x)$来告诉我们一个样本是正常样本的概率，然后用一个阈值$\epsilon$来区分正常样本和异常样本（$p(x) &lt; \epsilon$）。</p><p>我们假设特征符合高斯分布。课程中提到了两种，一种是对于特征的每一维，都符合一个一维高斯分布；另一种是多维特征符合一个多维高斯分布。多维高斯分布相比于多个一维高斯分布，能够自动地捕捉到特征之间的联系，但计算量较大。然后根据训练数据来估计高斯分布的均值和方差。对于样本点$x$，只需计算其在估计出的模型$p(x)$上的值，并与$\epsilon$比较即可。</p><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>关于推荐系统介绍了协同过滤算法。假设我们的推荐系统要用于电影的推荐，那么关于电影其实可以用一个特征向量$x$来表述，例如<code>[动作，爱情，悬疑，恐怖]</code>等，然后用户对电影的喜爱其实是由一个参数向量$\theta$决定的，这样就可以根据评分数据在已知电影特征向量的情况下，为每个用户拟合一个线性回归模型，估计出每个用户的参数向量$\theta$，这就是<strong>基于内容的推荐系统</strong>。即最小化如下损失函数：</p><script type="math/tex; mode=display">min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u}  \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2</script><p>然而很多时候，电影的特征其实不是很好定义，一部电影的动作值是多少？爱情值是多少？这时候可以采用<strong>基于用户的推荐系统</strong>， 给出用户的参数向量$\theta$（即用户对动作电影的喜欢程度是多少，对爱情电影的喜欢程度是多少等），来拟合电影的特征数据，即最小化如下损失函数：</p><script type="math/tex; mode=display">min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m}  \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2</script><p>而<strong>协同过滤算法</strong>，就是整合了基于内容的推荐系统和基于用户的推荐系统，同时求解用户参数$\theta$和电影特征$x$，其损失函数如下：</p><script type="math/tex; mode=display">J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2</script><p>参数向量$\theta$和电影特征向量$x$的初始化可以采取随机值，在参数估计完之后，想要预测某用户$j$对某部没有看过电影$i$的评分，可以用$(\theta^{(j)})^T x^{(i)}$来获得。用于电影推荐的推荐系统，在具体实现时，常常对评分矩阵进行均值归一化（即对于每部电影的评分，减去所有用户对其评分的均值），这样对于某位一部电影都没看过的用户，会将其对其他电影的评分预测为平均分，而不全都预测为0分，预测时采用$(\theta^{(j)})^T x^{(i)} + \mu_i$公式来计算。</p><h2 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h2><p>当训练数据量非常大时，传统的梯度下降法（批量梯度下降）会非常耗时，因为每次求梯度都需要遍历所有的训练数据，课程中介绍了两种替代方案：随机梯度下降和小批量梯度下降。</p><h3 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h3><p>对样本进行随机排序，然后按序遍历样本，每次只用当前样本来计算梯度，进行梯度下降。</p><blockquote><p>对样本进行随机排序；<br>对于$i = 1 … m$，$\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j$</p></blockquote><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>每次只用小批量样本来计算梯度，假设小批量为10，算法如下：</p><blockquote><p>$For$  $i=1,11,21,31,…,991$<br>$\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>洋洋洒洒写了这么多，也算是对课程的一个回顾吧。总的来说，这门课讲得比较形象，虽然涉及到很多公式，但并没有深入去讲公式如何推导等，而是侧重于阐述直观上的理解，非常适合入门。这篇博文虽然涉及到了很多方面，但很多也只是列个公式，并没有做深入的讲解，只是列了一些课程中提到的注意点，说再多也显得晦涩，感兴趣的读者不如直接去看课程的视频吧。</p><p>后面准备开始动手实现一遍机器学习中的各种模型，加深对各个模型的理解，敬请期待！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了两周时间学完了吴恩达的机器学习课程，做完了所有作业。虽然编程作业大多是照着公式写代码，框架都给你搭好，几乎是保姆式的服务，但还是或多或少有所收获。因为有点基础，所以看得比较快，十一周的课程只花了两周的时间。总的来说还是值得一看，涉及到了很多方面，在此做个小结。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://rearcher.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://rearcher.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="coursera" scheme="https://rearcher.github.io/tags/coursera/"/>
    
  </entry>
  
  <entry>
    <title>从Solr源码看自动机的实现</title>
    <link href="https://rearcher.github.io/solr-automation.html"/>
    <id>https://rearcher.github.io/solr-automation.html</id>
    <published>2016-11-23T12:43:13.000Z</published>
    <updated>2018-04-02T04:11:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在公司被分配了一个任务，看一下Solr通配符查询的实现，主要是因为发现Solr对于通配符查询很慢，于是就哼哧哼哧跑去看源码了。源码看起来很累，大量调用的嵌套、委托模式，一层套一层，不过还是很有收获，今天不谈Solr具体查询的逻辑，谈一谈其查询的实现方式——自动机。对于想自己实现用自动机进行字符串匹配、正则表达式匹配的同学，还是值得一看的。(基于Solr_5.0.0)</p><h2 id="自动机的定义"><a href="#自动机的定义" class="headerlink" title="自动机的定义"></a>自动机的定义</h2><blockquote><p>An automaton is a self-operating machine, or a machine or control mechanism designed to follow automatically a predetermined sequence of operations, or respond to predetermined instructions.<br><a id="more"></a></p></blockquote><p>“自动机就是自我运行的机器，它的运行机制被设计为自动执行一些预定义好的操作序列，或者对一些预定义好的指令做出反映。”维基百科如是说。此时的自动机，仅仅是停留在机器的字面意思，指的就是物理的机器，例如上发条的机器，上了发条之后，会按照预定义好的逻辑行动。</p><h2 id="有限状态自动机"><a href="#有限状态自动机" class="headerlink" title="有限状态自动机"></a>有限状态自动机</h2><p>在计算机上运用比较普遍的就是有限状态自动机，简称状态机。所谓状态机，是一种表示若干个状态以及在这些状态之间的转移和动作等行为的数学模型。可以发现，状态自动机为什么称之为自动机，是因为其上的状态和状态之间的转移操作也都是预定义好的。而状态，在计算机程序中，就可以被抽象成很多东西，从而实现很多应用。状态自动机通常包含以下几个部分：</p><blockquote><ol><li>唯一初始状态</li><li>中间状态(可能是一个集合)</li><li>接收状态(可能是一个集合)</li><li>输入符号(可能是一个集合)</li><li>转换函数(从一个状态到另一个状态的转换)</li></ol></blockquote><p>来看一个简单的自动机：<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy4yyr9czj30gi06h3yu.jpg" alt="nfa_1"><br>其中，初始状态是0，中间状态是1，接收状态是2，{a,b}是输入符号。自动机最主要的一个应用就是用于正则表达式，上面的自动机对应的正则表达式是(a|b)*ab。其实到这里，自动机如何用于正则表达式匹配，就已经可见一斑了。一般过程如下：</p><blockquote><ol><li>对正则表达式生成相应的有限状态自动机</li><li>对输入的字符串，根据字符串中的字符，和状态机上的状态转移函数，进行状态转移，直到无法进行转移，或者到达输入的字符串的末尾</li><li>无法进行转移或者到达输入字符串的末尾时，判断当前状态是不是可接收状态，如果是的话，则代表该输入的字符串被正则表达式接收，反之则不接收</li></ol></blockquote><h2 id="确定的有限状态自动机-DFA-和不确定的有限状态自动机-NFA"><a href="#确定的有限状态自动机-DFA-和不确定的有限状态自动机-NFA" class="headerlink" title="确定的有限状态自动机(DFA)和不确定的有限状态自动机(NFA)"></a>确定的有限状态自动机(DFA)和不确定的有限状态自动机(NFA)</h2><p>上面举的自动机就是一个NFA，NFA和DFA的主要区别在于：</p><blockquote><ol><li>DFA没有输入空串上的转换操作</li><li>对于DFA，一个特定的符号输入，有且只能得到一个状态，而NFA可能得到一个状态集</li></ol></blockquote><p>上面举的NFA的例子，对于状态0，输入符号a，可能得到状态1，也可能得到状态0。至于输入空串上的转换，就是输入空，可以转到下一个状态。输入空通常用希腊字母Epsilon表示，如下图中状态0到状态1的转换。<br><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy4yyib3wj30go08q0tc.jpg" alt="此处输入图片的描述"><br>NFA和DFA具体的区别也很多，NFA也能转化成DFA，这里都不再展开了。</p><h2 id="Solr中的自动机实现"><a href="#Solr中的自动机实现" class="headerlink" title="Solr中的自动机实现"></a>Solr中的自动机实现</h2><p>好了，终于到了实现自动机的时候了。从上面的自动机概念的介绍可以发现，实现自动机重要的两个组成部分，就是状态以及状态之间的转移。Solr中状态之间的转换用Transition类来表示。Transition类如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Transition</span> </span>&#123;</span><br><span class="line">  <span class="comment">/** Sole constructor. */</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Transition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Source state. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> source;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Destination state. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> dest;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Minimum accepted label (inclusive). */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> min;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/** Maximum accepted label (inclusive). */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span> max;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">int</span> transitionUpto = -<span class="number">1</span>;</span><br><span class="line"> </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> source + <span class="string">" --&gt; "</span> + dest + <span class="string">" "</span> + (<span class="keyword">char</span>) min + <span class="string">"-"</span> + (<span class="keyword">char</span>) max;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到，成员变量主要有代表源状态的source，目标状态的dest，以及转换边上的label。可能会有同学觉得label的概念不是很好理解，可以看一下上面的NFA的实例图，label就是每条转换边上的字母。Solr中用min和max两个变量来指定了label的范围，而且都是int型变量，这是因为Solr会把字符转换成在Unicode字符集里的编号，即Java中的CodePoint。比如某个状态可以接受任意字符，那么用Transition类来表示的话，<code>min=Character.MIN_CODE_POINT</code>, <code>max=Character.MAX_CODE_POINT</code>。</p><p>下面就可以看一下自动机类Automaton的实现了，按照上面的思路，Automaton类里面，得包含一个整型数组，保存所有的状态，还得包含一个Transition数组，保存所有状态间的转换。但Solr源码可不是这样实现的，而是用到了一些小trick。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Index in the transitions array, where this states</span></span><br><span class="line"><span class="comment"> *  leaving transitions are stored, or -1 if this state</span></span><br><span class="line"><span class="comment"> *  has not added any transitions yet, followed by number</span></span><br><span class="line"><span class="comment"> *  of transitions. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span>[] states;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> BitSet isAccept;</span><br><span class="line"></span><br><span class="line"><span class="comment">/** Holds toState, min, max for each transition. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span>[] transitions;</span><br></pre></td></tr></table></figure></p><p>可以看到，代码中用transitions整型数组保存状态间的转移，用states整型数组保存状态信息。transitions数组用三个位置保存一个转换信息，依次是toState、min、max，这样三个一组三个一组，依次保存每一个转换的信息。你可能会好奇，为什么没有保存源状态信息，这就要说到states数组。states数组是两个一组，作为一个状态的信息，第一个位置表示该状态上的转移在transitions数组中的下标，第二个位置表示在该状态上的转移边的数量。这就容易理解了，假设我要知道状态i(假设从0开始)的第j个转移,那么直接访问<code>transitions[states[i*2] + j*3]</code>就可以了。</p><p>除此之外，我们还需要一个step函数，用于一步一步执行自动机，源码如下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Performs lookup in transitions, assuming determinism.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> state starting state</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> label codepoint to look up</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> destination state, -1 if no matching outgoing transition</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">step</span><span class="params">(<span class="keyword">int</span> state, <span class="keyword">int</span> label)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> state &gt;= <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">assert</span> label &gt;= <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> trans = states[<span class="number">2</span>*state];</span><br><span class="line">  <span class="keyword">int</span> limit = trans + <span class="number">3</span>*states[<span class="number">2</span>*state+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> we could do bin search; transitions are sorted</span></span><br><span class="line">  <span class="keyword">while</span> (trans &lt; limit) &#123;</span><br><span class="line">    <span class="keyword">int</span> dest = transitions[trans];</span><br><span class="line">    <span class="keyword">int</span> min = transitions[trans+<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">int</span> max = transitions[trans+<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span> (min &lt;= label &amp;&amp; label &lt;= max) &#123;</span><br><span class="line">      <span class="keyword">return</span> dest;</span><br><span class="line">    &#125;</span><br><span class="line">    trans += <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到，基本逻辑就是根据输入的label，判断label在不在该状态的Transition的min和max之间，在的话，就可以跳到该Transition的dest状态。</p><p>其实到这里，这个自动机已经可以跑了，如果你想用于字符串匹配，那么就根据字符串长度构造出相等数量的状态，再根据字符串中的每个字符，添加Transition，初始化好states和transiitons数组，匹配的时候通过step函数一步一步执行即可。Solr中还考虑了很多细节，不再展开叙述。</p><h2 id="Solr中自动机的快速执行"><a href="#Solr中自动机的快速执行" class="headerlink" title="Solr中自动机的快速执行"></a>Solr中自动机的快速执行</h2><p>由于Solr对于查询的匹配，需要从索引文件中读出很多Term在自动机上进行匹配，所以能够快速执行自动机匹配能够有效降低结果返回的延迟。基本的思想还是用空间换时间。Solr中把每个Automaton类转换成RunAutomaton类，从而快速执行，看一下RunAutomaton类中的变量，就可以了解其实现方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] transitions; </span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] points; <span class="comment">// char interval start points</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span>[] classmap; <span class="comment">// map from char number to class class</span></span><br></pre></td></tr></table></figure></p><p>先说一下这个points整型数组的作用，用于保存自动机的所有转换边上的输入，即所有转换边上输入字符的CodePoint，并且是从小到大排序好的。这里的transitions数组不同于Automaton类中的Transition数组，这里的transitions数组看似是一个一维数组，实则是当二维数组来用的，用于保存自动机上的状态经过points数组中保存的输入字符的转换能够到达的状态。如果说状态数是n，那么transitions数组的长度就是<code>n * points.length</code>。如果不存在该输入字符的转换，那么transitions数组上对应位置的元素是-1。可以看到，这样用一个很大的数组保存所有的转换，具有很大的冗余。不过优势在于，只需要生成一次RunAutomaton实例，就可以重复使用。</p><p>现在假设我们在状态i，输入字符c，想要知道下一个状态。那么首先需要从points数组中找出字符c对应的CodePoint在points数组中的下标j，那么<code>transitions[i*points.length + j]</code>即代表下一个状态。现在已经很快了，不过还有一个不足的地方，倘若points数组很大，从头到尾遍历需要很长的时间，一种方式是采用二分查找（因为是排好序的，不过Solr并未采用），另一种方式是更极端的用空间换时间，启用classmap整型数组。classmap数组保存的是某个输入字符的CodePoint在points数组中的下标，也就是说，比如输入字符c，其对应的CodePoint是d，那么<code>points[classmap[d]] = d</code>。现在只需要直接访问<code>transitiosn[i*points.length + classmap[d]]</code>就可以知道下一个状态了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在公司被分配了一个任务，看一下Solr通配符查询的实现，主要是因为发现Solr对于通配符查询很慢，于是就哼哧哼哧跑去看源码了。源码看起来很累，大量调用的嵌套、委托模式，一层套一层，不过还是很有收获，今天不谈Solr具体查询的逻辑，谈一谈其查询的实现方式——自动机。对于想自己实现用自动机进行字符串匹配、正则表达式匹配的同学，还是值得一看的。(基于Solr_5.0.0)&lt;/p&gt;
&lt;h2 id=&quot;自动机的定义&quot;&gt;&lt;a href=&quot;#自动机的定义&quot; class=&quot;headerlink&quot; title=&quot;自动机的定义&quot;&gt;&lt;/a&gt;自动机的定义&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;An automaton is a self-operating machine, or a machine or control mechanism designed to follow automatically a predetermined sequence of operations, or respond to predetermined instructions.&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="solr" scheme="https://rearcher.github.io/tags/solr/"/>
    
      <category term="automaton" scheme="https://rearcher.github.io/tags/automaton/"/>
    
  </entry>
  
  <entry>
    <title>基于Netty的HTTP代理服务器</title>
    <link href="https://rearcher.github.io/netty-http-proxy.html"/>
    <id>https://rearcher.github.io/netty-http-proxy.html</id>
    <published>2016-09-09T10:17:14.000Z</published>
    <updated>2018-04-02T03:24:16.131Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看netty，光看看书也是挺无聊的，就想着写点东西。Netty的<a href="https://github.com/netty/netty" target="_blank" rel="noopener">Github</a>上面有很多example可以学习，想到之前做的项目用Javascript写了一个http/https代理服务器来获取某些请求的参数，就想着自己再用netty来实现一下。思想很简单，实现的时候还是有很多细节的地方要注意。而且暂时https的代理还没能实现，遇到了一些问题，暂时避开这方面。<br><a id="more"></a><br>首先看一张架构图吧，代码实现基于下图的流程：<br> <img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy508njcgj30ux0ds3z2.jpg" alt="Netty http proxy"><br>有几点需要解释一下：</p><ol><li>代理服务器通过bind()操作，与本机地址的某个端口绑定，bind()操作会产生一个Channel，属于本地的Channel。这个Channel没有在上图中画出来;</li><li>代理服务器通过与客户端建立连接，为每个客户端生成一个Channel，即图中名为inbound的通道;</li><li>这个时候，客户端的请求发送到了我们的代理服务器，代理服务器要做的就是解析请求的host，然后与真正的server建立连接，此时建立的即为图中名为outbound的通道，然后将客户端的请求发送过去;</li><li>outbound中收到Server的回复时，再写入inbound通道，客户端就可以看见真正的服务器返回的数据。</li></ol><p>代码中值得一提的是，什么时候建立outbound通道？因为建立outbound通道需要知道Server的地址，所以在读取到第一个FullHttpRequest的时候时候，解析Request中host和port，然后建立。还有一点需要注意的是，outbound通道所依赖的EventLoop应该与inbound一致，这样避免了新建额外的线程，也省去了一些线程切换的开销。</p><p>完整代码在我的<a href="https://github.com/Rearcher/netty-demos/tree/master/src/main/java/com/renhuanhuan/proxy" target="_blank" rel="noopener">Github</a>上，基于maven构建，写的比较糙，欢迎交流、批评指正。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在看netty，光看看书也是挺无聊的，就想着写点东西。Netty的&lt;a href=&quot;https://github.com/netty/netty&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt;上面有很多example可以学习，想到之前做的项目用Javascript写了一个http/https代理服务器来获取某些请求的参数，就想着自己再用netty来实现一下。思想很简单，实现的时候还是有很多细节的地方要注意。而且暂时https的代理还没能实现，遇到了一些问题，暂时避开这方面。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="netty" scheme="https://rearcher.github.io/tags/netty/"/>
    
      <category term="maven" scheme="https://rearcher.github.io/tags/maven/"/>
    
  </entry>
  
  <entry>
    <title>网易面试小记</title>
    <link href="https://rearcher.github.io/netease-interview.html"/>
    <id>https://rearcher.github.io/netease-interview.html</id>
    <published>2016-09-03T11:36:19.000Z</published>
    <updated>2018-04-02T03:24:04.959Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间有幸拿到了网易校招内推批次的内推码，于是就抱着试一试的心态投了杭研的Java职位。首先是笔试，笔试过程也是充满了戏剧性。第一次笔试由于平台的原因导致我提前交卷，没能答好，于是就有了第二次笔试的机会。鉴于第一次笔试的经验，第二次做的比较顺利，编程题有的很快也就有了思路，所以最后笔试也是过了，还是挺高兴的。<br><a id="more"></a><br>笔试结果出来的一个多礼拜之后，赶赴杭州面试。先说结果，比较遗憾，没能通过，不过也在预料之中。当天有免费的午餐，第一次身处那么多程序员的环境，感觉还挺奇妙的。总共进行了两次技术面试，一次HR面试。感觉技术面试还是很轻松的，主要是讲一讲做过的项目，之前参加比赛的经历等，还有一些技术上的问题，跟技术人员谈话，总有莫名的亲切感，一开始的紧张也就烟消云散了。不过HR面就没那么轻松了，也是我自己的原因，之前并没有在春招的时候找公司实习，项目经验也不是很多，简历显得非常精简，入不得HR的法眼吧。我也问HR，没有实习经验怎么弥补，他说，“有些东西是弥补不了的”，确实很无奈，如果大二那时候的我有我现在的觉悟就好啦……</p><p>不过多去经历总是好的，总要一步一步来嘛。比如笔试的编程题，由于平时在LeetCode上坚持刷了一段时间，就能很快有些思路。感觉找到了些学习的路子，还是继续努力吧，积少成多，聚沙成塔，总是这么个道理。</p><p>最后分享一下技术面问的问题(Java开发工程师)：</p><ul><li>Object类有哪些方法</li><li>hashmap的实现原理，散列冲突的解决方案</li><li>文件每行包含一个字符串，对这些字符串进行排序，但是文件大小大于可用内存</li><li>多线程，线程启动的方式，线程间资源冲突怎么解决</li><li>设计模式，应用场景</li><li>网络相关，TCP建立连接、释放连接</li><li>JVM内存布局，自动内存管理，垃圾收集</li><li>Java多态实现原理</li><li>求第K个最小数</li><li>最后就是个人项目，技术面都会问</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前段时间有幸拿到了网易校招内推批次的内推码，于是就抱着试一试的心态投了杭研的Java职位。首先是笔试，笔试过程也是充满了戏剧性。第一次笔试由于平台的原因导致我提前交卷，没能答好，于是就有了第二次笔试的机会。鉴于第一次笔试的经验，第二次做的比较顺利，编程题有的很快也就有了思路，所以最后笔试也是过了，还是挺高兴的。&lt;br&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://rearcher.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="java" scheme="https://rearcher.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Java自动内存管理</title>
    <link href="https://rearcher.github.io/java-memory-management.html"/>
    <id>https://rearcher.github.io/java-memory-management.html</id>
    <published>2016-08-11T02:55:15.000Z</published>
    <updated>2018-04-02T03:23:49.347Z</updated>
    
    <content type="html"><![CDATA[<p>最近找了两本Java虚拟机方面的书，看了看其中对于Java自动内存管理的章节，写的都大同小异，在此总结一下，主要是三个方面：内存划分、内存分配、内存回收。<br><a id="more"></a></p><h2 id="内存划分（运行时数据区）"><a href="#内存划分（运行时数据区）" class="headerlink" title="内存划分（运行时数据区）"></a>内存划分（运行时数据区）</h2><p><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy51fqxsbj310u0p9dgk.jpg" alt="JVM运行时数据区"></p><p>从线程的角度来分，可分为线程私有和线程共享的，上图中左边的灰色区域就是线程共享的区域，包括堆、方法区、运行时常量池。而右边的区域则是线程私有的，包括程序计数器、虚拟机栈。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>堆是虚拟机管理的内存中最大的一块，是被线程共享的一块区域，主要用于存放对象实例，<code>但并不是所有对象都是在堆上分配的</code>。同时堆也是垃圾收集器管理的主要区域。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p>方法区与堆一样，也是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。方法区虽然逻辑上与堆独立，但物理上属于堆。</p><h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><p>运行时常量池属于方法区的一部分，用于存放class文件中的常量池信息，主要是各种字面值和符号引用。另外，运行时常量池并不要求常量一定只能在编译期产生，运行期间也可能将新的常量放入池中，例如String类的intern()方法。</p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p>类似于操作系统中的程序计数器，不过这里的程序计数器指示的是正在执行的字节码指令的地址。字节码解释器的执行完一条指令后，会改变程序计数器的值，指向下一条需要执行的指令地址。之所以需要每个线程都使用一个独立的程序计数器，是因为能够让多线程程序正确执行，各条线程之间的计数器互不影响。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p>虚拟机栈描述的是Java方法执行的内存模型，其基本单位是栈帧，每个方法执行的时候都会创建一个栈帧。虚拟机一直在执行栈顶的栈帧所对应的方法，当一个方法中调用另一个方法时，就会新建一个被调用方法的栈帧，push进虚拟机栈，被调用方法执行结束，会将返回值写入调用他的栈帧，并将自己的栈帧从栈中弹出。<br>而方法的栈帧中，存放了局部变量表、操作数栈、动态链接、方法出口等信息。局部变量表所需的内存空间都是在编译器就能够确定的，用于存放方法内部的本地变量;操作数栈则是用来进行运算操作，将两个操作数从栈顶弹出，计算结果，压入栈。</p><p>还有一个没有提及的是<code>本地方法栈</code>，与虚拟机栈相似，不过是为本地方法服务的，虚拟机规范中对其没有强制规定，可由虚拟机具体实现。</p><h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><p><img src="https://ws1.sinaimg.cn/large/006b3mDlly1fpy51fqgspj30k80bxgll.jpg" alt="java堆分代"></p><p>上面说到，堆是内存管理的主要区域，堆中存放了各种各样的对象，进一步可以划分为新生代和老年代。其中，新生代里有Eden空间、From Survivor空间、To Survivor空间。这样划分主要是为了方便内存回收。具体各个空间的用途，到内存回收就会知道。</p><p>从Java代码中new一个对象说起，JVM首先会检查这个new指令的参数能够在常量池中定位到一个类的符号引用，然后检查与这个符号引用相对应的类是否已经成功经历过加载、解析、初始化等步骤，当类完成装载之后，就可以完全确定创建该类实例所需要的空间大小。然后JVM就会为该实例进行内存分配。</p><p>下面就是分配在哪的问题。一般会分配在堆中的Eden空间，如果启动了本地线程分配缓冲，会优先在TLAB(Tread Local Allocation Buffer,即本地线程分配缓冲区)中分配，TLAB是Eden空间中线程私有的部分，大约占据Eden总空间的1%。 如果分配到Eden空间失败，就会进行一次新生代的垃圾收集工作。对于需要大量连续内存的大对象，会直接分配到老年代。</p><p>另外涉及到的一个概念是<code>逃逸分析</code>。上文也提到，并不是所有的对象都在堆中分配，其中有一部分对象是在栈上分配的，这里说的栈就是指虚拟机栈帧中的局部变量表部分。逃逸分析是JVM执行性能优化之前的一种分析技术，具体目标是分析出对象的作用域。如果一个对象的作用于仅限于方法体内部，就会在栈上为其分配内存，栈帧随着方法退出而销毁，不需要参与到垃圾收集中去。但一旦方法内部的对象被外部对象引用，这个对象就因此发生了逃逸，就不会在栈上分配。</p><h2 id="内存回收"><a href="#内存回收" class="headerlink" title="内存回收"></a>内存回收</h2><p>内存回收涉及到几个方面：哪些内存需要回收？什么时候回收？如何回收？</p><h3 id="可回收对象判定"><a href="#可回收对象判定" class="headerlink" title="可回收对象判定"></a>可回收对象判定</h3><p>常用的有<code>引用计数算法</code>和<code>根搜索算法</code>。</p><p>引用计数就是为每一个对象添加一个引用计数器，每当有一个地方引用它时，就将计数器的值加1，当引用失效时，计数器的值减1。任何时刻计数器值为0,说明对象不再被使用。此方法的缺陷在于，很难解决对象之间相互循环引用，如果两个需要回收的对象分别引用彼此，就无法被垃圾收集器回收。</p><p>根搜索算法通过一系列名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连，就证明此对象是不可用的。GC Roots对象包括栈帧中本地变量表中引用的对象、方法区中类静态属性引用的对象、方法区中常量引用的对象、本地方法栈中引用的对象。</p><h3 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h3><p>主要有标记-清除算法、复制算法、标记-压缩算法。</p><p>标记-清楚算法：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。标记和清除过程的效率都不高，而且标记清除之后会产生大量不连续的内存碎片，碎片太多会导致需要分配大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集操作。空闲的内存碎片可以用空闲列表来表示，从而提供下一次分配对象的内存地址。</p><p>复制算法：将内存划分为大小相等的两块，每次只使用其中的一块，当一块内存用完了，就将还活着的对象复制到另外一块上面，然后把已使用过的内存空间一次清理掉。运行高效，代价是损失了一般的内存空间。在堆中的新生代垃圾收集算法中，就使用了复制算法。将Eden空间、From Survivor空间中存活的对象复制到To Survivor空间，然后将From Survivor空间和To Survivor空间互换。(如果Eden空间和From Survivor空间的存活对象的分代年龄大于一定阈值或者To Survivor空间已满，会直接被分配到老年代）Eden空间和两个Survivor空间的缺省比例是8:1:1。之所以可以在新生代使用复制算法，是因为大多数新生代对象的生命周期都非常短暂。</p><p>标记-整理算法：与标记-清除算法差不多，不过此算法将所有存活对象向内存的一端移动，然后直接清理掉另一端的内存。此算法应用于老年代的垃圾收集。由于能够整理出一大块连续的空闲内存区域，所以用一个指针指向空闲内存区域的起点，用于指向下一次内存分配的位置。</p><h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><p>垃圾收集器有很多，而且虚拟机里整合了很多种垃圾收集器，本文不再赘述，值得一提的是<code>Stop-the-World</code>机制，通俗来说，垃圾收集进行的时候，工作线程必须停止一段时间，无论以哪种收集器进行垃圾收集，都会有或多或少的Stop-the-World时间。</p><p>另外一个是程序吞吐量与低延迟的权衡。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值。可通过<code>-XX:MaxGcPauseMillis</code>设置垃圾收集造成的Stop-the-World的时间，但为了低延迟而将该值调小之后，会导致相应的新生代内存空间变小，内存空间越小越容易被耗尽，会导致GC更加频繁，总的用于GC的时间可能反而会变多，导致程序吞吐量下降。</p><p>暂时就写这么多吧，下面是我看的两本书：</p><ul><li><a href="https://book.douban.com/subject/24722612/" target="_blank" rel="noopener">深入理解Java虚拟机</a>  </li><li><a href="https://book.douban.com/subject/26353219/" target="_blank" rel="noopener">Java虚拟机精讲</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近找了两本Java虚拟机方面的书，看了看其中对于Java自动内存管理的章节，写的都大同小异，在此总结一下，主要是三个方面：内存划分、内存分配、内存回收。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="jvm" scheme="https://rearcher.github.io/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>Java线程池的简单实现</title>
    <link href="https://rearcher.github.io/simple-java-thread-pool.html"/>
    <id>https://rearcher.github.io/simple-java-thread-pool.html</id>
    <published>2016-07-20T08:02:36.000Z</published>
    <updated>2018-04-02T03:08:34.069Z</updated>
    
    <content type="html"><![CDATA[<p>最近在写Java程序的时候，接触到一些多线程方面的东西，用到了Java中的线程池。JDK中对线程池的支持比较完善，在<code>java.util.concurrent</code>包中，用<code>ThreadPoolExecuter</code>类表示一个线程池，同时还有一个<code>Executor</code>类扮演着线程池工厂的角色。例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newFixedThreadPool</span><span class="params">(<span class="keyword">int</span> nThreads)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newSingleThreadExecutor</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ExecutorService <span class="title">newCachedThreadPool</span><span class="params">()</span></span></span><br><span class="line"><span class="function">...</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>这些工厂方法，从本质上，都是调用了<code>ThreadPoolExecutor</code>类的构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ThreadPoolExecutor</span><span class="params">(<span class="keyword">int</span> corePoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">int</span> maximumPoolSize,</span></span></span><br><span class="line"><span class="function"><span class="params">                    <span class="keyword">long</span> keepAliveTime,</span></span></span><br><span class="line"><span class="function"><span class="params">                    TimeUnit unit,</span></span></span><br><span class="line"><span class="function"><span class="params">                    BlockingQueue&lt;Runnable&gt; workQueue,</span></span></span><br><span class="line"><span class="function"><span class="params">                    ThreadFactory threadFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">                    RejectedExecutionHandler handler)</span></span></span><br></pre></td></tr></table></figure><p>参数含义如下：</p><ul><li>corePoolSize：线程池中应该保持的线程数量，即使线程空闲</li><li>maximumPoolSize：线程池中最大线程的数量</li><li>keepAliveTime：当线程数量大于corePoolSize时，指定空闲线程存在多久会被销毁</li><li>unit：keepAliveTime的单位</li><li>workQueue：任务队列，被提交但还没有执行</li><li>threadFactory：线程工厂</li><li>handler：拒绝策略</li></ul><h2 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h2><p>从<code>ThreadPoolExecutor</code>的构造函数中，我们大概知道实现一个线程池需要哪些东西，如果完全按照构造函数中的参数来的话，太麻烦，有太多地方需要考虑，因此实现一个简单版的。</p><ol><li>首先需要考虑线程池中存放多少线程。可以简单用一个变量来指定，并且这些线程要放在一个容器里，便于销毁，也便于知道他们的状态。</li><li>然后我们要考虑一个作为任务队列的容器。假如线程池中有5个线程，如果5个线程都处于工作状态的话，这时候送来的任务就需要放在任务队列中等待。</li><li>最后是线程池中工作线程的形式。工作线程在创建时开始就应该启动，其所做的工作主要是：<code>从任务队列中取出任务-执行任务</code>这样的无限循环。</li></ol><p>工作线程的一种实现方式：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WorkerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Boolean isRunning = <span class="keyword">true</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        isRunning = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">            Runnable task = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                task = taskQueue.take();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            task.run();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>WorkerThread是线程池的内部类，其中，<code>taskQueue</code>是任务队列，这里采用了<code>BlockingQueue</code>接口的一种实现，其<code>put</code>和<code>take</code>方法都是阻塞的。提交任务和销毁线程池如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">(Runnable task)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (currentWorking() &lt; limits) &#123;</span><br><span class="line">        WorkerThread worker = <span class="keyword">new</span> WorkerThread();</span><br><span class="line">        worker.start();</span><br><span class="line">        workers.add(worker);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        taskQueue.put(task);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!taskQueue.isEmpty());</span><br><span class="line">    <span class="keyword">for</span> (WorkerThread worker : workers) &#123;</span><br><span class="line">        worker.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完整代码在<a href="https://github.com/Rearcher/template/blob/master/Java/tools/SimpleThreadPool.java" target="_blank" rel="noopener">这里</a>。</p><p>线程池看似简单，其实很复杂，因为如果真要到一个应用级别的话，要考虑的东西还有很多，例如何时该启动一个线程，何时线程应该中止与挂起，任务队列的阻塞与超时，任务拒绝策略，线程生命周期等。至于本文实现的线程池，just for fun~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在写Java程序的时候，接触到一些多线程方面的东西，用到了Java中的线程池。JDK中对线程池的支持比较完善，在&lt;code&gt;java.util.concurrent&lt;/code&gt;包中，用&lt;code&gt;ThreadPoolExecuter&lt;/code&gt;类表示一个线程池，同时还有一个&lt;code&gt;Executor&lt;/code&gt;类扮演着线程池工厂的角色。例如：&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newFixedThreadPool&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; nThreads)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newSingleThreadExecutor&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; ExecutorService &lt;span class=&quot;title&quot;&gt;newCachedThreadPool&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;...&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
  </entry>
  
  <entry>
    <title>我理解中的JVM</title>
    <link href="https://rearcher.github.io/myjvm.html"/>
    <id>https://rearcher.github.io/myjvm.html</id>
    <published>2016-07-18T07:49:14.000Z</published>
    <updated>2018-04-02T03:08:34.070Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子amazon搞优惠活动，随便浏览了一下，发现了几本想看的书，其中一本就是<a href="https://book.douban.com/subject/26802084/" target="_blank" rel="noopener">《自己动手写Java虚拟机》</a>，一是最近正好在学java，二是对go语言比较好奇，于是就下单了。书挺薄的，300页不到，期末考完花了一个多星期的时间看完了，本文算是对这本书的一点总结吧。<br><a id="more"></a><br>全书主要讨论了以下几个方面：</p><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>顾名思义，类加载器就是把类从文件加载到内存中。Java程序通常是一个类放一个文件，并且会被编译成平台无关的字节码，成为一个个独立的class文件。类加载器工作步骤如下：</p><ol><li>获得类名，通常是<code>java/lang/Object.class</code>的形式</li><li>从<code>path/to/jre/lib</code>、<code>path/to/jre/lib/ext</code>和用户指定的<code>classpath</code>搜索类文件，找到就读入到内存中，否则出错</li><li><p>读入内存中的是原始的二进制数据，然后就需要对这一串数据进行解析，将其解析成一个结构体，大致如下：(书中先解析成class_file结构体，然后又进一步解析成了class结构体)</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ClassFile &#123;</span><br><span class="line">    u4             magic;</span><br><span class="line">    u2             minor_version;</span><br><span class="line">    u2             major_version;</span><br><span class="line">    u2             constant_pool_count;</span><br><span class="line">    cp_info        constant_pool[constant_pool_count<span class="number">-1</span>];</span><br><span class="line">    u2             access_flags;</span><br><span class="line">    u2             this_class;</span><br><span class="line">    u2             super_class;</span><br><span class="line">    u2             interfaces_count;</span><br><span class="line">    u2             interfaces[interfaces_count];</span><br><span class="line">    u2             fields_count;</span><br><span class="line">    field_info     fields[fields_count];</span><br><span class="line">    u2             methods_count;</span><br><span class="line">    method_info    methods[methods_count];</span><br><span class="line">    u2             attributes_count;</span><br><span class="line">    attribute_info attributes[attributes_count];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>最后进行链接。链接包括验证和准备阶段，验证阶段对类进行验证以确保安全性，准备阶段给类变量分配空间并且给予初值。</p></li></ol><p>这一部分比较容易，涉及到一点设计模式，例如组合模式。但有一个需要注意的地方是，数组和字符串相比于普通的类有点不一样。<br>数组的类由JVM在运行时生成，并且创建数组的方式和创建普通对象的方式不同，以及数组和普通对象存放的数据也是不同的。加载数组类的时候，需要自行完善class结构体的各个部分。<br>对于字符串，在class文件中是以MUTF-8格式保存的，在JVM运行期间，字符串被表示成java.lang.String对象的形式存在，在String对象内部，字符串又是以UTF-16格式保存的。新建字符串对象时，首先加载java.lang.String类，调用其构造函数，然后将读取到的数据以UTF-16格式存进去。</p><h2 id="运行时数据区"><a href="#运行时数据区" class="headerlink" title="运行时数据区"></a>运行时数据区</h2><p>JVM在运行的时候，需要一个地方，来存放各种各样的数据，比如类信息、各种变量等，这部分功能由运行时数据区来实现。运行时数据区可以分成两类：<strong>多线程共享的</strong>和<strong>线程私有的</strong>。</p><ol><li><p>多线程共享的运行时数据区<br>这一部分主要存放两类数据，<strong>类数据</strong>和<strong>类实例</strong>。类实例存放在堆中，类数据存放在方法区中。根据书上的图来理解的话，多线程共享的运行时数据区就是一个堆，里面包含了方法区，同时还有其他对象实例。堆由垃圾收集器定期清理。</p></li><li><p>线程私有的运行数据区<br>线程私有的运行时数据区用于辅助执行字节码。这里以线程为单位，每个线程拥有自己的JVM栈和PC寄存器，PC寄存器用来保存当前正在执行的指令的地址，JVM栈中保存了一个个JVM栈帧，一个帧中包含了一个局部变量表和操作数栈。<br><strong>操作数栈</strong>主要用来进行一些运算，有相应的指令把函数传入的参数和局部变量表中的数据推入操作数栈，进行运算时，就从操作数栈中弹出相应数量的操作数，进行运算，同时中间结果继续压入操作数栈，最后的结果会保存到局部变量表中。<br><strong>局部变量表</strong>主要用来存放局部变量的值。需要用到局部变量值的时候，会根据索引将其取出，并压入操作数栈。</p></li></ol><p>这一部分占了很多代码，一开始看很乱，来来回来看了好多遍。大致理解了一下，其实有了线程私有的运行数据区就可以执行单个方法了，只要从类数据中找到某个方法，然后直接从那部分字节码开始执行就可以。顺序执行还是跳转，都由相应的指令控制，指令会设置线程的PC寄存器。方法区是为了方法调用准备的。<br>同时这一部分还讲到了类和字段符号引用的解析。因为在定义变量的时候，需要用到很多类名和字段等。类符号引用的解析，除了得到类数据外，还要考虑当前执行代码的类是否有权限访问那个类。字段也是如此，但还要考虑继承关系，找不到的情况下要在超类中递归查找。</p><h2 id="方法调用"><a href="#方法调用" class="headerlink" title="方法调用"></a>方法调用</h2><p>方法调用是很有意思的一部分。用作者的话来说，实现了方法调用，这个JVM“从只能在地上爬的baby变成了能够到处跑”。<br>从调用的角度来看，方法可以分为静态方法和实例方法，静态方法是静态绑定的，实例方法是动态绑定的。从实现的角度来看，用Java语言实现的方法叫做Java方法，其他无法用Java语言实现而用本地语言实现的方法叫做本地方法。</p><h3 id="Java方法调用的过程"><a href="#Java方法调用的过程" class="headerlink" title="Java方法调用的过程"></a>Java方法调用的过程</h3><ol><li>调用方法首先涉及到的一个问题就是方法符号引用的解析。其中，<strong>非接口方法符号引用</strong>和<strong>接口方法符号引用</strong>的解析又是不同的。<strong>非接口方法符号引用</strong>的解析首先需要解析得到相应的类，根据方法名和描述符查找方法，考虑继承关系、接口以及访问权限。<strong>接口方法符号引用</strong>的解析则先在接口中查找，查不到再去超接口。</li><li>解析完成，就需要做<strong>方法调用和参数传递</strong>。JVM需要给这个方法创建一个新的JVM栈帧并推入栈顶，然后传递参数。传递的参数首先放在调用者的操作数栈中，只需要计算出参数的个数然后将其放到新JVM栈帧的局部变量表中就可以了。</li><li>方法执行完毕需要<strong>返回</strong>。返回指令通常把要返回的结果存入调用者的操作数栈，并且将之前新建的用于调用方法的栈帧从栈中弹出。</li></ol><h3 id="本地方法"><a href="#本地方法" class="headerlink" title="本地方法"></a>本地方法</h3><ol><li>本地方法注册，通过一个Map将key(类名+方法名+描述符)和实现函数建立一一对应的关系。</li><li>本地方法查找，从Map中查找本地方法。</li><li>本地方法调用。Java虚拟机规范并没有规定如何实现和调用本地方法，书的作者是通过定义虚拟机规范未定义的操作码指令来实现本地方法调用的。可以通过方法的访问标志来判断是不是本地方法，如果是本地方法的话，在新建方法对象放入方法区的时候就注入字节码（操作码+若干信息字节），在执行本地方法的时候，会跳到我们注入的字节码，其实就是一条指令，通过这条指令来调用本地方法。</li></ol><h2 id="指令集和解释器"><a href="#指令集和解释器" class="headerlink" title="指令集和解释器"></a>指令集和解释器</h2><p>解释器其实以上就有涉及，字节码的执行就是解释器来完成的。书的作者通过实现各种指令集来完成指令的解码。在读取字节码的过程中，先读取操作码，然后通过操作码生成新的指令，执行，计算下一条指令地址，重复上述过程，直至程序结束。书的作者实现了约200条指令，是个复杂并且需要细心耐心的工作。</p><p>总结差不多就到这吧，感谢作者。对我来说，这本书算是对JVM的一个启蒙教育吧。感觉时间越来越不够用，还是要不断学习呀。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前阵子amazon搞优惠活动，随便浏览了一下，发现了几本想看的书，其中一本就是&lt;a href=&quot;https://book.douban.com/subject/26802084/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《自己动手写Java虚拟机》&lt;/a&gt;，一是最近正好在学java，二是对go语言比较好奇，于是就下单了。书挺薄的，300页不到，期末考完花了一个多星期的时间看完了，本文算是对这本书的一点总结吧。&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="https://rearcher.github.io/categories/java/"/>
    
    
      <category term="go" scheme="https://rearcher.github.io/tags/go/"/>
    
      <category term="jvm" scheme="https://rearcher.github.io/tags/jvm/"/>
    
  </entry>
  
</feed>
